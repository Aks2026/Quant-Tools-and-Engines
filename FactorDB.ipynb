{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import functools as ft\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import math\n",
    "from datetime import date, timedelta\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "periodMapping = {\n",
    "                    'LTM'        : {\"1Y\" : {\"S\" : 0,   \"F\" : 252},\n",
    "                                    \"3Y\" : {\"S\" : 252, \"F\" : 500},\n",
    "                                    \"5Y\" : {\"S\" : 750, \"F\" : 500}},\n",
    "\n",
    "                    'Momentum'   : {'1W' : 5, \"1M\" : 20, \n",
    "                                    \"3M\" : 62, \"6M\" : 125, \n",
    "                                    \"1Y\" : 250,\"2Y\" : 500,  \n",
    "                                    \"3Y\" : 750},\n",
    "\n",
    "                    'Volatility' : {'1W' : 5, \"2W\" : 10, \n",
    "                                    \"1M\" : 20, \"3M\" : 62, \n",
    "                                    \"6M\" : 125, \"1Y\" : 250, \n",
    "                                    \"3Y\" : 750},\n",
    "                        \n",
    "                    \"Theme\"      : {'1W' : 5, \"2W\" : 10, \"2M\" : 40,\n",
    "                                    \"1M\" : 20, \"3M\" : 62,\n",
    "                                    \"1Y\" : 250, \"6M\" : 125,\n",
    "                                    \"3Y\" : 750}\n",
    "                }\n",
    "\n",
    "class EquityFactors:\n",
    "\n",
    "    def __init__(self, noOfYears, correct, peer = \"Theme\"):\n",
    "        ## Initialize the variables\n",
    "        self.stockPriceData = None\n",
    "        self.stockValueData = None\n",
    "        self.sectorData = None\n",
    "        self.benchmark = None\n",
    "        self.stockFactors = None\n",
    "        self.themeFactor = None\n",
    "        self.companyMaster = None\n",
    "        self.cashFlow = None\n",
    "        self.profitLossGrowth = None\n",
    "        self.profitLossQuality = None\n",
    "        self.financialRatio = None\n",
    "        self.appender = None\n",
    "        self.financeBalanceSheet = None\n",
    "\n",
    "        self.factorData = dict()\n",
    "        self.recentFactorUpdate = dict()\n",
    "        # self.__dataCursor = Data()\n",
    "        self.rawFactorData = dict()\n",
    "\n",
    "        self.noOfYears = noOfYears\n",
    "        self.correct = correct\n",
    "        self.peer = peer\n",
    "\n",
    "    def uniqueFileName(self, filename):\n",
    "        \"\"\"\n",
    "        Generates a unique filename by appending a counter to the base name if a file with \n",
    "        the specified filename already exists in the current directory.\n",
    "\n",
    "        Args:\n",
    "            filename (str): The desired filename.\n",
    "\n",
    "        Returns:\n",
    "            str: A unique filename that does not already exist in the current directory.\n",
    "        \"\"\"\n",
    "\n",
    "        # Split the path into directory and filename\n",
    "        directory, file_name = os.path.split(filename)\n",
    "        \n",
    "        # Split the filename into name and extension\n",
    "        name, ext = os.path.splitext(file_name)\n",
    "        \n",
    "        # Initialize counter\n",
    "        counter = 1\n",
    "        \n",
    "        # Generate the full path\n",
    "        full_path = os.path.join(directory, file_name)\n",
    "        \n",
    "        # Check if the file already exists\n",
    "        while os.path.exists(full_path):\n",
    "            # If it does, create a new filename with the counter\n",
    "            new_filename = f\"{name}_{counter}{ext}\"\n",
    "            full_path = os.path.join(directory, new_filename)\n",
    "            counter += 1\n",
    "            \n",
    "        return full_path\n",
    "\n",
    "    def __readPriceData(self,):\n",
    "        ''' Function  is used to read the price data '''\n",
    "        ## Read the price Data.\n",
    "        # self.stockPriceData = self.__dataCursor.fetch_price_data(no_of_years = self.noOfYears, inc_indices=True)\n",
    "        self.stockPriceData = pd.read_csv(\"./price_data.csv\")\n",
    "\n",
    "        self.stockPriceData[\"Date\"] = pd.to_datetime(self.stockPriceData[\"Date\"])\n",
    "\n",
    "        # if self.correct:\n",
    "        #     # Replace The Old Symbol With the New Symbol\n",
    "        #     mapping = pd.read_excel(\"company_master_mapping.xlsx\")\n",
    "        #     mapping = dict(zip(mapping[\"SYMBOL_NSE\"], mapping[\"SYMBOL_CM\"]))\n",
    "        #     self.stockPriceData[\"Symbol\"] = self.stockPriceData[\"Symbol\"].replace(mapping)\n",
    "\n",
    "        self.stockPriceData[\"Symbol\"] = self.stockPriceData[\"Symbol\"].str.strip()\n",
    "        self.stockPriceData1 = self.stockPriceData.copy()\n",
    "        self.stockPriceDataMom = self.stockPriceData.copy()\n",
    "        ## Drop the Rows with NaN Values\n",
    "        self.stockPriceData = self.stockPriceData.dropna()\n",
    "        ## Remove the data of Erroneous Date.\n",
    "        self.stockPriceData = self.stockPriceData[self.stockPriceData[\"Date\"] != \"2022-03-07\"]#.reset_index(drop = True)\n",
    "\n",
    "        ## Filter the data for the Benchmark data\n",
    "        self.benchmark = self.stockPriceData[self.stockPriceData[\"Symbol\"] == \"NIFTY500\"].reset_index(drop = True)\n",
    "        ## Filter the necessary columns for benchmark\n",
    "        self.benchmark = self.benchmark.filter([\"Date\",\"Close\"])\n",
    "        ## Calculate the daily percentage change for the benchmark and renaming the columns.\n",
    "        self.benchmark[\"BenchmarkReturn\"] = self.benchmark[\"Close\"].pct_change()\n",
    "        self.benchmark.rename(columns = {'Close' : \"BenchmarkPrice\"}, inplace = True)\n",
    "\n",
    "        ## Fetch the ETF Indices List\n",
    "        # self.etf = self.__dataCursor.fetch_data_from_database(table_name=\"Etf_Indices\")\n",
    "        self.etf = pd.read_csv(\"./etf_indices.csv\")\n",
    "        ## Remove the ETF Indices data from the stock data\n",
    "        self.stockPriceData = self.stockPriceData[~self.stockPriceData['Symbol'].isin(self.etf['Symbol'])]\n",
    "\n",
    "        self.priceDataLTM = self.stockPriceData.copy()\n",
    "        ## Remove the data for the Saturday and Sunday and Sort the dataframe\n",
    "        self.stockPriceData = self.stockPriceData[~self.stockPriceData[\"Date\"].dt.day_name().isin(['Sunday', \"Saturday\"])]\n",
    "        self.stockPriceData = self.stockPriceData.sort_values([\"Symbol\", \"Date\"]).reset_index(drop = True)\n",
    "\n",
    "        # ## Identify Companies which have been in Top-500 historically.\n",
    "        # top500Companies  = self.stockPriceData.groupby([\"Date\"]).apply(lambda x: x.sort_values(\"Mcap\", ascending=False).head(500), include_groups = False).reset_index(level = 0).reset_index(drop = True)\n",
    "        # self.top500Companies = top500Companies[\"Symbol\"].unique().tolist()\n",
    "        # ## Identify Companies which have been in TOP-1000 historically.\n",
    "        # top1000Companies  = self.stockPriceData.groupby([\"Date\"]).apply(lambda x: x.sort_values(\"Mcap\", ascending=False).head(1000), include_groups=False).reset_index(level = 0).reset_index(drop = True)\n",
    "        # self.top1000Companies = top1000Companies[\"Symbol\"].unique().tolist()\n",
    "\n",
    "        self.strategyUniverse = pd.read_csv(\"Universe.csv\")\n",
    "        # self.strategyUniverse = self.__dataCursor.fetch_data_from_database(table_name=\"universe_mcap_500\", no_of_years=50)\n",
    "        self.strategyUniverse[\"Date\"] = pd.to_datetime(self.strategyUniverse[\"Date\"])\n",
    "        self.strategyUniverse[\"Peer\"] = self.strategyUniverse[self.peer].copy()\n",
    "        self.strategyUniverse.rename(columns = {\"MCAP\" : \"Mcap\"}, inplace = True)\n",
    "\n",
    "        # sectordata = self.__dataCursor.fetch_data_from_database(table_name=\"SectorThemeGics\")\n",
    "        # self.strategyUniverse = pd.merge(self.strategyUniverse, sectordata, on = \"Symbol\", how = \"left\")\n",
    "        # self.strategyUniverse[[\"Theme\", \"Sector\"]] = self.strategyUniverse[[\"Theme\", \"Sector\"]].fillna(\"Others\")\n",
    "        # self.strategyUniverse[\"Peer\"] = self.strategyUniverse[self.peer].copy()\n",
    "\n",
    "    def __readSectorData(self,):\n",
    "        ## Inout the Sector Mapping data\n",
    "        # self.sectorData = self.__dataCursor.fetch_data_from_database(table_name=\"SectorThemeGics\")\n",
    "        self.sectorData = pd.read_csv(\"./gics.csv\")\n",
    "\n",
    "    def __readCompanyMaster(self,):\n",
    "        # self.companyMaster = self.__dataCursor.fetch_data_from_database(table_name = \"Companymaster\")\n",
    "        self.companyMaster = pd.read_csv(\"./CM.csv\")\n",
    "        self.companyMaster = self.companyMaster[self.companyMaster[\"SERIES\"] == \"EQ\"]\n",
    "\n",
    "        self.companyMaster = self.companyMaster[[\"FINCODE\", \"SYMBOL\"]].copy()\n",
    "        self.companyMaster = self.companyMaster.dropna().reset_index(drop = True)\n",
    "        self.companyMaster.rename(columns = {'SYMBOL' : \"Symbol\"}, inplace = True)\n",
    "\n",
    "    def __readProfitLossGrowth(self,):\n",
    "        # self.profitLossGrowth = self.__dataCursor.fetch_data_from_database(table_name = \"Quarterly\")\n",
    "        self.profitLossGrowth = pd.read_csv(\"./Quarterly.csv\")\n",
    "        self.profitLossGrowth = self.profitLossGrowth[self.profitLossGrowth[\"Result_Type\"] == \"Q\"].reset_index(drop = True)\n",
    "        self.profitLossGrowth = self.profitLossGrowth.filter(items = [\"Fincode\", \"Date_End\",  \"PAT\", \"OPERATING_PROFIT\", \"GROSS_PROFIT\",\n",
    "                                                                      \"NET_SALES\", \"EPS_DILUTED\", \"PBT\", 'Debt/Equity Ratio', 'Adj_eps_abs', \"Dividend payout ratio\"])\n",
    "        self.profitLossGrowth.rename(columns = {'Fincode' : \"FINCODE\"}, inplace = True)\n",
    "\n",
    "    def __readCashFlow(self, ):\n",
    "        # self.cashFlow = self.__dataCursor.fetch_data_from_database(table_name = \"Finance_cf\")\n",
    "        self.cashFlow = pd.read_csv(\"./Finance_Cf.csv\")\n",
    "\n",
    "    def __readValueData(self):\n",
    "            ## Input the Company Valuation Data.\n",
    "\n",
    "            # if self.correct:\n",
    "            #     self.stockValueData = self.__dataCursor.fetch_data_from_database(table_name = \"Value_RawData_Test\", no_of_years = self.noOfYears)\n",
    "            # else:\n",
    "            #     self.stockValueData = self.__dataCursor.fetch_data_from_database(table_name = \"Value_RawData\", no_of_years = self.noOfYears)\n",
    "            \n",
    "            self.stockValueData = pd.read_csv(\"./Value_Data.csv\")\n",
    "            ## COnvert the Date column from string to Datetime format\n",
    "            self.stockValueData[\"Date\"] = pd.to_datetime(self.stockValueData[\"Date\"])\n",
    "\n",
    "            # Import the Nifty Valuation Fields\n",
    "            # self.benchmarkValueData = self.__dataCursor.fetch_data_from_database(table_name = \"Nifty_PE_PB\", no_of_years = self.noOfYears)\n",
    "            self.benchmarkValueData = pd.read_csv(\"./Nifty_PE_PB.csv\")\n",
    "            self.benchmarkValueData[\"Date\"] = pd.to_datetime(self.benchmarkValueData[\"Date\"])\n",
    "\n",
    "    def __readProfitLossQuality(self,):\n",
    "        # self.profitLossQuality = self.__dataCursor.fetch_data_from_database(table_name = \"Finance_pl\")\n",
    "        self.profitLossQuality = pd.read_csv(\"./Finance_pl.csv\")\n",
    "        self.profitLossQuality = self.profitLossQuality[[\"FINCODE\", \"Year_end\", \"Profit_after_tax\", \"Net_sales\", \n",
    "                                                        \"Operating_profit\", \"Gross_profits\", \"Adj_Eps\"]]\n",
    "\n",
    "    def __readFinancialRatio(self,):\n",
    "        # self.financialRatio = self.__dataCursor.fetch_data_from_database(table_name = \"Finance_fr\")\n",
    "\n",
    "        self.financialRatio = pd.read_csv(\"./Finance_fr.csv\")\n",
    "        self.financialRatio = self.financialRatio[[\"FINCODE\", \"Year_end\", \"Inventory_Days\", \"Receivable_days\", \"Payable_days\", \"ROE\", \"ROCE\", \n",
    "                                                   \"Total_Debt_Equity\", \"Interest_Cover\", \"CEPS\", \"ROA\", \"FCF_Share\", \"Dividend_Payout_Per\"]]\n",
    "        self.financialRatio = self.financialRatio[self.financialRatio[\"Year_end\"] >= 200012]\n",
    "        self.financialRatio = self.financialRatio.reset_index(drop = True)\n",
    "\n",
    "    def __readFinanceBalanceSheet(self,):\n",
    "        # self.financeBalanceSheet = self.__dataCursor.fetch_data_from_database(table_name = \"Finance_bs\")\n",
    "        self.financeBalanceSheet = pd.read_csv(\"./Finance_bs.csv\")\n",
    "        # self.financeBalanceSheet = self.financeBalanceSheet[[\"FINCODE\", \"Year_end\", \"Profit_after_tax\", \"Net_sales\", \n",
    "        #                                                  \"Operating_profit\", \"Gross_profits\", \"Adj_Eps\"]]\n",
    "        self.financeBalanceSheet.rename(columns = {\"Fincode\" : \"FINCODE\"}, inplace = True)\n",
    "\n",
    "    def generate_LowVol(self,):\n",
    "\n",
    "        ################\n",
    "        ## PRICE DATA ##\n",
    "        ################\n",
    "        # Check if stock price data is loaded; if not, read it\n",
    "        if self.stockPriceData is None:\n",
    "            self.__readPriceData()\n",
    "\n",
    "        ## Drop the Unnecessary COlumns\n",
    "        priceData = self.stockPriceData.drop(columns = [\"Open\", \"High\", \"Low\", \"Volume\", \"Mcap\"]).copy()\n",
    "        \n",
    "        ## Filter the Symbol which are present in strategy universe\n",
    "        priceData = priceData[priceData[\"Symbol\"].isin(self.strategyUniverse[\"Symbol\"].unique())].reset_index(drop = True)\n",
    "\n",
    "        # Calculate daily returns for each stock by symbol, based on \"Close\" prices\n",
    "        priceData[\"Returns\"] = priceData.groupby(\"Symbol\")[\"Close\"].pct_change()\n",
    "        \n",
    "        # Calculate squared returns for down days only; store in \"DReturns\" (for downside volatility)\n",
    "        priceData[\"DReturns\"] = np.square(np.where(priceData[\"Returns\"] < 0, priceData[\"Returns\"], 0))\n",
    "\n",
    "        # Calculate 1-year rolling average of downside returns for each symbol and take the square root\n",
    "        priceData[\"DownVol\"] = priceData.groupby(\"Symbol\")[\"DReturns\"].transform(lambda x: x.rolling(252).mean())\n",
    "        priceData[\"DownVol\"] = np.sqrt(priceData[\"DownVol\"])\n",
    "\n",
    "        # Calculate 1-year rolling standard deviation of returns as low volatility measure\n",
    "        priceData['LowVol'] = priceData.groupby(\"Symbol\")[\"Returns\"].transform(lambda x: x.rolling(window=252).std())\n",
    "\n",
    "        # Calculate average volatility as the mean of LowVol and DownVol\n",
    "        priceData[\"AvgVol\"] = priceData[[\"LowVol\", \"DownVol\"]].mean(axis=1)\n",
    "\n",
    "        ## Merge the Computed Metrics against the symbol on each day's universe\n",
    "        priceData = pd.merge(self.strategyUniverse, priceData, on = [\"Date\", \"Symbol\"], how = \"left\")\n",
    "\n",
    "        # lowvol raw data\n",
    "        self.rawFactorData['lowvol'] = priceData.copy()\n",
    "\n",
    "        # Rank stocks daily within the top 500 by LowVol, DownVol, and AvgVol, assigning percentile ranks\n",
    "        priceData[[\"LowVolRank\", \"DownVolRank\", \"AvgVolRank\"]] = priceData.groupby(\"Date\")[[\"LowVol\", \"DownVol\", \"AvgVol\"]].rank(ascending=False, pct=True)\n",
    "    \n",
    "        # Compute average volatilities for each theme on each date\n",
    "        themeScore = priceData.groupby([\"Peer\", \"Date\"])[[\"LowVol\", \"DownVol\", \"AvgVol\"]].mean()\n",
    "        \n",
    "        # Rename columns to reflect theme-based volatility measures\n",
    "        themeScore.columns = [\"PeerLowVol\", \"PeerDownVol\", \"PeerAvgVol\"]\n",
    "\n",
    "        # Merge theme scores with price data\n",
    "        priceData = pd.concat([priceData.set_index([\"Peer\", \"Date\"]), themeScore], axis=1).reset_index()\n",
    "\n",
    "        # Rank theme-based volatilities across all themes daily\n",
    "        priceData[[\"PeerLowVol\", \"PeerDownVol\", \"PeerAvgVol\"]] = priceData.groupby([\"Date\"])[[\"PeerLowVol\", \"PeerDownVol\", \"PeerAvgVol\"]].rank(ascending=False, pct=True)\n",
    "\n",
    "        # Select relevant columns and rename to remove \"Rank\" suffix for final output\n",
    "        df = priceData[[\"Date\", \"Symbol\", \"LowVolRank\", \"DownVolRank\", \"AvgVolRank\", \"PeerLowVol\", \"PeerDownVol\", \"PeerAvgVol\"]].copy()\n",
    "        df.columns = df.columns.str.replace(\"Rank\", \"\")\n",
    "\n",
    "        # Filter data from January 1, 2006, onwards and reset index\n",
    "        df = df[df[\"Date\"] >= \"2006-01-01\"].reset_index(drop=True)\n",
    " \n",
    "        # Initialize an empty list to store the transformed data for each volatility column\n",
    "        result = list()\n",
    "\n",
    "        # Iterate over each volatility-related column\n",
    "        for col in [\"LowVol\", \"DownVol\", \"AvgVol\", \"PeerLowVol\", \"PeerDownVol\", \"PeerAvgVol\"]:\n",
    "\n",
    "            # Filter the dataframe to keep only the \"Symbol\", \"Date\", and current column\n",
    "            temp = df.filter([\"Symbol\", \"Date\", col])\n",
    "\n",
    "            # Pivot the table to have dates as rows and symbols as columns, with values from the current column\n",
    "            temp = temp.pivot_table(index='Date', columns='Symbol', values=col).reset_index()\n",
    "\n",
    "            # Shift the \"Date\" column up by one row to align data with the next date\n",
    "            temp[\"Date\"] = temp[\"Date\"].shift(-1)\n",
    "\n",
    "            # Set the last row's \"Date\" to today's date to capture current data\n",
    "            temp.iloc[-1, temp.columns.get_loc(\"Date\")] = pd.to_datetime(date.today())\n",
    "\n",
    "            # Unpivot the table to return it to long format with \"Date\", \"Symbol\", and current column values\n",
    "            temp = temp.melt(id_vars='Date', value_name=col)\n",
    "\n",
    "            # Drop rows with missing values and reset the index\n",
    "            temp = temp.dropna().reset_index(drop=True)\n",
    "\n",
    "            # Set the index to \"Date\" and \"Symbol\" for each transformed column and add to the result list\n",
    "            result.append(temp.set_index([\"Date\", \"Symbol\"]))\n",
    "\n",
    "        # Concatenate all transformed columns along the columns axis to create a combined dataframe\n",
    "        scores = pd.concat(result, axis=1).reset_index()\n",
    "\n",
    "        # Sort the final dataframe by \"Symbol\" and \"Date\" columns for organized viewing and reset the index\n",
    "        scores = scores.sort_values([\"Symbol\", \"Date\"]).reset_index(drop=True)\n",
    "        scores.columns = scores.columns.str.replace(\"Peer\", self.peer)\n",
    "\n",
    "        ## Store the Factor Data in dictionary.\n",
    "        self.factorData[\"LowVol\"] = scores.copy()\n",
    "\n",
    "        ## Filter the data for latest update or values.\n",
    "        self.recentFactorUpdate[\"LowVol\"] = scores[scores[\"Date\"] == scores[\"Date\"].max()].reset_index(drop = True)\n",
    "        \n",
    "        ## Combining the Factors in one Dataframe\n",
    "        self.stockFactors = pd.concat([self.recentFactorUpdate[key].set_index([\"Date\", \"Symbol\"]) for key in self.recentFactorUpdate.keys()], axis = 1)\n",
    "        self.stockFactors.reset_index(inplace = True)\n",
    "\n",
    "        print(\"### LOWVOL COMPLETE ###\")\n",
    "\n",
    "        del priceData, themeScore, df, scores, result\n",
    "\n",
    "    def generate_AM(self,):\n",
    "\n",
    "        # Function to calculate log returns\n",
    "        def calculate_log_returns(df):\n",
    "            df['LogReturn'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "            return df.dropna()\n",
    "\n",
    "        # Function to calculate annualized standard deviation\n",
    "        def calculate_annualized_std(df, window=252):\n",
    "            return df['LogReturn'].rolling(window).std() * np.sqrt(window)\n",
    "\n",
    "        # Function to calculate momentum ratios\n",
    "        def calculate_momentum_ratios(series, period):\n",
    "            return series / series.shift(period) - 1\n",
    "\n",
    "        ################\n",
    "        ## PRICE DATA ##\n",
    "        ################\n",
    "        # Check if stock price data is loaded; if not, read it\n",
    "        if self.stockPriceData is None:\n",
    "            self.__readPriceData()\n",
    "\n",
    "        ## Drop the Unnecessary COlumns\n",
    "        priceData = self.stockPriceData.drop(columns = [\"Open\", \"High\", \"Low\", \"Volume\", \"Mcap\"]).copy()\n",
    "        \n",
    "        ## Filter the Symbol which are present in strategy universe\n",
    "        priceData = priceData[priceData[\"Symbol\"].isin(self.strategyUniverse[\"Symbol\"].unique())].reset_index(drop = True)\n",
    "\n",
    "        # Define periods for momentum ratios (in trading days)\n",
    "        periods = {\n",
    "            'MR1': 21,   # 1 month\n",
    "            'MR2': 42,   # 2 months\n",
    "            'MR3': 63,   # 3 months\n",
    "            'MR6': 126,  # 6 months\n",
    "            'MR12': 252, # 12 months\n",
    "        }\n",
    "\n",
    "        # Apply log return calculation\n",
    "        priceData = priceData.groupby('Symbol', group_keys=False).apply(calculate_log_returns)\n",
    "    \n",
    "        # Calculate momentum ratios for each period\n",
    "        for label, period in periods.items():\n",
    "            priceData[label] = priceData.groupby('Symbol')['Close'].transform(lambda x: calculate_momentum_ratios(x, period))\n",
    "\n",
    "        # momentum raw data\n",
    "        self.rawFactorData['momentum'] = priceData.copy()\n",
    "\n",
    "        # Calculate annualized standard deviation\n",
    "        priceData['AnnualizedStd'] = priceData.groupby('Symbol', group_keys=False).apply(calculate_annualized_std)\n",
    "\n",
    "        # Normalize the momentum ratios by dividing by the annualized standard deviation\n",
    "        for label in periods.keys():\n",
    "            priceData[label] /= priceData['AnnualizedStd']\n",
    "\n",
    "        priceData = priceData.sort_values([\"Date\", \"Symbol\"]).reset_index(drop = True)\n",
    "\n",
    "        # Calculate the mean and std deviation of each momentum ratio across the universe\n",
    "        for label in periods.keys():\n",
    "            priceData[f'mu_{label}'] = priceData.groupby('Date')[label].transform(lambda x: x.mean())\n",
    "            priceData[f'sigma_{label}'] = priceData.groupby('Date')[label].transform(lambda x: x.std())\n",
    "\n",
    "        # Calculate Z-scores for each period\n",
    "        for label in periods.keys():\n",
    "            priceData[f'Z_{label}'] = (priceData[label] - priceData[f'mu_{label}']) / priceData[f'sigma_{label}']\n",
    "\n",
    "        # Define specific combinations for which to calculate the final Z-scores\n",
    "        metrics = ['Z_MR1', 'Z_MR2', 'Z_MR3', 'Z_MR6', 'Z_MR12']\n",
    "\n",
    "        # Weighted average Z-score\n",
    "        priceData[\"WtdZScore\"] =priceData[metrics].mean(axis= 1)\n",
    "\n",
    "        # Normalized momentum score\n",
    "        priceData[f'Momentum'] = np.where(priceData[f'WtdZScore'] >= 0,\n",
    "                                                1 + priceData[f'WtdZScore'],\n",
    "                                                (1 - priceData[f'WtdZScore']) ** -1)\n",
    "\n",
    "        ## Filter the Required Columns  \n",
    "        priceData = priceData.filter(items = [\"Date\", \"Symbol\", \"Momentum\"])\n",
    "\n",
    "        ## Merge the Computed Metrics against the symbol on each day's universe\n",
    "        priceData = pd.merge(self.strategyUniverse, priceData, on = [\"Date\", \"Symbol\"], how = \"left\")\n",
    "\n",
    "        ## Computing the Percentile Score of the Symbols on each Date\n",
    "        priceData[\"Momentum\"] = priceData.groupby([\"Date\"])[\"Momentum\"].rank(ascending = True, pct = True)\n",
    "\n",
    "        ## Computing the Aggreate rank of Peer (Theme / Sector / GICS) on each date using Symbol\n",
    "        priceData[\"PeerMomentum\"] = priceData.groupby([\"Date\", \"Peer\"])[\"Momentum\"].transform(lambda x: x.mean())\n",
    "\n",
    "        ## Re-Ranki The Agg.Score of Peer (Theme / Sector / GICS) on each Date.\n",
    "        priceData[\"PeerMomentum\"] = priceData.groupby([\"Date\"])[\"PeerMomentum\"].rank(ascending = True, pct = True)\n",
    "\n",
    "        ## Filter the data after year 2006\n",
    "        priceData = priceData[priceData[\"Date\"] >= \"2006-01-01\"].reset_index(drop = True)\n",
    "         \n",
    "        # Initialize an empty list to store the transformed data for each volatility column\n",
    "        result = list()\n",
    "\n",
    "        # Iterate over each volatility-related column\n",
    "        for col in [\"Momentum\", \"PeerMomentum\"]:\n",
    "\n",
    "            # Filter the dataframe to keep only the \"Symbol\", \"Date\", and current column\n",
    "            temp = priceData.filter([\"Symbol\", \"Date\", col])\n",
    "\n",
    "            # Pivot the table to have dates as rows and symbols as columns, with values from the current column\n",
    "            temp = temp.pivot_table(index='Date', columns='Symbol', values=col).reset_index()\n",
    "\n",
    "            # Shift the \"Date\" column up by one row to align data with the next date\n",
    "            temp[\"Date\"] = temp[\"Date\"].shift(-1)\n",
    "\n",
    "            # Set the last row's \"Date\" to today's date to capture current data\n",
    "            temp.iloc[-1, temp.columns.get_loc(\"Date\")] = pd.to_datetime(date.today())\n",
    "\n",
    "            # Unpivot the table to return it to long format with \"Date\", \"Symbol\", and current column values\n",
    "            temp = temp.melt(id_vars='Date', value_name=col)\n",
    "\n",
    "            # Drop rows with missing values and reset the index\n",
    "            temp = temp.dropna().reset_index(drop=True)\n",
    "\n",
    "            # Filter data to include only records from January 1, 2006, onwards and reset the index\n",
    "            temp = temp[temp[\"Date\"] >= \"2006-01-01\"].reset_index(drop=True).copy()\n",
    "\n",
    "            # Set the index to \"Date\" and \"Symbol\" for each transformed column and add to the result list\n",
    "            result.append(temp.set_index([\"Date\", \"Symbol\"]))\n",
    "\n",
    "        \n",
    "        # Concatenate all transformed columns along the columns axis to create a combined dataframe\n",
    "        scores = pd.concat(result, axis=1).reset_index()\n",
    "        scores.columns = scores.columns.str.replace(\"Momentum\", \"AM\").str.replace(\"Peer\", self.peer)\n",
    "\n",
    "        # Sort the final dataframe by \"Symbol\" and \"Date\" columns for organized viewing and reset the index\n",
    "        scores = scores.sort_values([\"Symbol\", \"Date\"]).reset_index(drop=True)\n",
    "\n",
    "        ## Store the Factor Data in dictionary.\n",
    "        self.factorData[\"AM\"] = scores.copy()\n",
    "\n",
    "        ## Filter the data for latest update or values.\n",
    "        self.recentFactorUpdate[\"AM\"] = scores[scores[\"Date\"] == scores[\"Date\"].max()].reset_index(drop = True)\n",
    "        \n",
    "        ## Combining the Factors in one Dataframe\n",
    "        self.stockFactors = pd.concat([self.recentFactorUpdate[key].set_index([\"Date\", \"Symbol\"]) for key in self.recentFactorUpdate.keys()], axis = 1)\n",
    "        self.stockFactors.reset_index(inplace = True)\n",
    "\n",
    "        print(\"### MOMENTUM COMPLETE ###\")\n",
    "\n",
    "        del priceData, result, temp, scores\n",
    "    \n",
    "    def generate_LTMA(self,):\n",
    "\n",
    "        ## If Price is ot imported form Databse, then read it from DB.\n",
    "        if self.stockPriceData is None:\n",
    "            self.__readPriceData()\n",
    "        \n",
    "        ## Drop the Unnecessary COlumns\n",
    "        priceData = self.stockPriceData.drop(columns = [\"Open\", \"High\", \"Low\", \"Volume\", \"Mcap\"]).copy()\n",
    "        \n",
    "        ## Filter the Symbol which are present in strategy universe\n",
    "        priceData = priceData[priceData[\"Symbol\"].isin(self.strategyUniverse[\"Symbol\"].unique())].reset_index(drop = True)\n",
    "\n",
    "        # Sort the Dataframe by Symbol and Date, Reset the index\n",
    "        priceData = priceData.sort_values([\"Symbol\", \"Date\"]).reset_index(drop = True)\n",
    "\n",
    "        # Calculate 1-year, 3-year, and 5-year percentage changes in stock closing prices.\n",
    "        for key in [3, 6, 12, 18]:\n",
    "            priceData[f\"{key}_return\"] = priceData.groupby('Symbol')[\"Close\"].pct_change(22*key)\n",
    "            priceData[f\"{key}_vol\"] = priceData.groupby('Symbol')[\"Close\"].transform(lambda x: x.pct_change().rolling(key*22).std())\n",
    "            priceData[f\"{key}_vol\"] *= np.sqrt(252)\n",
    "            priceData[f\"{key}_sharpe\"] = priceData[f\"{key}_return\"] / priceData[f\"{key}_vol\"]\n",
    "            priceData.drop(columns = [f\"{key}_return\", f\"{key}_vol\"], inplace = True)\n",
    "\n",
    "        ## Sort and reset the index\n",
    "        priceData.sort_values(\"Date\", inplace = True)\n",
    "    \n",
    "        ## Merge the Computed Metrics against the symbol on each day's universe\n",
    "        priceData = pd.merge(self.strategyUniverse, priceData, on = [\"Date\", \"Symbol\"], how = \"left\")\n",
    "\n",
    "        ## Assigning the rank to each 500 companies on particular date.\n",
    "        for key in [3, 6, 12, 18]:\n",
    "            priceData[f\"{key}_Rank\"]=priceData.groupby('Date')[f\"{key}_sharpe\"].rank(pct=True)\n",
    "\n",
    "        ## Aggregate the rank for composite score.\n",
    "        priceData[\"LTMA\"] = priceData[[col for col in priceData.columns if \"Rank\" in col]].mean(axis = 1, skipna = False)\n",
    "\n",
    "        ## Filter out the necessary columns\n",
    "        priceData = priceData.filter(items = [\"Date\", \"Symbol\", \"LTMA\"])\n",
    "\n",
    "        ## Convert the long from Dataframe to wide form dataframe\n",
    "        priceData = priceData.pivot_table(index='Date',columns='Symbol',values='LTMA').reset_index()\n",
    "        ## Shifting the Date column\n",
    "        priceData[\"Date\"] = priceData[\"Date\"].shift(-1)\n",
    "        ## Fill NaN value of date with todays date.\n",
    "        priceData.iloc[-1, priceData.columns.get_loc(\"Date\")] = pd.to_datetime(date.today())\n",
    "\n",
    "        ## Re-converting the wide form dataframe to long form dataframe\n",
    "        priceData = priceData.melt(id_vars='Date', value_name = \"LTMA\")\n",
    "        ## Drop na and reset the index\n",
    "        priceData = priceData.dropna().reset_index(drop = True)\n",
    "        priceData = priceData[priceData[\"Date\"] >= \"2006-01-01\"].reset_index(drop = True).copy()\n",
    "\n",
    "        ## Store the Factor Data in dictionary.\n",
    "        self.factorData[\"LTMA\"] = priceData.copy()\n",
    "        ## Filter the data for latest update or values.\n",
    "        self.recentFactorUpdate[\"LTMA\"] = priceData[priceData[\"Date\"] == priceData[\"Date\"].max()].reset_index(drop = True)\n",
    "        ## Combining the Factors in one Dataframe\n",
    "        self.stockFactors = pd.concat([self.recentFactorUpdate[key].set_index([\"Date\", \"Symbol\"]) for key in self.recentFactorUpdate.keys() if key != \"Theme\"], axis = 1)\n",
    "        self.stockFactors.reset_index(inplace = True)\n",
    "\n",
    "        print(\"### LTMA COMPLETE ###\")\n",
    "\n",
    "        del priceData\n",
    "\n",
    "    def generate_LTM(self,):\n",
    "\n",
    "        ## If Price is ot imported form Databse, then read it from DB.\n",
    "        if self.stockPriceData is None:\n",
    "            self.__readPriceData()\n",
    "\n",
    "        ## Set the maping dictionary for LTM Factor.\n",
    "        mapper = periodMapping[\"LTM\"]\n",
    "\n",
    "        ## Function to calculate 3-year return after shifting by 100 days\n",
    "        def rollingReturn(group, shift, period, key):\n",
    "            group[f'{key}_Return'] = group['Close'].ffill().shift(shift).pct_change(period, fill_method = None)\n",
    "            return group\n",
    "            \n",
    "        ## Drop the Unnecessary COlumns\n",
    "        priceData = self.stockPriceData.drop(columns = [\"Open\", \"High\", \"Low\", \"Volume\", \"Mcap\"]).copy()\n",
    "        \n",
    "        ## Filter the Symbol which are present in strategy universe\n",
    "        priceData = priceData[priceData[\"Symbol\"].isin(self.strategyUniverse[\"Symbol\"].unique())].reset_index(drop = True)\n",
    "\n",
    "        # Sort the Dataframe by Symbol and Date, Reset the index\n",
    "        priceData = priceData.sort_values([\"Symbol\", \"Date\"]).reset_index(drop = True)\n",
    "\n",
    "        # Calculate 1-year, 3-year, and 5-year percentage changes in stock closing prices.\n",
    "        for key in mapper.keys():\n",
    "            priceData = priceData.groupby('Symbol').apply(rollingReturn, shift = mapper[key][\"S\"], \n",
    "                                                            period = mapper[key][\"F\"], \n",
    "                                                            key = key, include_groups = False).reset_index(level = 0)\n",
    "\n",
    "        ## Sort and reset the index\n",
    "        priceData.sort_values(\"Date\", inplace = True)\n",
    "    \n",
    "        ## Merge the Computed Metrics against the symbol on each day's universe\n",
    "        priceData = pd.merge(self.strategyUniverse, priceData, on = [\"Date\", \"Symbol\"], how = \"left\")\n",
    "\n",
    "        ## Assigning the rank to each 500 companies on particular date.\n",
    "        for key in mapper.keys():\n",
    "            priceData[f\"{key}_Rank\"]=priceData.groupby('Date')[f\"{key}_Return\"].rank(pct=True)\n",
    "        \n",
    "        ## Aggregate the rank for composite score.\n",
    "        priceData[\"LTM\"] = priceData[[col for col in priceData.columns if \"Rank\" in col]].mean(axis = 1, skipna = False)\n",
    "\n",
    "        ## Filter out the necessary columns\n",
    "        priceData = priceData.filter(items = [\"Date\", \"Symbol\", \"LTM\"])\n",
    "\n",
    "        ## Convert the long from Dataframe to wide form dataframe\n",
    "        priceData = priceData.pivot_table(index='Date',columns='Symbol',values='LTM').reset_index()\n",
    "        \n",
    "        ## Shifting the Date column\n",
    "        priceData[\"Date\"] = priceData[\"Date\"].shift(-1)\n",
    "\n",
    "        ## Fill NaN value of date with todays date.\n",
    "        priceData.iloc[-1, priceData.columns.get_loc(\"Date\")] = pd.to_datetime(date.today())\n",
    "\n",
    "        ## Re-converting the wide form dataframe to long form dataframe\n",
    "        priceData = priceData.melt(id_vars='Date', value_name = \"LTM\")\n",
    "\n",
    "        ## Drop na and reset the index\n",
    "        priceData = priceData.dropna().reset_index(drop = True)\n",
    "        priceData = priceData[priceData[\"Date\"] >= \"2006-01-01\"].reset_index(drop = True).copy()\n",
    "\n",
    "        ## Store the Factor Data in dictionary.\n",
    "        self.factorData[\"LTM\"] = priceData.copy()\n",
    "\n",
    "        ## Filter the data for latest update or values.\n",
    "        self.recentFactorUpdate[\"LTM\"] = priceData[priceData[\"Date\"] == priceData[\"Date\"].max()].reset_index(drop = True)\n",
    "        \n",
    "        ## Combining the Factors in one Dataframe\n",
    "        self.stockFactors = pd.concat([self.recentFactorUpdate[key].set_index([\"Date\", \"Symbol\"]) for key in self.recentFactorUpdate.keys() if key != \"Theme\"], axis = 1)\n",
    "        self.stockFactors.reset_index(inplace = True)\n",
    "\n",
    "        print(\"### LTM COMPLETE ###\")\n",
    "\n",
    "        del priceData\n",
    "\n",
    "    def generate_EM(self):\n",
    "        # RSI function\n",
    "        def rsi(closes, n):\n",
    "            diff_serie = closes.diff()\n",
    "            gain = diff_serie.where(diff_serie > 0, 0)\n",
    "            loss = -diff_serie.where(diff_serie < 0, 0)\n",
    "            avg_gain = gain.rolling(window=n).mean()\n",
    "            avg_loss = loss.rolling(window=n).mean()\n",
    "            rs = avg_gain / avg_loss\n",
    "            rsi = 100 - (100 / (1 + rs))\n",
    "            return rsi.fillna(0)\n",
    "\n",
    "        ################\n",
    "        ## PRICE DATA ##\n",
    "        ################\n",
    "        if self.stockPriceData is None:\n",
    "            self.__readPriceData()\n",
    "\n",
    "        ## Drop the Unnecessary COlumns\n",
    "        priceData = self.stockPriceData.drop(columns = [\"Open\", \"High\", \"Low\", \"Volume\", \"Mcap\"]).copy()\n",
    "        \n",
    "        ## Filter the Symbol which are present in strategy universe\n",
    "        priceData = priceData[priceData[\"Symbol\"].isin(self.strategyUniverse[\"Symbol\"].unique())].reset_index(drop = True)\n",
    "        priceData = priceData.sort_values([\"Symbol\", \"Date\"]).reset_index(drop = True)\n",
    "        \n",
    "        ## Calculate the 50 and 200 day moving average\n",
    "        priceData['200_ma'] = priceData.groupby('Symbol')['Close'].transform(lambda x: x.rolling(200).mean())\n",
    "        priceData['50_ma'] = priceData.groupby('Symbol')['Close'].transform(lambda x: x.rolling(50).mean())\n",
    "\n",
    "        # Calculate Close to 200DMA and 50DMA to 200DMA Ratio.\n",
    "        priceData['200_ma_ratio'] = priceData['Close'] / priceData['200_ma']\n",
    "        priceData['50_200_ratio'] = priceData['50_ma'] / priceData['200_ma']\n",
    "\n",
    "        # Compute the RSI\n",
    "        priceData['rsi'] = priceData.groupby('Symbol')['Close'].transform(lambda x: rsi(x, 14))\n",
    "\n",
    "        ## Compute the rank for each of the sub-factors\n",
    "        priceData['200_ma_ratio_score'] = priceData.groupby('Date')['200_ma_ratio'].rank(ascending=False, pct=True)\n",
    "        priceData['50_200_score'] = priceData.groupby('Date')['50_200_ratio'].rank(ascending=False, pct=True)\n",
    "        priceData['rsi_score'] = priceData.groupby('Date')['rsi'].rank(ascending=False, pct=True)\n",
    "\n",
    "        # Calculate final scores and sor the dataframe\n",
    "        priceData['finalScore'] = priceData[['200_ma_ratio_score', '50_200_score', 'rsi_score']].sum(axis = 1, skipna = False)\n",
    "\n",
    "        ## Merge the Computed Metrics against the symbol on each day's universe\n",
    "        priceData = pd.merge(self.strategyUniverse, priceData, on = [\"Date\", \"Symbol\"], how = \"left\")\n",
    "\n",
    "        ## Rank the top500 companies and filter the necessary columns\n",
    "        priceData['AM_New'] = priceData.groupby('Date')['finalScore'].rank(ascending=False, pct=True)\n",
    "        priceData = priceData.filter(items= ['Date','Symbol','AM_New'])\n",
    "\n",
    "        ## Filter the data after year 2006 and sor the dataframe.\n",
    "        priceData = priceData[priceData[\"Date\"] >= \"2006-01-01\"].copy()\n",
    "        priceData = priceData.sort_values([\"Symbol\", \"Date\"]).reset_index(drop = True)\n",
    "    \n",
    "        ## Convert the long from Dataframe to wide form dataframe\n",
    "        priceData = priceData.pivot_table(index='Date',columns='Symbol',values='AM_New').reset_index()\n",
    "        ## Shifting the Date column\n",
    "        priceData[\"Date\"] = priceData[\"Date\"].shift(-1)\n",
    "        ## Fill NaN value of date with todays date.\n",
    "        priceData.iloc[-1, priceData.columns.get_loc(\"Date\")] = pd.to_datetime(date.today())\n",
    "        ## Re-converting the wide form dataframe to long form dataframe\n",
    "        priceData = priceData.melt(id_vars='Date', value_name= \"EM\")\n",
    "        ## Drop na and reset the index\n",
    "        priceData = priceData.dropna().reset_index(drop = True)\n",
    "\n",
    "        ## Store the Factor Data in dictionary.\n",
    "        self.factorData[\"EM\"] = priceData.copy()\n",
    "        ## Filter the data for latest update or values.\n",
    "        self.recentFactorUpdate[\"EM\"] = priceData[priceData[\"Date\"] == priceData[\"Date\"].max()].reset_index(drop = True)\n",
    "        ## Combining the Factors in one Dataframe\n",
    "        self.stockFactors = pd.concat([self.recentFactorUpdate[key].set_index([\"Date\", \"Symbol\"]) for key in self.recentFactorUpdate.keys()], axis = 1)\n",
    "        self.stockFactors.reset_index(inplace = True)\n",
    "\n",
    "        print(\"### EM COMPLETE ###\")\n",
    "\n",
    "        del priceData\n",
    "\n",
    "    def generate_Dividend(self, valueYieldCall = False):\n",
    "\n",
    "        def previousQuarter(currDate):\n",
    "            if pd.Period(currDate,freq = \"Q\").end_time.date() == currDate.date():\n",
    "                return currDate\n",
    "            else:\n",
    "                return currDate - pd.offsets.QuarterEnd()\n",
    "            \n",
    "        #########################\n",
    "        ## COMPANY MASTER DATA ##\n",
    "        #########################\n",
    "        if self.companyMaster is None:\n",
    "            self.__readCompanyMaster()\n",
    "\n",
    "        if self.profitLossGrowth is None:\n",
    "            self.__readProfitLossGrowth() \n",
    "    \n",
    "        if self.stockPriceData is None:\n",
    "            self.__readPriceData()\n",
    "            \n",
    "        # Fetch the rebalance dates for the last 100 years from the \"GrowthDate\" table in the database\n",
    "        # rebalanceDates = self.__dataCursor.fetch_data_from_database(table_name = \"GrowthDate\", no_of_years=100)\n",
    "        rebalanceDates = pd.read_csv('growth_date_latest.csv')\n",
    "        rebalanceDates['Date'] = pd.to_datetime(rebalanceDates['Date'])\n",
    "\n",
    "        # Create a copy of the strategy universe (likely a dataframe or similar structure) to work with for company composition\n",
    "        composition  = self.strategyUniverse.copy()\n",
    "\n",
    "        # Extract the quarter and date values from the rebalanceDates dataframe\n",
    "        rebalanceQtr = list(rebalanceDates['Quarter'])\n",
    "        rebalanceDate = list(rebalanceDates['Date'])\n",
    "\n",
    "        # Create a dictionary mapping each date to its corresponding quarter and vice versa\n",
    "        qtrDateDict = dict(zip(rebalanceDate, rebalanceQtr))\n",
    "        dateQtrDict = dict(zip(rebalanceQtr, rebalanceDate))\n",
    "\n",
    "        # Add a new column 'Quarter' to the composition dataframe by mapping the 'Date' column to the respective quarter using qtrDateDict\n",
    "        composition['Quarter'] = composition['Date'].map(qtrDateDict)\n",
    "\n",
    "        # Filter the necessary columns ('FINCODE', 'Date_End', 'Dividend payout ratio', 'PAT') from the profitLossGrowth dataframe\n",
    "        div_qtr = self.profitLossGrowth.filter(items=[\"FINCODE\", \"Date_End\", \"Dividend payout ratio\", \"PAT\"])\n",
    "\n",
    "        # Merge div_qtr with companyMaster on the 'FINCODE' column to include company information\n",
    "        div_qtr = pd.merge(div_qtr, self.companyMaster, on=\"FINCODE\", how=\"inner\")\n",
    "\n",
    "        # Convert the 'Date_End' column to datetime format\n",
    "        div_qtr[\"Date_End\"] = pd.to_datetime(div_qtr['Date_End'], format=\"%Y%m\")\n",
    "\n",
    "        # Shift 'Date_End' to the respective month-end date\n",
    "        div_qtr[\"MonthEnd\"] = div_qtr[\"Date_End\"] + pd.offsets.MonthEnd()\n",
    "\n",
    "        # Create a new column 'QuarterEnd' to calculate the end of the financial quarter based on the 'MonthEnd'\n",
    "        div_qtr[\"QuarterEnd\"] = div_qtr[\"MonthEnd\"].apply(previousQuarter) \n",
    "\n",
    "        # Convert the 'QuarterEnd' to the Indian Financial Year format, which ends in March\n",
    "        div_qtr[\"YearQuarter\"] = div_qtr[\"QuarterEnd\"].dt.to_period(\"Q-MAR\")\n",
    "\n",
    "        # Extract the quarter and year from 'YearQuarter' and create a new 'Quarter' column\n",
    "        div_qtr[\"Quarter\"] = div_qtr[\"YearQuarter\"].astype(str).str[-2:] + div_qtr[\"YearQuarter\"].astype(str).str[:4]\n",
    "\n",
    "        # Extract the year from 'YearQuarter' and create a 'Year' column\n",
    "        div_qtr[\"Year\"] = div_qtr[\"YearQuarter\"].astype(str).str[:4].astype(int)\n",
    "\n",
    "        # Sort the dataframe by 'YearQuarter' in ascending order for chronological order\n",
    "        div_qtr.sort_values(\"YearQuarter\", ascending=True, inplace=True)\n",
    "        div_qtr.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # Calculate the dividend by multiplying 'PAT' (Profit After Tax) by the 'Dividend payout ratio'\n",
    "        div_qtr['dividend'] = div_qtr['PAT'] * div_qtr['Dividend payout ratio']\n",
    "\n",
    "        # Sort by 'Symbol' and 'MonthEnd' for proper chronological order within each company\n",
    "        div_qtr.sort_values(['Symbol', 'MonthEnd'], inplace=True)\n",
    "\n",
    "        # Calculate the rolling dividend over the last 4 quarters for each company\n",
    "        div_qtr['Rollingdividend'] = div_qtr.groupby('Symbol')['dividend'].transform(lambda x: x.rolling(4).sum())\n",
    "\n",
    "        # Sort the dataframe again by 'Symbol' and 'YearQuarter', and reset the index\n",
    "        div_qtr.sort_values([\"Symbol\", \"YearQuarter\"], inplace=True)\n",
    "        div_qtr.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # Map the 'Quarter' to its respective 'Date' using the dateQtrDict for final data\n",
    "        div_qtr['Date'] = div_qtr['Quarter'].map(dateQtrDict)\n",
    "\n",
    "        # Merge the 'div_qtr' with the 'composition' dataframe on 'Symbol', 'Date', and 'Quarter' to get the final composition\n",
    "        div_qtr = pd.merge(div_qtr, composition, on=['Symbol', 'Date', 'Quarter'])\n",
    "\n",
    "        # Extract the year from the 'Date' column and assign it to a new 'Year' column\n",
    "        div_qtr['Year'] = div_qtr['Date'].dt.year\n",
    "\n",
    "        # Sort the dataframe by 'Date' and 'Symbol' for easier analysis and reset the index\n",
    "        div_qtr = div_qtr.sort_values([\"Date\", \"Symbol\"]).reset_index(drop=True)\n",
    "\n",
    "        # Create a dataframe 'divRank' containing only relevant columns like dividend, PAT, and Rollingdividend\n",
    "        divRank = div_qtr.filter(items=[\"Date\", \"Symbol\", \"dividend\", \"PAT\", 'Dividend payout ratio', 'Rollingdividend'])\n",
    "\n",
    "        # Map the 'divRank' to the 'composition' dataframe on 'Date' and 'Symbol' to align dividends with composition details\n",
    "        divRank = pd.merge(composition, divRank, on=['Date', 'Symbol'], how='left')\n",
    "\n",
    "        # Sort the 'divRank' dataframe by 'Symbol' and 'Date' for proper chronological order\n",
    "        divRank = divRank.sort_values([\"Symbol\", \"Date\"]).reset_index(drop=True)\n",
    "\n",
    "        # Forward fill missing values in 'dividend', 'PAT', 'Dividend payout ratio', and 'Rollingdividend' within each 'Symbol'\n",
    "        divRank[['dividend', 'PAT', 'Dividend payout ratio', 'Rollingdividend']] = divRank.groupby('Symbol')[['dividend', 'PAT', 'Dividend payout ratio', 'Rollingdividend']].ffill()\n",
    "\n",
    "        # Calculate the dividend yield for each company as (Rollingdividend / Market Capitalization) * 100\n",
    "        divRank['yield'] = (divRank['Rollingdividend'] / divRank['Mcap']) * 100\n",
    "\n",
    "        # Calculate the 4-quarter rolling average of the dividend yield for each company\n",
    "        divRank[\"yield\"] = divRank.groupby(\"Symbol\")[\"yield\"].transform(lambda x: x.rolling(4).mean())\n",
    "\n",
    "        # Final dataframe 'divRankFinal' contains 'Date', 'Symbol', and 'Dividend' columns, with no missing values\n",
    "        divRankFinal = divRank[['Date', 'Symbol', 'yield']].dropna()\n",
    "\n",
    "        # dividend raw data\n",
    "        self.rawFactorData['dividend'] = divRankFinal.copy()\n",
    "\n",
    "        # Rename the 'yield' column to 'Dividend' for better clarity\n",
    "        divRankFinal.rename(columns={'yield': 'Dividend'}, inplace=True)\n",
    "\n",
    "        if valueYieldCall:\n",
    "            return divRankFinal\n",
    "\n",
    "        ## Convert the long from Dataframe to wide form dataframe\n",
    "        divRankFinal = divRankFinal.pivot_table(index='Date',columns='Symbol',values='Dividend').reset_index()\n",
    "\n",
    "        ## Shifting the Date column\n",
    "        divRankFinal[\"Date\"] = divRankFinal[\"Date\"].shift(-1)\n",
    "        \n",
    "        ## Fill NaN value of date with todays date.\n",
    "        divRankFinal.iloc[-1, divRankFinal.columns.get_loc(\"Date\")] = pd.to_datetime(date.today())\n",
    "        \n",
    "        ## Re-converting the wide form dataframe to long form dataframe\n",
    "        divRankFinal = divRankFinal.melt(id_vars='Date', value_name= \"Dividend\")\n",
    "\n",
    "        ## Drop na and reset the index\n",
    "        divRankFinal = divRankFinal.dropna().reset_index(drop = True)\n",
    "        divRankFinal = divRankFinal[divRankFinal[\"Date\"] >= \"2006-01-01\"].reset_index(drop = True).copy()\n",
    "\n",
    "        self.div= divRankFinal.copy()\n",
    "\n",
    "        divRankFinal['Dividend']=divRankFinal.groupby('Date')['Dividend'].rank(pct=True)\n",
    "\n",
    "        ## Store the Factor Data in dictionary.\n",
    "        self.factorData[\"Dividend\"] = divRankFinal.copy()\n",
    "        \n",
    "        ## Filter the data for latest update or values.\n",
    "        self.recentFactorUpdate[\"Dividend\"] = divRankFinal[divRankFinal[\"Date\"] == divRankFinal[\"Date\"].max()].reset_index(drop = True)\n",
    "        \n",
    "        ## Combining the Factors in one Dataframe\n",
    "        self.stockFactors = pd.concat([self.recentFactorUpdate[key].set_index([\"Date\", \"Symbol\"]) for key in self.recentFactorUpdate.keys()], axis = 1)\n",
    "        self.stockFactors.reset_index(inplace = True)\n",
    "\n",
    "        print(\"### DIVIDEND COMPLETE ###\")\n",
    "\n",
    "        del div_qtr, divRank, divRankFinal\n",
    "\n",
    "    def generate_Growth(self,valueYieldCall = False):\n",
    "\n",
    "        ## FUnction to get the previous Quarter Date\n",
    "        def previousQuarter(currDate):\n",
    "            if pd.Period(currDate,freq = \"Q\").end_time.date() == currDate.date():\n",
    "                return currDate\n",
    "            else:\n",
    "                return currDate - pd.offsets.QuarterEnd()\n",
    "\n",
    "        #########################\n",
    "        ## COMPANY MASTER DATA ##\n",
    "        #########################\n",
    "        if self.companyMaster is None:\n",
    "            self.__readCompanyMaster()\n",
    "\n",
    "        if self.profitLossGrowth is None:\n",
    "            self.__readProfitLossGrowth()\n",
    "\n",
    "        if self.cashFlow is None:\n",
    "            self.__readCashFlow()\n",
    "\n",
    "        if self.stockPriceData is None:\n",
    "            self.__readPriceData()\n",
    "\n",
    "        if self.sectorData is None:\n",
    "            self.__readSectorData()\n",
    "\n",
    "\n",
    "        ######################\n",
    "        ## DATA PREPARATION ##\n",
    "        ######################\n",
    "        ## List of the Rebalance Dates\n",
    "        # rebalanceDates = self.__dataCursor.fetch_data_from_database(table_name = \"GrowthDate\", no_of_years=100)\n",
    "        rebalanceDates = pd.read_csv('growth_date_latest.csv')\n",
    "        rebalanceDates['Date'] = pd.to_datetime(rebalanceDates['Date'])\n",
    "        \n",
    "        # Identify Companies which have been in Top-500 historically\n",
    "        composition  = self.strategyUniverse.copy()\n",
    "        \n",
    "        ## List of Quarter and Dates\n",
    "        rebalanceQtr=list(rebalanceDates['Quarter'])\n",
    "        rebalanceDate=list(rebalanceDates['Date'])\n",
    "        \n",
    "        ## Mappinf Dictionary of Date to Quarter and vice versa\n",
    "        qtrDateDict=dict(zip(rebalanceDate,rebalanceQtr))\n",
    "        dateQtrDict=dict(zip(rebalanceQtr,rebalanceDate))\n",
    "\n",
    "        ## Quarter column in PriceData\n",
    "        composition['Quarter']=composition['Date'].map(qtrDateDict)\n",
    "\n",
    "        ############################\n",
    "        ## OPERATION ON CASH DATA ##\n",
    "        ############################\n",
    "        \n",
    "        ## Import the Cash data\n",
    "        cf_yearly = self.cashFlow.filter(items = [\"FINCODE\", \"Year_end\", \"Cash_from_Operation\"])\n",
    "        ## Map the Company Names\n",
    "        cf_yearly = pd.merge(cf_yearly, self.companyMaster, how = \"inner\", on = \"FINCODE\")\n",
    "        ## Convert the string into Datetime\n",
    "        cf_yearly[\"Year_end\"] = pd.to_datetime(cf_yearly['Year_end'],format=\"%Y%m\")\n",
    "        cf_yearly[\"Year\"] = cf_yearly[\"Year_end\"].dt.year\n",
    "        ## Shift the Date Year\n",
    "        cf_yearly['Year'] = np.where(cf_yearly['Year_end'].dt.month == 12, \n",
    "                                    (cf_yearly['Year'] + 1), \n",
    "                                    (cf_yearly['Year']))\n",
    "        cf_yearly[\"Year\"] = cf_yearly[\"Year\"] + 1\n",
    "        ## Sort the dataframe\n",
    "        cf_yearly.sort_values('Year',inplace=True)\n",
    "        cf_yearly.reset_index(drop = True, inplace = True)\n",
    "\n",
    "        self.cf_yearly = cf_yearly.copy()\n",
    "\n",
    "        ###################################\n",
    "        ## OPERATION ON PROFIT/LOSS DATA ##\n",
    "        ###################################\n",
    "        ## Read the Fundamental data of the comapnies - Growth Related\n",
    "        pl_yearly = self.profitLossGrowth.drop(columns = [ 'Debt/Equity Ratio', 'Adj_eps_abs', 'Dividend payout ratio']).copy()\n",
    "        ## Convert the string into Datetime\n",
    "        pl_yearly[\"Date_End\"] = pd.to_datetime(pl_yearly['Date_End'],format=\"%Y%m\")\n",
    "\n",
    "        ## Merge the Fundamental data with master list of the companies \n",
    "        pl_yearly = pd.merge(pl_yearly, self.companyMaster, on = \"FINCODE\", how = \"inner\")\n",
    "\n",
    "        ## Calculate the Operating_Margin and Gross_Margin\n",
    "        pl_yearly[\"Operating_Margin\"] = pl_yearly[\"OPERATING_PROFIT\"].div(pl_yearly[\"NET_SALES\"])\n",
    "        pl_yearly[\"Gross_Margin\"] = pl_yearly[\"GROSS_PROFIT\"].div(pl_yearly[\"NET_SALES\"])\n",
    "\n",
    "        # ## Shift Date to respective Month End date.\n",
    "        pl_yearly[\"MonthEnd\"] = pl_yearly[\"Date_End\"] + pd.offsets.MonthEnd()\n",
    "        pl_yearly[\"QuarterEnd\"] = pl_yearly[\"MonthEnd\"].apply(previousQuarter) \n",
    "\n",
    "        ## Convert the Dates as per Indian Financial Quarter\n",
    "        pl_yearly[\"YearQuarter\"] = pl_yearly[\"QuarterEnd\"].dt.to_period(\"Q-MAR\")\n",
    "        pl_yearly[\"Quarter\"] = pl_yearly[\"YearQuarter\"].astype(str).str[-2:] + pl_yearly[\"YearQuarter\"].astype(str).str[:4]\n",
    "        pl_yearly[\"Year\"] =pl_yearly[\"YearQuarter\"].astype(str).str[:4].astype(int)\n",
    "\n",
    "        ## sort the Data Frame based on Quarter\n",
    "        pl_yearly.sort_values(\"YearQuarter\", ascending = True, inplace = True)\n",
    "        pl_yearly.reset_index(drop = True, inplace = True)\n",
    "\n",
    "        self.pl_yearly = pl_yearly.copy()\n",
    "\n",
    "        ###############\n",
    "        ## COMBINING ##\n",
    "        ###############\n",
    "        ## Combine the Profit loss and Cash data\n",
    "        data = pd.merge(pl_yearly, cf_yearly, on = ['FINCODE',\"Symbol\", \"Year\"])\n",
    "\n",
    "        ## List of Factors\n",
    "        factorUniverse = ['PAT','OPERATING_PROFIT','GROSS_PROFIT','NET_SALES','EPS_DILUTED','PBT','Operating_Margin','Gross_Margin','Cash_from_Operation']\n",
    "        ## SUbste the columns\n",
    "        data = data[[\"Year\", \"YearQuarter\", \"Quarter\", \"Symbol\", \"Date_End\"] + factorUniverse]\n",
    "\n",
    "        # growth raw data\n",
    "        self.rawFactorData['growth'] = data.copy()\n",
    "\n",
    "        ## Sort the DataFrame\n",
    "        data.sort_values([\"Symbol\", \"YearQuarter\"], inplace = True)\n",
    "        data.reset_index(drop = True, inplace = True)\n",
    "\n",
    "        ## Calculating the Change in factors over a year\n",
    "        year_chg = lambda ser: (ser - ser.shift(4))/ser.abs().shift(4)\n",
    "        data[[f\"{c}_change\" for c in factorUniverse]] = data.groupby(\"Symbol\", group_keys=False)[factorUniverse].apply(year_chg)\n",
    "    \n",
    "        ## Mapping the Date column to Quarter Date\n",
    "        data['Date']=data['Quarter'].map(dateQtrDict)\n",
    "        \n",
    "        data=pd.merge(data,composition,on=['Symbol','Date','Quarter'])\n",
    "        data['Year']=data['Date'].dt.year\n",
    "\n",
    "        ## Compute the Absolute / Peer and Historical Rank\n",
    "        data = data.sort_values([\"Date\", \"Symbol\"]).reset_index(drop = True)\n",
    "\n",
    "        if valueYieldCall:\n",
    "            return data\n",
    "\n",
    "        data.drop(columns = [\"Date_End\"], inplace = True)\n",
    "\n",
    "        factorUniverse = [f\"{col}_change\" for col in factorUniverse]\n",
    "        data[\"AbsRank\"] = data.groupby(\"Date\")[factorUniverse].rank(pct = True).mean(axis = 1)\n",
    "        data[\"PeerRank\"] = data.groupby([\"Date\", \"Sector\"])[factorUniverse].rank(pct = True).mean(axis = 1)\n",
    "\n",
    "        data = data.sort_values([\"Symbol\", \"Date\"]).reset_index(drop = True)\n",
    "        data[\"HistRank\"] = data.groupby(\"Symbol\")[factorUniverse].rolling(window = 12, min_periods = 4).rank(pct = True).mean(axis = 1).reset_index(drop = True)\n",
    "        growthRank = data.filter(items = [\"Date\", \"Symbol\", \"AbsRank\", \"PeerRank\", \"HistRank\"])\n",
    "        growthRank[\"Rank\"] = growthRank[[ \"AbsRank\", \"PeerRank\", \"HistRank\"]].mean(axis = 1)\n",
    "        \n",
    "        ## Mapping the Growth score to each date\n",
    "        growthRank = pd.merge(composition,growthRank,on=['Date','Symbol'],how='left')\n",
    "        growthRank = growthRank.sort_values([\"Symbol\",  \"Date\"]).reset_index(drop = True)\n",
    "\n",
    "        ## Forward fill the growth score, as growth is for each quarter\n",
    "        growthRank[['AbsRank','PeerRank','HistRank','FinalRank']]=growthRank.groupby('Symbol')[['AbsRank','PeerRank','HistRank','Rank']].ffill()\n",
    "        growthRank['Growth'] = growthRank['PeerRank']*0.95 + growthRank['AbsRank']*0.05\n",
    "\n",
    "        ## Convert the long from Dataframe to wide form dataframe\n",
    "        growthRank = growthRank.pivot_table(index='Date',columns='Symbol',values='Growth').reset_index()\n",
    "        ## Shifting the Date column\n",
    "        growthRank[\"Date\"] = growthRank[\"Date\"].shift(-1)\n",
    "        ## Fill NaN value of date with todays date.\n",
    "        growthRank.iloc[-1, growthRank.columns.get_loc(\"Date\")] = pd.to_datetime(date.today())\n",
    "        ## Re-converting the wide form dataframe to long form dataframe\n",
    "        growthRank = growthRank.melt(id_vars='Date', value_name= \"Growth\")\n",
    "        ## Drop na and reset the index\n",
    "        growthRank = growthRank.dropna().reset_index(drop = True)\n",
    "        growthRank = growthRank[growthRank[\"Date\"] >= \"2006-01-01\"].reset_index(drop = True).copy()\n",
    "\n",
    "        ## Store the Factor Data in dictionary.\n",
    "        self.factorData[\"Growth\"] = growthRank.copy()\n",
    "        ## Filter the data for latest update or values.\n",
    "        self.recentFactorUpdate[\"Growth\"] = growthRank[growthRank[\"Date\"] == growthRank[\"Date\"].max()].reset_index(drop = True)\n",
    "        ## Combining the Factors in one Dataframe\n",
    "        self.stockFactors = pd.concat([self.recentFactorUpdate[key].set_index([\"Date\", \"Symbol\"]) for key in self.recentFactorUpdate.keys()], axis = 1)\n",
    "        self.stockFactors.reset_index(inplace = True)\n",
    "\n",
    "        print(\"### GROWTH COMPLETE ###\")\n",
    "\n",
    "    def generate_ValueYield(self):\n",
    "\n",
    "        ## Fetch the Stock Value Data\n",
    "        if self.stockValueData is None:\n",
    "            self.__readValueData()\n",
    "\n",
    "        ## Fetch the \n",
    "        if self.stockPriceData is None:\n",
    "            self.__readPriceData()\n",
    "\n",
    "        if self.sectorData is None:\n",
    "            self.__readSectorData()\n",
    "\n",
    "        \n",
    "        ### VALUE ###\n",
    "        # Filter the rows for Top-500 companies historically\n",
    "        valueData = self.stockValueData[self.stockValueData[\"Symbol\"].isin(self.strategyUniverse[\"Symbol\"].unique())].copy()\n",
    "\n",
    "        ## List of Fundamental Factor Value.\n",
    "        fundamentalValueFactor = [\"EV_EBITDA\", \"PS\", \"PB\", \"PE\"]\n",
    "        ## Replace -ve value with NaN value for all the fundamental value factor\n",
    "        for factor in fundamentalValueFactor:\n",
    "            valueData[factor] = np.where(valueData[factor] <= 0, np.nan, valueData[factor])\n",
    "            valueData[factor] = 1/valueData[factor]\n",
    "\n",
    "        ## Sort and reset the index\n",
    "        valueData.sort_values([\"Symbol\", \"Date\"], inplace = True)\n",
    "        valueData.reset_index(drop = True, inplace = True)\n",
    "\n",
    "\n",
    "        ### Dividend ###\n",
    "        dividendData = self.generate_Dividend(valueYieldCall = True)\n",
    "\n",
    "        ## EPS ###\n",
    "        epsData = self.generate_Growth(valueYieldCall = True)\n",
    "\n",
    "        ## ROE\n",
    "        roe = self.generate_QualityQuarter(valueYieldCall=True)\n",
    "        roe[\"Date_End\"] = pd.to_datetime(roe['Date_End'],format=\"%Y%m\")\n",
    "\n",
    "        growth = pd.merge(epsData[[\"Date\", \"Date_End\", \"Symbol\",\"EPS_DILUTED_change\"]], \n",
    "                          roe[[\"Date_End\", \"Symbol\", \"ROE_ttm\"]], on = [\"Date_End\", \"Symbol\"], how = \"left\")\n",
    "\n",
    "        final=pd.merge(valueData,dividendData,on=['Date','Symbol'])\n",
    "        final=pd.merge(final,growth,on=['Date','Symbol'],how='left')\n",
    "\n",
    "        final = final.sort_values([\"Symbol\",\"Date\"]).reset_index(drop = True)\n",
    "        final[['ROE_ttm', \"EPS_DILUTED_change\"]]=final.groupby('Symbol')[['ROE_ttm', \"EPS_DILUTED_change\"]].ffill()\n",
    "        final['PEG']=final['PE']/final['EPS_DILUTED_change']\n",
    "\n",
    "        factorUniverse = ['EV_EBITDA', 'PS','PB', 'PE', 'Dividend','PEG','ROE_ttm']\n",
    "\n",
    "        final = pd.merge(self.strategyUniverse, final[[\"Date\", \"Symbol\"] + factorUniverse], on = [\"Date\",\"Symbol\"], how = \"left\")\n",
    "\n",
    "        # valueyield raw data\n",
    "        self.rawFactorData['value_yield'] = final.copy()\n",
    "\n",
    "        final[\"AbsRank\"] = final.groupby(\"Date\")[factorUniverse].rank(pct = True).mean(axis = 1)\n",
    "        final[\"PeerRank\"] = final.groupby([\"Date\", \"Sector\"])[factorUniverse].rank(pct = True).mean(axis = 1)\n",
    "\n",
    "        final['ValueYield'] = final['PeerRank']*0.95 + final['AbsRank']*0.05\n",
    "        final['ValueABS'] = final['PeerRank']*0.05 + final['AbsRank']*0.95        \n",
    "\n",
    "        result = list()\n",
    "\n",
    "        # Iterate over each volatility-related column\n",
    "        for col in [\"ValueYield\", \"ValueABS\"]:\n",
    "\n",
    "            # Filter the dataframe to keep only the \"Symbol\", \"Date\", and current column\n",
    "            temp = final.filter([\"Symbol\", \"Date\", col])\n",
    "\n",
    "            # Pivot the table to have dates as rows and symbols as columns, with values from the current column\n",
    "            temp = temp.pivot_table(index='Date', columns='Symbol', values=col).reset_index()\n",
    "\n",
    "            # Shift the \"Date\" column up by one row to align data with the next date\n",
    "            temp[\"Date\"] = temp[\"Date\"].shift(-1)\n",
    "\n",
    "            # Set the last row's \"Date\" to today's date to capture current data\n",
    "            temp.iloc[-1, temp.columns.get_loc(\"Date\")] = pd.to_datetime(date.today())\n",
    "\n",
    "            # Unpivot the table to return it to long format with \"Date\", \"Symbol\", and current column values\n",
    "            temp = temp.melt(id_vars='Date', value_name=col)\n",
    "\n",
    "            # Drop rows with missing values and reset the index\n",
    "            temp = temp.dropna().reset_index(drop=True)\n",
    "\n",
    "            # Filter data to include only records from January 1, 2006, onwards and reset the index\n",
    "            temp = temp[temp[\"Date\"] >= \"2006-01-01\"].reset_index(drop=True).copy()\n",
    "\n",
    "            # Set the index to \"Date\" and \"Symbol\" for each transformed column and add to the result list\n",
    "            result.append(temp.set_index([\"Date\", \"Symbol\"]))\n",
    "\n",
    "\n",
    "        # Concatenate all transformed columns along the columns axis to create a combined dataframe\n",
    "        scores = pd.concat(result, axis=1).reset_index()\n",
    "\n",
    "        ## Store the Factor Data in dictionary.\n",
    "        self.factorData[\"ValueYield\"] = scores.copy()\n",
    "        ## Filter the data for latest update or values.\n",
    "        self.recentFactorUpdate[\"ValueYield\"] = scores[scores[\"Date\"] == scores[\"Date\"].max()].reset_index(drop = True)\n",
    "        ## Combining the Factors in one Dataframe\n",
    "        self.stockFactors = pd.concat([self.recentFactorUpdate[key].set_index([\"Date\", \"Symbol\"]) for key in self.recentFactorUpdate.keys()], axis = 1)\n",
    "        self.stockFactors.reset_index(inplace = True)\n",
    "\n",
    "        print(\"### ValueYield COMPLETE ###\")\n",
    "\n",
    "    def generate_QualityAnnual(self,):\n",
    "\n",
    "        ## Function to calculate the volatility of the ratio\n",
    "        def calculateVol(series):\n",
    "            _max = series.rolling(window = 5, min_periods = 3).max()\n",
    "            _min = series.rolling(window = 5, min_periods = 3).min()\n",
    "            _mean = series.rolling(window = 5, min_periods = 3).mean() \n",
    "            return (_max - _min)/(_mean)\n",
    "\n",
    "        #########################\n",
    "        ## COMPANY MASTER DATA ##\n",
    "        #########################\n",
    "        if self.companyMaster is None:\n",
    "            self.__readCompanyMaster()\n",
    "\n",
    "        if self.profitLossQuality is None:\n",
    "            self.__readProfitLossQuality()\n",
    "\n",
    "        if self.financialRatio is None:\n",
    "            self.__readFinancialRatio()\n",
    "\n",
    "        if self.cashFlow is None:\n",
    "            self.__readCashFlow()\n",
    "\n",
    "        if self.stockPriceData is None:\n",
    "            self.__readPriceData()\n",
    "\n",
    "        if self.sectorData is None:\n",
    "            self.__readSectorData()\n",
    "\n",
    "        ## Read the Rebalance Dates\n",
    "        # rebalanceDates = self.__dataCursor.fetch_data_from_database(table_name = \"QualityDate\", no_of_years = 100)\n",
    "        rebalanceDates = pd.read_csv('QualityDate.csv')\n",
    "        rebalanceDates[\"Date\"] = pd.to_datetime(rebalanceDates[\"Date\"])\n",
    "        rebalances = rebalanceDates[\"Date\"].tolist()\n",
    "\n",
    "        ## Mapping the Data Receive Year\n",
    "        priceData = pd.merge(self.strategyUniverse, rebalanceDates, on = \"Date\", how = \"left\")\n",
    "        priceData = priceData.sort_values([\"Symbol\", \"Date\"]).reset_index(drop = True)\n",
    "        \n",
    "        ## Filter the dataframe for the rebalance dates.\n",
    "        priceDataTop500Rebal = priceData[priceData[\"Date\"].isin(rebalances)]\n",
    "\n",
    "        ############################\n",
    "        ### PROFIT and LOSS DATA ###\n",
    "        ############################\n",
    "        ## Mapping Company Symbol to Cash Flow Data\n",
    "        profitLoss = pd.merge(self.profitLossQuality, self.companyMaster, on = [\"FINCODE\"], how = \"inner\")\n",
    "\n",
    "        ## Calculating the Operating Margin and Gross Margin\n",
    "        profitLoss[\"OperatingMargin\"] = profitLoss[\"Operating_profit\"].div(profitLoss[\"Net_sales\"])\n",
    "        profitLoss[\"GrossMargin\"] = profitLoss[\"Gross_profits\"].div(profitLoss[\"Net_sales\"])\n",
    "\n",
    "        ## Convert the Date String to python datetime\n",
    "        profitLoss[\"Year_end\"] = pd.to_datetime(profitLoss[\"Year_end\"], format = \"%Y%m\")\n",
    "        profitLoss[\"Year\"]  = profitLoss[\"Year_end\"].dt.year\n",
    "        profitLoss[\"Year\"] = np.where(profitLoss[\"Year_end\"].dt.month == 12, profitLoss[\"Year\"]+1, profitLoss[\"Year\"])\n",
    "\n",
    "        ## Drop the duplicates and Keep the last record\n",
    "        profitLoss = profitLoss.sort_values([\"Year_end\", \"Symbol\"])#.drop_duplicates([\"Year\", \"Symbol\"], keep = \"last\")\n",
    "\n",
    "        ## Sort the dataframe by Year column and reset the dataframe\n",
    "        profitLoss.sort_values(\"Year\", inplace = True)\n",
    "        profitLoss.reset_index(drop = True, inplace = True)\n",
    "        \n",
    "        ############################\n",
    "        ### FINANCIAL RATIO DATA ###\n",
    "        ############################\n",
    "        ## Replace NaN value with 0\n",
    "        financialRatio = self.financialRatio.copy()\n",
    "        financialRatio[\"Payable_days\"] = financialRatio[\"Payable_days\"].fillna(0)\n",
    "        ## Calculating the Working Capital Days\n",
    "        financialRatio[\"WC_Days\"] = financialRatio[\"Inventory_Days\"] + financialRatio[\"Receivable_days\"] - financialRatio[\"Payable_days\"]\n",
    "        ## Mapping COmpany Symbol to Financial Ratio data\n",
    "        financialRatio = pd.merge(financialRatio, self.companyMaster, on = [\"FINCODE\"], how = \"inner\")\n",
    "\n",
    "        ## Convert the Date string to python datetime\n",
    "        financialRatio[\"Year_end\"] = pd.to_datetime(financialRatio.Year_end, format = \"%Y%m\")\n",
    "        financialRatio[\"Year\"] = financialRatio[\"Year_end\"].dt.year\n",
    "        financialRatio[\"Year\"] = np.where(financialRatio[\"Year_end\"].dt.month == 12, financialRatio[\"Year\"] + 1, financialRatio[\"Year\"])\n",
    "\n",
    "        ## Drop the duplicates and Keep the last record\n",
    "        financialRatio = financialRatio.sort_values([\"Year_end\", \"Symbol\"])#.drop_duplicates([\"Year\", \"Symbol\"], keep = \"last\")\n",
    "\n",
    "        ## Sort the dataframe by Year column and reset the dataframe\n",
    "        financialRatio.sort_values(\"Year\", inplace = True)\n",
    "        financialRatio.reset_index(drop = True, inplace = True)\n",
    "\n",
    "        ######################\n",
    "        ### CASH FLOW DATA ###\n",
    "        ######################\n",
    "        ## Mapping Company Symbol to Cash Flow Data\n",
    "        cashFlow = self.cashFlow[[\"FINCODE\", \"Year_end\",\"Cash_from_Operation\"]].copy()\n",
    "        cashFlow = pd.merge(cashFlow, self.companyMaster, on = [\"FINCODE\"], how = \"inner\")\n",
    "\n",
    "        ## Convert the Date String to python datetime\n",
    "        cashFlow[\"Year_end\"] = pd.to_datetime(cashFlow[\"Year_end\"], format = \"%Y%m\")\n",
    "        cashFlow[\"Year\"]  = cashFlow[\"Year_end\"].dt.year\n",
    "        cashFlow[\"Year\"] = np.where(cashFlow[\"Year_end\"].dt.month == 12, cashFlow[\"Year\"]+1, cashFlow[\"Year\"])\n",
    "\n",
    "        ## Drop the duplicates and Keep the last record\n",
    "        cashFlow = cashFlow.sort_values([\"Year_end\", \"Symbol\"])#.drop_duplicates([\"Year\", \"Symbol\"], keep = \"last\")\n",
    "        ## Sort the dataframe by Year column and reset the dataframe\n",
    "        cashFlow.sort_values(\"Year\", inplace = True)\n",
    "        cashFlow.reset_index(drop = True, inplace = True)\n",
    "\n",
    "        ## Merge the dataframes of Profit and Loss, Cash Flow and Financial Ratios into one dataframe\n",
    "        data = ft.reduce(lambda left, right: pd.merge(left, right, on=['Symbol','Year','FINCODE']), [profitLoss, financialRatio, cashFlow])\n",
    "        ## Calculating Cash Flow Operation (CFO) BY EBITDA  and Free cash Flow (FCF) COnversion ratio.\n",
    "        data[\"CFO_By_EBITDA\"] = data[\"Cash_from_Operation\"].div(data[\"Operating_profit\"])\n",
    "        data[\"FCF_Conversion\"] = data[\"FCF_Share\"].div(data[\"Adj_Eps\"])\n",
    "\n",
    "        ## Sort the dataframe and reset the index\n",
    "        data.sort_values([\"Symbol\", \"Year\"], inplace = True)\n",
    "        data.reset_index(drop = True, inplace = True)\n",
    "\n",
    "        ## Calculating the Volatility of Selected Financial ratio (\"ROE\", \"Adj_EPS\", \"CEPS\", \"OperatingMargin\")\n",
    "        tempFactors = ['ROE', 'Adj_Eps', 'CEPS', 'OperatingMargin']\n",
    "        data[[f\"Vol_{value}\" for value in tempFactors]] = data.groupby(\"Symbol\", group_keys = False)[tempFactors].apply(calculateVol)\n",
    "\n",
    "        ## Calculating the Percentage of Selected Financila Ratio (\"Net_Sales\", \"Oprating_profit\", \"Adj_Eps\", \"Cash_from_operation\", \"FCF_Share\")\n",
    "        tempFactors = [\"Net_sales\", \"Operating_profit\", \"Adj_Eps\", \"Cash_from_Operation\", \"FCF_Share\"]\n",
    "        data[[f\"{value}_Growth\" for value in tempFactors]] = data.groupby(\"Symbol\", group_keys = False)[tempFactors].apply(lambda ser: ser.pct_change())\n",
    "\n",
    "        ## Consolidated List of individual factors to create the Consolidated Quality Factor\n",
    "        factorUniverse = [\"FCF_Share\", \"OperatingMargin\", \"GrossMargin\", \"CFO_By_EBITDA\", \"ROE\", \"ROCE\",\n",
    "                        \"Total_Debt_Equity\", \"Interest_Cover\", \"ROA\", \"Inventory_Days\", \"Receivable_days\",\n",
    "                        \"WC_Days\", \"FCF_Conversion\",\"Vol_ROE\", \"Vol_Adj_Eps\", \"Vol_CEPS\", \"Vol_OperatingMargin\",\n",
    "                        \"Dividend_Payout_Per\"]\n",
    "\n",
    "        # \"Growth\"\n",
    "        ## Indivodual facrtor in universe are divided in two list, one to be ranked in ascending order and other list to be ranked in descending order.\n",
    "        ascendingFactorUniverse = [\"CFO_By_EBITDA\", \"Dividend_Payout_Per\", \n",
    "                                \"FCF_Conversion\",  \"FCF_Share\", \"GrossMargin\", \"Interest_Cover\", \"OperatingMargin\", \"ROA\",\n",
    "                                \"ROCE\", \"ROE\"]\n",
    "\n",
    "        descendingFactorUniverse = [\"Inventory_Days\", \"Receivable_days\", \"WC_Days\", \"Total_Debt_Equity\", \n",
    "                                    \"Vol_ROE\", \"Vol_Adj_Eps\", \"Vol_CEPS\", \"Vol_OperatingMargin\"]\n",
    "        \n",
    "        ## Filter the Necessary columns\n",
    "        data = data.filter(items = [\"Symbol\", 'Year'] + factorUniverse)\n",
    "\n",
    "        # quality raw data\n",
    "        self.rawFactorData['quality'] = data.copy()\n",
    "        \n",
    "        ## Mapping the Factor to Stock price Data.\n",
    "        rank = pd.merge(priceDataTop500Rebal, data, on = [\"Symbol\", \"Year\"])\n",
    "\n",
    "        ## Sort the Dataframe and reset the index\n",
    "        rank.sort_values(\"Date\", inplace = True)\n",
    "        rank.reset_index(drop = True, inplace = True)\n",
    "\n",
    "        ## Individual Absolute rank for Ascending and Descending Factor\n",
    "        absAscRank = rank.groupby(\"Year\")[ascendingFactorUniverse].rank(ascending = True, pct = True)\n",
    "        absDescRank = rank.groupby(\"Year\")[descendingFactorUniverse].rank(ascending = False, pct = True)\n",
    "        ## Take the mean of individual factor to get Aboslute rank for each stock\n",
    "        rank[\"AbsoluteRank\"] = pd.concat([absAscRank, absDescRank],axis = 1).mean(axis = 1)\n",
    "\n",
    "        ## Individual Peer rank for Ascending and Descending Factor\n",
    "        peerAscRank = rank.groupby([\"Year\", \"Sector\"])[ascendingFactorUniverse].rank(ascending = True, pct = True)\n",
    "        peerDescRank = rank.groupby([\"Year\", \"Sector\"])[descendingFactorUniverse].rank(ascending = False, pct = True)\n",
    "        ## Take the mean of individual factor to get Peer rank for each stock\n",
    "        rank[\"PeerRank\"] = pd.concat([peerAscRank, peerDescRank],axis = 1).mean(axis = 1)\n",
    "\n",
    "        ## Mapping the Rank against The dates of each stock.\n",
    "        quality = pd.merge(priceData, rank[[\"Date\", \"Symbol\", \"AbsoluteRank\", \"PeerRank\"]], on = [\"Date\", \"Symbol\"], how = \"left\")\n",
    "\n",
    "         ## Sort the dataframe and reset the index\n",
    "        quality.sort_values([\"Symbol\", \"Date\"], inplace = True)\n",
    "        quality.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "        ## Forward fill the Rank\n",
    "        quality[[\"AbsoluteRank\", \"PeerRank\"]] = quality.groupby(\"Symbol\", group_keys = False)[[\"AbsoluteRank\", \"PeerRank\"]].ffill()\n",
    "    \n",
    "        ## Selecting the relevant columns\n",
    "        quality = quality.filter([\"Date\", \"Symbol\", \"AbsoluteRank\", \"PeerRank\"])\n",
    "\n",
    "        ## Calculate the Weighted Quality Factor Score\n",
    "        quality[\"QualityFactor\"] = quality[\"AbsoluteRank\"] * 0.05 + quality[\"PeerRank\"] * 0.95\n",
    "        ## Selecting the relevant columns\n",
    "        quality = quality.filter([\"Date\", \"Symbol\", \"QualityFactor\"])\n",
    "\n",
    "        ## Convert the long from Dataframe to wide form dataframe\n",
    "        quality = quality.pivot_table(index='Date',columns='Symbol',values='QualityFactor').reset_index()\n",
    "        ## Shifting the Date column\n",
    "        quality[\"Date\"] = quality[\"Date\"].shift(-1)\n",
    "        ## Fill NaN value of date with todays date.\n",
    "        quality.iloc[-1, quality.columns.get_loc(\"Date\")] = pd.to_datetime(date.today())\n",
    "        ## Re-converting the wide form dataframe to long form dataframe\n",
    "        quality = quality.melt(id_vars='Date', value_name= \"QualityAnnual\")\n",
    "        ## Drop na and reset the index\n",
    "        quality = quality.dropna().reset_index(drop = True)\n",
    "        quality = quality[quality[\"Date\"] >= \"2006-01-01\"].reset_index(drop = True).copy()\n",
    "\n",
    "        ## Store the Factor Data in dictionary.\n",
    "        self.factorData[\"QualityAnnual\"] = quality.copy()\n",
    "        ## Filter the data for latest update or values.\n",
    "        self.recentFactorUpdate[\"QualityAnnual\"] = quality[quality[\"Date\"] == quality[\"Date\"].max()].reset_index(drop = True)\n",
    "        ## Combining the Factors in one Dataframe\n",
    "        self.stockFactors = pd.concat([self.recentFactorUpdate[key].set_index([\"Date\", \"Symbol\"]) for key in self.recentFactorUpdate.keys() ], axis = 1)\n",
    "        self.stockFactors.reset_index(inplace = True)\n",
    "\n",
    "        print(\"### QUALITY ANNUAL COMPLETE ###\")\n",
    "            \n",
    "    def generate_QualityQuarter(self, valueYieldCall = False):\n",
    "\n",
    "        # Quarterly Networth Calculation\n",
    "        def fill_networth(row, prev_networth):\n",
    "            if pd.isna(row['NetWorth']):\n",
    "                return prev_networth + row['PAT']\n",
    "            else:\n",
    "                return row['NetWorth']\n",
    "\n",
    "        def fill_networth_column(df):\n",
    "            df = df.sort_values(by=['FINCODE', 'Date_End']) \n",
    "            df['NetWorth_Filled'] = np.nan\n",
    "            \n",
    "            for fincode in df['FINCODE'].unique():\n",
    "                prev_networth = None\n",
    "                for i, row in df[df['FINCODE'] == fincode].iterrows():\n",
    "                    if prev_networth is None:\n",
    "                        prev_networth = row['NetWorth']\n",
    "                    else:\n",
    "                        df.at[i, 'NetWorth_Filled'] = fill_networth(row, prev_networth)\n",
    "                        prev_networth = df.at[i, 'NetWorth_Filled']\n",
    "            \n",
    "            return df\n",
    "        \n",
    "        #  EPS Growth Calculation\n",
    "        def calculate_yoy_eps_growth(eps):\n",
    "            \"\"\"Calculates YoY EPS Growth based on the provided rules.\"\"\"\n",
    "            growth = []\n",
    "            for i in range(len(eps)):\n",
    "                if i < 4:\n",
    "                    growth.append(np.nan) \n",
    "                else:\n",
    "                    prev_eps = eps.iloc[i - 4]  \n",
    "                    curr_eps = eps.iloc[i]      \n",
    "                    if prev_eps > 0:\n",
    "                        growth.append((curr_eps - prev_eps) / prev_eps)\n",
    "                    elif prev_eps < 0:\n",
    "                        growth.append(-(curr_eps - prev_eps) / prev_eps)\n",
    "                    else:\n",
    "                        growth.append(np.nan)  \n",
    "            return pd.Series(growth, index=eps.index)  \n",
    "\n",
    "        # Calculate 5-year mean and std deviation for each quarter separately\n",
    "        def calc_quarterly_stats(group):\n",
    "            group = group.sort_values('Date_End')\n",
    "            group['Mean_YoY_EPS_Growth'] = group['YoY_EPS_Growth'].rolling(window=5, min_periods=1).mean()\n",
    "            group['Std_YoY_EPS_Growth'] = group['YoY_EPS_Growth'].rolling(window=5, min_periods=1).std()\n",
    "            return group\n",
    "        \n",
    "        # Weighted Average Z Quality Score\n",
    "        def calculate_weighted_avg_z(row):\n",
    "            if row['Sector'] == 'Bank':\n",
    "                return (1/2) * row['Z_ROE_ttm_fin'] - (1/2) * abs(row['Z_EPS_Growth_Variability_fin'])\n",
    "            else:\n",
    "                return (1/3) * row['Z_ROE_ttm'] - (1/3) * abs(row['Z_DE']) - (1/3) * abs(row['Z_EPS_Growth_Variability'])\n",
    "        \n",
    "        # Define a function to check for negatives in the last 16 quarters\n",
    "        def has_negative_in_last_4_years(series):\n",
    "            return series.rolling(window=16, min_periods=16).apply(lambda x: (x < 0).any(), raw=True)\n",
    "\n",
    "        #########################\n",
    "        ## COMPANY MASTER DATA ##\n",
    "        #########################\n",
    "        if self.companyMaster is None:\n",
    "            self.__readCompanyMaster()\n",
    "\n",
    "        if self.profitLossGrowth is None:\n",
    "            self.__readProfitLossGrowth()\n",
    "\n",
    "        if self.financeBalanceSheet is None:\n",
    "            self.__readFinanceBalanceSheet()\n",
    "\n",
    "        if self.stockPriceData is None:\n",
    "            self.__readPriceData()\n",
    "\n",
    "        if self.sectorData is None:\n",
    "            self.__readSectorData()\n",
    "\n",
    "\n",
    "        price_data = self.stockPriceData.copy()\n",
    "        top_500 = self.strategyUniverse.copy()\n",
    "\n",
    "        # rebalDates = self.__dataCursor.fetch_data_from_database(table_name=\"GrowthDate\", no_of_years=50)\n",
    "        rebalDates = pd.read_csv('growth_date_latest.csv')\n",
    "        rebalDates['Date'] = pd.to_datetime(rebalDates['Date'])\n",
    "        Rebalance_Qtr=list(rebalDates['Quarter'])\n",
    "        Rebalance_Date=list(rebalDates['Date'])\n",
    "        Qtr_date_dict=dict(zip(Rebalance_Date,Rebalance_Qtr))\n",
    "        date_Qtr_dict=dict(zip(Rebalance_Qtr,Rebalance_Date))\n",
    "\n",
    "        top_500['Quarter']=top_500['Date'].map(Qtr_date_dict)\n",
    "\n",
    "        ## Financial Balance Sheet Modfication\n",
    "        financialBalanceSheet = self.financeBalanceSheet[[\"Year_end\", \"FINCODE\", \"Share_Capital\", \"Reserve\"]]\n",
    "        financialBalanceSheet[\"NetWorth\"] = financialBalanceSheet[[\"Share_Capital\", \"Reserve\"]].sum(axis = 1)\n",
    "        financialBalanceSheet  = pd.merge(financialBalanceSheet, self.companyMaster[[\"FINCODE\", \"Symbol\"]], on = \"FINCODE\")\n",
    "        financialBalanceSheet = financialBalanceSheet[[\"Year_end\", \"FINCODE\", \"Symbol\", \"NetWorth\"]]\n",
    "\n",
    "        ## PAT Modification\n",
    "        pat = self.profitLossGrowth[[\"Date_End\", \"FINCODE\", \"PAT\"]]\n",
    "        pat[\"PAT\"] /= 10\n",
    "\n",
    "        patBS = pd.merge(pat, financialBalanceSheet, left_on = [\"Date_End\", \"FINCODE\"], \n",
    "                right_on = [\"Year_end\", \"FINCODE\"], how = \"left\")\n",
    "        patBS['Symbol'] = patBS.groupby('FINCODE', group_keys=False)['Symbol'].apply(lambda x: x.fillna(method='ffill'))\n",
    "        patBS.drop(columns = [\"Year_end\"], inplace = True)\n",
    "\n",
    "        # Apply the function\n",
    "        patBS = fill_networth_column(patBS)\n",
    "        # Fill any remaining NaN in the original NetWorth column with the filled values\n",
    "        patBS['NetWorth'] = patBS['NetWorth'].combine_first(patBS['NetWorth_Filled'])\n",
    "        patBS.drop(columns=['NetWorth_Filled'], inplace=True)  \n",
    "        patBS = patBS.dropna()\n",
    "\n",
    "        # Calculating ROE from scratch\n",
    "        patBS['ROE'] = patBS.groupby('Symbol', group_keys=False).apply(lambda x: x['PAT']/x['NetWorth'])\n",
    "        patBS['ROE_ttm'] = patBS.groupby('Symbol', group_keys=False)['ROE'].apply(lambda x: x.rolling(window=4).sum())\n",
    "\n",
    "        self.patBS = patBS.copy()\n",
    "\n",
    "        roe_ttm = patBS[['FINCODE', 'Date_End', 'ROE_ttm']].copy()\n",
    "\n",
    "        if valueYieldCall:\n",
    "            return  patBS[['FINCODE', \"Symbol\", 'Date_End', 'ROE_ttm']]\n",
    "\n",
    "        quality = self.profitLossGrowth[['FINCODE','Date_End','Debt/Equity Ratio', 'Adj_eps_abs']]\n",
    "        # quality = factor.profitLossGrowth[['FINCODE','Date_End','Debt/Equity Ratio', 'Adj_eps_abs']]\n",
    "        quality = pd.merge(quality, roe_ttm, on=['FINCODE', 'Date_End'])\n",
    "        pl_quality = pd.merge(quality, self.companyMaster, on = \"FINCODE\", how='inner')\n",
    "\n",
    "        self.pl_quality = pl_quality.copy()\n",
    "    \n",
    "        # Getting required Quality ratios\n",
    "        pl_quality = pl_quality[['FINCODE', \"Symbol\", 'Date_End', 'Debt/Equity Ratio', 'ROE_ttm', 'Adj_eps_abs']].reset_index(drop=True)\n",
    "\n",
    "        # Removing the rows with NaN Symbol Names\n",
    "        pl_quality = pl_quality[pl_quality[\"Symbol\"].notna()]\n",
    "\n",
    "        # Filtering Stocks which have been historically in top 500 universe only.\n",
    "        pl_quality = pl_quality[pl_quality[\"Symbol\"].isin(top_500.Symbol.unique())]\n",
    "\n",
    "        # Calculate Adjusted EPS TTM\n",
    "        pl_quality['Adj_eps_abs_TTM'] = pl_quality.groupby('Symbol')['Adj_eps_abs'].transform(lambda x: x.rolling(4).sum())\n",
    "\n",
    "        # Apply the function to create a flag for exclusion\n",
    "        pl_quality['exclude_flag'] = pl_quality.groupby('Symbol')['Adj_eps_abs_TTM'].transform(has_negative_in_last_4_years)\n",
    "\n",
    "        # Remove symbols with fewer than 16 quarters of data\n",
    "        # Count the number of quarters for each symbol\n",
    "        symbol_quarter_counts = pl_quality.groupby('Symbol').size()\n",
    "\n",
    "        # Create a list of symbols with at least 16 quarters\n",
    "        valid_symbols = symbol_quarter_counts[symbol_quarter_counts >= 16].index\n",
    "\n",
    "        # Filter the DataFrame to keep only rows with valid symbols\n",
    "        pl_quality = pl_quality[pl_quality['Symbol'].isin(valid_symbols)]\n",
    "\n",
    "        # Filter out rows based on the exclude flag\n",
    "        b_filtered = pl_quality[pl_quality['exclude_flag'] != 1].reset_index(drop=True)\n",
    "\n",
    "        # Drop the intermediate column used for filtering\n",
    "        b_filtered = b_filtered.drop(columns=['exclude_flag'])\n",
    "\n",
    "        # Mapping GICS to each Symbol\n",
    "        df = pd.merge(b_filtered, self.sectorData[['Symbol', 'Sector']], on='Symbol', how='left')\n",
    "\n",
    "        # Calculating ROE for TTM for the past 4 Years years\n",
    "        df['ROE_ttm'] = df.groupby('Symbol')['ROE_ttm'].transform(lambda x : x.rolling(16).mean())\n",
    "\n",
    "        # Create a new column for quarter information\n",
    "        df['Quarter'] = pd.to_datetime(df['Date_End'], format='%Y%m').dt.quarter\n",
    "\n",
    "        # Calculating YoY EPS Growth for each quarter\n",
    "        df['YoY_EPS_Growth'] = df.groupby('Symbol')['Adj_eps_abs'].transform(calculate_yoy_eps_growth)\n",
    "\n",
    "        # Apply function to each SYMBOL and Quarter group\n",
    "        df = df.groupby(['Symbol', 'Quarter'], group_keys=False).apply(calc_quarterly_stats)\n",
    "\n",
    "        # Dropping Quarter column after calculation to keep the original dataframe structure\n",
    "        df.drop(columns=['Quarter'], inplace=True)\n",
    "\n",
    "        # Calculating rolling 5-year mean and std deviation for EPS growth\n",
    "        df['Mean_YoY_EPS_Growth'] = df.groupby('Symbol')['YoY_EPS_Growth'].transform(lambda x: x.rolling(window=20).mean())\n",
    "        df['Std_YoY_EPS_Growth'] = df.groupby('Symbol')['YoY_EPS_Growth'].transform(lambda x: x.rolling(window=20).std())\n",
    "\n",
    "        # Calulating EPS Growth Variability\n",
    "        df['EPS_Growth_Variability'] = df['Mean_YoY_EPS_Growth'].div(df['Std_YoY_EPS_Growth'])\n",
    "\n",
    "        # Split data into financial and non-financial sectors\n",
    "        financials_df = df[df['Sector'] == 'Bank']\n",
    "        non_financials_df = df[df['Sector'] != 'Bank']\n",
    "\n",
    "        # Calculate mean and std for financials\n",
    "        financials_df['mean_roe_fin'] = financials_df.groupby('Date_End')['ROE_ttm'].transform('mean')\n",
    "        financials_df['std_roe_fin'] = financials_df.groupby('Date_End')['ROE_ttm'].transform('std')\n",
    "\n",
    "        financials_df['mean_eps_growth_variability_fin'] = financials_df.groupby('Date_End')['EPS_Growth_Variability'].transform('mean')\n",
    "        financials_df['std_eps_growth_variability_fin'] = financials_df.groupby('Date_End')['EPS_Growth_Variability'].transform('std')\n",
    "\n",
    "        # Calculate mean and std for non-financials\n",
    "        non_financials_df['mean_roe'] = non_financials_df.groupby('Date_End')['ROE_ttm'].transform('mean')\n",
    "        non_financials_df['std_roe'] = non_financials_df.groupby('Date_End')['ROE_ttm'].transform('std')\n",
    "\n",
    "        non_financials_df['mean_de'] = non_financials_df.groupby('Date_End')['Debt/Equity Ratio'].transform('mean')\n",
    "        non_financials_df['std_de'] = non_financials_df.groupby('Date_End')['Debt/Equity Ratio'].transform('std')\n",
    "\n",
    "        non_financials_df['mean_eps_growth_variability'] = non_financials_df.groupby('Date_End')['EPS_Growth_Variability'].transform('mean')\n",
    "        non_financials_df['std_eps_growth_variability'] = non_financials_df.groupby('Date_End')['EPS_Growth_Variability'].transform('std')\n",
    "\n",
    "        # Combine the financials and non-financials dataframes back together\n",
    "        df = pd.concat([financials_df, non_financials_df]).reset_index(drop=True)\n",
    "\n",
    "        # Z-Score Calculation\n",
    "        df['Z_ROE_ttm'] = (df['ROE_ttm'] - df['mean_roe']) / df['std_roe']\n",
    "        df['Z_ROE_ttm_fin'] = (df['ROE_ttm'] - df['mean_roe_fin']) / df['std_roe_fin']\n",
    "        df['Z_DE'] = (df['Debt/Equity Ratio'] - df['mean_de']) / df['std_de']\n",
    "        df['Z_EPS_Growth_Variability'] = (df['EPS_Growth_Variability'] - df['mean_eps_growth_variability']) / df['std_eps_growth_variability']\n",
    "        df['Z_EPS_Growth_Variability_fin'] = (df['EPS_Growth_Variability'] - df['mean_eps_growth_variability_fin']) / df['std_eps_growth_variability_fin']\n",
    "\n",
    "        df['WeightedAvgZ'] = df.apply(calculate_weighted_avg_z, axis=1)\n",
    "\n",
    "        # Calculate Normalized Quality Score\n",
    "        df['Quality_Score'] = np.where(df['WeightedAvgZ'] >= 0, \n",
    "                                    1 + df['WeightedAvgZ'], \n",
    "                                    (1 - df['WeightedAvgZ'])**-1)\n",
    "\n",
    "        df = df[['Date_End', 'Symbol', 'Sector','Quality_Score']].dropna().reset_index(drop=True).dropna()\n",
    "\n",
    "        df['Quality_pct_rank'] = df.groupby('Date_End', group_keys=False)['Quality_Score'].apply(lambda x : x.rank(pct=True))\n",
    "        # GrowthDate = self.__dataCursor.fetch_data_from_database(table_name='GrowthDate', no_of_years=25)[['Date', 'Qtr']]\n",
    "        GrowthDate = pd.read_csv('growth_date_latest.csv')[['Date', 'Qtr']]\n",
    "        GrowthDate['Date'] = pd.to_datetime(GrowthDate['Date'])\n",
    "        GrowthDate['Qtr'] = GrowthDate['Qtr'].astype('int')\n",
    "\n",
    "        # GrowthDate Mapping  \n",
    "        final_df =  pd.merge(df, GrowthDate, left_on='Date_End', right_on='Qtr').drop(columns=['Date_End', 'Qtr'])\n",
    "        final_df = final_df[['Date', 'Symbol', 'Sector', 'Quality_Score', 'Quality_pct_rank']].sort_values(by='Date').reset_index(drop=True)\n",
    "        \n",
    "        price_data['Date'] = pd.to_datetime(price_data['Date'])\n",
    "        merged_df = pd.merge(final_df[['Date', 'Symbol', 'Quality_pct_rank']], price_data, on=['Date', 'Symbol'], how='outer')\n",
    "\n",
    "        merged_df['Quality_pct_rank'] = merged_df.groupby('Symbol', group_keys=False)['Quality_pct_rank'].apply(lambda x: x.fillna(method='ffill'))\n",
    "\n",
    "        merged_df = merged_df.groupby(\"Date\").apply(lambda x: x.sort_values(\"Mcap\", ascending = False).head(500)).reset_index(drop = True)\n",
    "\n",
    "        merged_df[\"Quality_pct_rank\"] = merged_df.groupby(\"Date\")[\"Quality_pct_rank\"].rank(pct = True)\n",
    "\n",
    "        qualityfinal = merged_df[['Date','Symbol','Quality_pct_rank']]\n",
    "\n",
    "        top500 = qualityfinal.filter([\"Symbol\", \"Date\", \"Quality_pct_rank\"])\n",
    "\n",
    "        ## Convert the long from Dataframe to wide form dataframe\n",
    "        top500 = top500.pivot_table(index='Date',columns='Symbol',values='Quality_pct_rank').reset_index()\n",
    "        ## Shifting the Date column\n",
    "        top500[\"Date\"] = top500[\"Date\"].shift(-1)\n",
    "        ## Fill NaN value of date with todays date.\n",
    "        top500.iloc[-1, top500.columns.get_loc(\"Date\")] = pd.to_datetime(date.today())\n",
    "\n",
    "        ## Re-converting the wide form dataframe to long form dataframe\n",
    "        top500 = top500.melt(id_vars='Date', value_name = \"QualityQuarter\")\n",
    "        ## Drop na and reset the index\n",
    "        top500 = top500.dropna().reset_index(drop = True)\n",
    "        top500 = top500[top500[\"Date\"] >= \"2006-01-01\"].reset_index(drop = True).copy()\n",
    "\n",
    "         ## Store the Factor Data in dictionary.\n",
    "        self.factorData[\"QualityQuarter\"] = top500.copy()\n",
    "        ## Filter the data for latest update or values.\n",
    "        self.recentFactorUpdate[\"QualityQuarter\"] = top500[top500[\"Date\"] == top500[\"Date\"].max()].reset_index(drop = True)\n",
    "        ## Combining the Factors in one Dataframe\n",
    "        self.stockFactors = pd.concat([self.recentFactorUpdate[key].set_index([\"Date\", \"Symbol\"]) for key in self.recentFactorUpdate.keys()], axis = 1)\n",
    "        self.stockFactors.reset_index(inplace = True)\n",
    "\n",
    "        print(\"### QUALITY QUARTER COMPLETE ###\")\n",
    "\n",
    "    def generate_ValueYieldNoPeg(self):\n",
    "\n",
    "        ## Fetch the Stock Value Data\n",
    "        if self.stockValueData is None:\n",
    "            self.__readValueData()\n",
    "\n",
    "        ## Fetch the \n",
    "        if self.stockPriceData is None:\n",
    "            self.__readPriceData()\n",
    "\n",
    "        if self.sectorData is None:\n",
    "            self.__readSectorData()\n",
    "\n",
    "        \n",
    "        ### VALUE ###\n",
    "        # Filter the rows for Top-500 companies historically\n",
    "        valueData = self.stockValueData[self.stockValueData[\"Symbol\"].isin(self.strategyUniverse[\"Symbol\"].unique())].copy()\n",
    "\n",
    "        ## List of Fundamental Factor Value.\n",
    "        fundamentalValueFactor = [\"EV_EBITDA\", \"PS\", \"PB\", \"PE\"]\n",
    "        ## Replace -ve value with NaN value for all the fundamental value factor\n",
    "        for factor in fundamentalValueFactor:\n",
    "            valueData[factor] = np.where(valueData[factor] <= 0, np.nan, valueData[factor])\n",
    "            valueData[factor] = 1/valueData[factor]\n",
    "\n",
    "        ## Sort and reset the index\n",
    "        valueData.sort_values([\"Symbol\", \"Date\"], inplace = True)\n",
    "        valueData.reset_index(drop = True, inplace = True)\n",
    "\n",
    "\n",
    "        ### Dividend ###\n",
    "        dividendData = self.generate_Dividend(valueYieldCall = True)\n",
    "\n",
    "        ## EPS ###\n",
    "        epsData = self.generate_Growth(valueYieldCall = True)\n",
    "\n",
    "        ## ROE\n",
    "        roe = self.generate_QualityQuarter(valueYieldCall=True)\n",
    "        roe[\"Date_End\"] = pd.to_datetime(roe['Date_End'],format=\"%Y%m\")\n",
    "\n",
    "        growth = pd.merge(epsData[[\"Date\", \"Date_End\", \"Symbol\",\"EPS_DILUTED_change\"]], \n",
    "                          roe[[\"Date_End\", \"Symbol\", \"ROE_ttm\"]], on = [\"Date_End\", \"Symbol\"], how = \"left\")\n",
    "\n",
    "        final=pd.merge(valueData,dividendData,on=['Date','Symbol'])\n",
    "        final=pd.merge(final,growth,on=['Date','Symbol'],how='left')\n",
    "\n",
    "        final = final.sort_values([\"Symbol\",\"Date\"]).reset_index(drop = True)\n",
    "        final[['ROE_ttm', \"EPS_DILUTED_change\"]]=final.groupby('Symbol')[['ROE_ttm', \"EPS_DILUTED_change\"]].ffill()\n",
    "        final['PEG']=final['PE']/final['EPS_DILUTED_change']\n",
    "\n",
    "        factorUniverse = ['EV_EBITDA', 'PS','PB', 'PE', 'Dividend','ROE_ttm']\n",
    "\n",
    "        final = pd.merge(self.strategyUniverse, final[[\"Date\", \"Symbol\"] + factorUniverse], on = [\"Date\",\"Symbol\"], how = \"left\")\n",
    "\n",
    "        final[\"AbsRank\"] = final.groupby(\"Date\")[factorUniverse].rank(pct = True).mean(axis = 1)\n",
    "        final[\"PeerRank\"] = final.groupby([\"Date\", \"Sector\"])[factorUniverse].rank(pct = True).mean(axis = 1)\n",
    "\n",
    "        final['ValueYieldNoPeg'] = final['PeerRank']*0.95 + final['AbsRank']*0.05\n",
    "        final['ValueABSNoPeg'] = final['PeerRank']*0.05 + final['AbsRank']*0.95        \n",
    "\n",
    "        result = list()\n",
    "\n",
    "        # Iterate over each volatility-related column\n",
    "        for col in [\"ValueYieldNoPeg\", \"ValueABSNoPeg\"]:\n",
    "\n",
    "            # Filter the dataframe to keep only the \"Symbol\", \"Date\", and current column\n",
    "            temp = final.filter([\"Symbol\", \"Date\", col])\n",
    "\n",
    "            # Pivot the table to have dates as rows and symbols as columns, with values from the current column\n",
    "            temp = temp.pivot_table(index='Date', columns='Symbol', values=col).reset_index()\n",
    "\n",
    "            # Shift the \"Date\" column up by one row to align data with the next date\n",
    "            temp[\"Date\"] = temp[\"Date\"].shift(-1)\n",
    "\n",
    "            # Set the last row's \"Date\" to today's date to capture current data\n",
    "            temp.iloc[-1, temp.columns.get_loc(\"Date\")] = pd.to_datetime(date.today())\n",
    "\n",
    "            # Unpivot the table to return it to long format with \"Date\", \"Symbol\", and current column values\n",
    "            temp = temp.melt(id_vars='Date', value_name=col)\n",
    "\n",
    "            # Drop rows with missing values and reset the index\n",
    "            temp = temp.dropna().reset_index(drop=True)\n",
    "\n",
    "            # Filter data to include only records from January 1, 2006, onwards and reset the index\n",
    "            temp = temp[temp[\"Date\"] >= \"2006-01-01\"].reset_index(drop=True).copy()\n",
    "\n",
    "            # Set the index to \"Date\" and \"Symbol\" for each transformed column and add to the result list\n",
    "            result.append(temp.set_index([\"Date\", \"Symbol\"]))\n",
    "\n",
    "\n",
    "        # Concatenate all transformed columns along the columns axis to create a combined dataframe\n",
    "        scores = pd.concat(result, axis=1).reset_index()\n",
    "\n",
    "        ## Store the Factor Data in dictionary.\n",
    "        self.factorData[\"ValueYieldNoPeg\"] = scores.copy()\n",
    "        ## Filter the data for latest update or values.\n",
    "        self.recentFactorUpdate[\"ValueYieldNoPeg\"] = scores[scores[\"Date\"] == scores[\"Date\"].max()].reset_index(drop = True)\n",
    "        ## Combining the Factors in one Dataframe\n",
    "        self.stockFactors = pd.concat([self.recentFactorUpdate[key].set_index([\"Date\", \"Symbol\"]) for key in self.recentFactorUpdate.keys()], axis = 1)\n",
    "        self.stockFactors.reset_index(inplace = True)\n",
    "\n",
    "        print(\"### ValueYieldNoPEG COMPLETE ###\")\n",
    "\n",
    "    def generate_ValueYieldExDiv(self):\n",
    "\n",
    "        ## Fetch the Stock Value Data\n",
    "        if self.stockValueData is None:\n",
    "            self.__readValueData()\n",
    "\n",
    "        ## Fetch the \n",
    "        if self.stockPriceData is None:\n",
    "            self.__readPriceData()\n",
    "\n",
    "        if self.sectorData is None:\n",
    "            self.__readSectorData()\n",
    "\n",
    "        \n",
    "        ### VALUE ###\n",
    "        # Filter the rows for Top-500 companies historically\n",
    "        valueData = self.stockValueData[self.stockValueData[\"Symbol\"].isin(self.strategyUniverse[\"Symbol\"].unique())].copy()\n",
    "\n",
    "        ## List of Fundamental Factor Value.\n",
    "        fundamentalValueFactor = [\"EV_EBITDA\", \"PS\", \"PB\", \"PE\"]\n",
    "        ## Replace -ve value with NaN value for all the fundamental value factor\n",
    "        for factor in fundamentalValueFactor:\n",
    "            valueData[factor] = np.where(valueData[factor] <= 0, np.nan, valueData[factor])\n",
    "            valueData[factor] = 1/valueData[factor]\n",
    "\n",
    "        ## Sort and reset the index\n",
    "        valueData.sort_values([\"Symbol\", \"Date\"], inplace = True)\n",
    "        valueData.reset_index(drop = True, inplace = True)\n",
    "\n",
    "\n",
    "        ### Dividend ###\n",
    "        dividendData = self.generate_Dividend(valueYieldCall = True)\n",
    "\n",
    "        ## EPS ###\n",
    "        epsData = self.generate_Growth(valueYieldCall = True)\n",
    "\n",
    "        ## ROE\n",
    "        roe = self.generate_QualityQuarter(valueYieldCall=True)\n",
    "        roe[\"Date_End\"] = pd.to_datetime(roe['Date_End'],format=\"%Y%m\")\n",
    "\n",
    "        growth = pd.merge(epsData[[\"Date\", \"Date_End\", \"Symbol\",\"EPS_DILUTED_change\"]], \n",
    "                          roe[[\"Date_End\", \"Symbol\", \"ROE_ttm\"]], on = [\"Date_End\", \"Symbol\"], how = \"left\")\n",
    "\n",
    "        final=pd.merge(valueData,dividendData,on=['Date','Symbol'])\n",
    "        final=pd.merge(final,growth,on=['Date','Symbol'],how='left')\n",
    "\n",
    "        final = final.sort_values([\"Symbol\",\"Date\"]).reset_index(drop = True)\n",
    "        final[['ROE_ttm', \"EPS_DILUTED_change\"]]=final.groupby('Symbol')[['ROE_ttm', \"EPS_DILUTED_change\"]].ffill()\n",
    "        final['PEG']=final['PE']/final['EPS_DILUTED_change']\n",
    "\n",
    "        factorUniverse = ['EV_EBITDA', 'PS','PB', 'PE','PEG','ROE_ttm']\n",
    "\n",
    "        final = pd.merge(self.strategyUniverse, final[[\"Date\", \"Symbol\"] + factorUniverse], on = [\"Date\",\"Symbol\"], how = \"left\")\n",
    "\n",
    "        final[\"AbsRank\"] = final.groupby(\"Date\")[factorUniverse].rank(pct = True).mean(axis = 1)\n",
    "        final[\"PeerRank\"] = final.groupby([\"Date\", \"Sector\"])[factorUniverse].rank(pct = True).mean(axis = 1)\n",
    "\n",
    "        final['ValueYieldExDiv'] = final['PeerRank']*0.95 + final['AbsRank']*0.05\n",
    "        final['ValueABSExDiv'] = final['PeerRank']*0.05 + final['AbsRank']*0.95        \n",
    "\n",
    "        result = list()\n",
    "\n",
    "        # Iterate over each volatility-related column\n",
    "        for col in [\"ValueYieldExDiv\", \"ValueABSExDiv\"]:\n",
    "\n",
    "            # Filter the dataframe to keep only the \"Symbol\", \"Date\", and current column\n",
    "            temp = final.filter([\"Symbol\", \"Date\", col])\n",
    "\n",
    "            # Pivot the table to have dates as rows and symbols as columns, with values from the current column\n",
    "            temp = temp.pivot_table(index='Date', columns='Symbol', values=col).reset_index()\n",
    "\n",
    "            # Shift the \"Date\" column up by one row to align data with the next date\n",
    "            temp[\"Date\"] = temp[\"Date\"].shift(-1)\n",
    "\n",
    "            # Set the last row's \"Date\" to today's date to capture current data\n",
    "            temp.iloc[-1, temp.columns.get_loc(\"Date\")] = pd.to_datetime(date.today())\n",
    "\n",
    "            # Unpivot the table to return it to long format with \"Date\", \"Symbol\", and current column values\n",
    "            temp = temp.melt(id_vars='Date', value_name=col)\n",
    "\n",
    "            # Drop rows with missing values and reset the index\n",
    "            temp = temp.dropna().reset_index(drop=True)\n",
    "\n",
    "            # Filter data to include only records from January 1, 2006, onwards and reset the index\n",
    "            temp = temp[temp[\"Date\"] >= \"2006-01-01\"].reset_index(drop=True).copy()\n",
    "\n",
    "            # Set the index to \"Date\" and \"Symbol\" for each transformed column and add to the result list\n",
    "            result.append(temp.set_index([\"Date\", \"Symbol\"]))\n",
    "\n",
    "\n",
    "        # Concatenate all transformed columns along the columns axis to create a combined dataframe\n",
    "        scores = pd.concat(result, axis=1).reset_index()\n",
    "\n",
    "        ## Store the Factor Data in dictionary.\n",
    "        self.factorData[\"ValueYieldExDiv\"] = scores.copy()\n",
    "        ## Filter the data for latest update or values.\n",
    "        self.recentFactorUpdate[\"ValueYieldExDiv\"] = scores[scores[\"Date\"] == scores[\"Date\"].max()].reset_index(drop = True)\n",
    "        ## Combining the Factors in one Dataframe\n",
    "        self.stockFactors = pd.concat([self.recentFactorUpdate[key].set_index([\"Date\", \"Symbol\"]) for key in self.recentFactorUpdate.keys()], axis = 1)\n",
    "        self.stockFactors.reset_index(inplace = True)\n",
    "\n",
    "        print(\"### ValueYieldExDiv COMPLETE ###\")\n",
    "\n",
    "    def generate_ShortAM(self,):\n",
    "\n",
    "        # Function to calculate log returns\n",
    "        def calculate_log_returns(df):\n",
    "            df['LogReturn'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "            return df.dropna()\n",
    "\n",
    "        # Function to calculate annualized standard deviation\n",
    "        def calculate_annualized_std(df, window=252):\n",
    "            return df['LogReturn'].rolling(window).std() * np.sqrt(window)\n",
    "\n",
    "        # Function to calculate momentum ratios\n",
    "        def calculate_momentum_ratios(series, period):\n",
    "            return series / series.shift(period) - 1\n",
    "\n",
    "        ################\n",
    "        ## PRICE DATA ##\n",
    "        ################\n",
    "        # Check if stock price data is loaded; if not, read it\n",
    "        if self.stockPriceData is None:\n",
    "            self.__readPriceData()\n",
    "\n",
    "        ## Drop the Unnecessary COlumns\n",
    "        priceData = self.stockPriceData.drop(columns = [\"Open\", \"High\", \"Low\", \"Volume\", \"Mcap\"]).copy()\n",
    "        \n",
    "        ## Filter the Symbol which are present in strategy universe\n",
    "        priceData = priceData[priceData[\"Symbol\"].isin(self.strategyUniverse[\"Symbol\"].unique())].reset_index(drop = True)\n",
    "\n",
    "        # Define periods for momentum ratios (in trading days)\n",
    "        periods = {\n",
    "            'MR1': 21,   # 1 month\n",
    "            'MR2': 42,   # 2 months\n",
    "            'MR3': 63,   # 3 months\n",
    "        }\n",
    "\n",
    "        # Apply log return calculation\n",
    "        priceData = priceData.groupby('Symbol', group_keys=False).apply(calculate_log_returns)\n",
    "    \n",
    "        # Calculate momentum ratios for each period\n",
    "        for label, period in periods.items():\n",
    "            priceData[label] = priceData.groupby('Symbol')['Close'].transform(lambda x: calculate_momentum_ratios(x, period))\n",
    "\n",
    "        # Calculate annualized standard deviation\n",
    "        priceData['AnnualizedStd'] = priceData.groupby('Symbol', group_keys=False).apply(calculate_annualized_std)\n",
    "\n",
    "        # Normalize the momentum ratios by dividing by the annualized standard deviation\n",
    "        for label in periods.keys():\n",
    "            priceData[label] /= priceData['AnnualizedStd']\n",
    "\n",
    "        priceData = priceData.sort_values([\"Date\", \"Symbol\"]).reset_index(drop = True)\n",
    "\n",
    "        # Calculate the mean and std deviation of each momentum ratio across the universe\n",
    "        for label in periods.keys():\n",
    "            priceData[f'mu_{label}'] = priceData.groupby('Date')[label].transform(lambda x: x.mean())\n",
    "            priceData[f'sigma_{label}'] = priceData.groupby('Date')[label].transform(lambda x: x.std())\n",
    "\n",
    "        # Calculate Z-scores for each period\n",
    "        for label in periods.keys():\n",
    "            priceData[f'Z_{label}'] = (priceData[label] - priceData[f'mu_{label}']) / priceData[f'sigma_{label}']\n",
    "\n",
    "        # Define specific combinations for which to calculate the final Z-scores\n",
    "        metrics = ['Z_MR1', 'Z_MR2', 'Z_MR3']\n",
    "\n",
    "        # Weighted average Z-score\n",
    "        priceData[\"WtdZScore\"] =priceData[metrics].mean(axis= 1)\n",
    "\n",
    "        # Normalized momentum score\n",
    "        priceData[f'Momentum'] = np.where(priceData[f'WtdZScore'] >= 0,\n",
    "                                                1 + priceData[f'WtdZScore'],\n",
    "                                                (1 - priceData[f'WtdZScore']) ** -1)\n",
    "\n",
    "        ## Filter the Required Columns  \n",
    "        priceData = priceData.filter(items = [\"Date\", \"Symbol\", \"Momentum\"])\n",
    "\n",
    "        ## Merge the Computed Metrics against the symbol on each day's universe\n",
    "        priceData = pd.merge(self.strategyUniverse, priceData, on = [\"Date\", \"Symbol\"], how = \"left\")\n",
    "\n",
    "        ## Computing the Percentile Score of the Symbols on each Date\n",
    "        priceData[\"Momentum\"] = priceData.groupby([\"Date\"])[\"Momentum\"].rank(ascending = True, pct = True)\n",
    "\n",
    "        ## Computing the Aggreate rank of Peer (Theme / Sector / GICS) on each date using Symbol\n",
    "        priceData[\"PeerMomentum\"] = priceData.groupby([\"Date\", \"Peer\"])[\"Momentum\"].transform(lambda x: x.mean())\n",
    "\n",
    "        ## Re-Ranki The Agg.Score of Peer (Theme / Sector / GICS) on each Date.\n",
    "        priceData[\"PeerMomentum\"] = priceData.groupby([\"Date\"])[\"PeerMomentum\"].rank(ascending = True, pct = True)\n",
    "\n",
    "        ## Filter the data after year 2006\n",
    "        priceData = priceData[priceData[\"Date\"] >= \"2006-01-01\"].reset_index(drop = True)\n",
    "         \n",
    "        # Initialize an empty list to store the transformed data for each volatility column\n",
    "        result = list()\n",
    "\n",
    "        # Iterate over each volatility-related column\n",
    "        for col in [\"Momentum\", \"PeerMomentum\"]:\n",
    "\n",
    "            # Filter the dataframe to keep only the \"Symbol\", \"Date\", and current column\n",
    "            temp = priceData.filter([\"Symbol\", \"Date\", col])\n",
    "\n",
    "            # Pivot the table to have dates as rows and symbols as columns, with values from the current column\n",
    "            temp = temp.pivot_table(index='Date', columns='Symbol', values=col).reset_index()\n",
    "\n",
    "            # Shift the \"Date\" column up by one row to align data with the next date\n",
    "            temp[\"Date\"] = temp[\"Date\"].shift(-1)\n",
    "\n",
    "            # Set the last row's \"Date\" to today's date to capture current data\n",
    "            temp.iloc[-1, temp.columns.get_loc(\"Date\")] = pd.to_datetime(date.today())\n",
    "\n",
    "            # Unpivot the table to return it to long format with \"Date\", \"Symbol\", and current column values\n",
    "            temp = temp.melt(id_vars='Date', value_name=col)\n",
    "\n",
    "            # Drop rows with missing values and reset the index\n",
    "            temp = temp.dropna().reset_index(drop=True)\n",
    "\n",
    "            # Filter data to include only records from January 1, 2006, onwards and reset the index\n",
    "            temp = temp[temp[\"Date\"] >= \"2006-01-01\"].reset_index(drop=True).copy()\n",
    "\n",
    "            # Set the index to \"Date\" and \"Symbol\" for each transformed column and add to the result list\n",
    "            result.append(temp.set_index([\"Date\", \"Symbol\"]))\n",
    "\n",
    "        \n",
    "        # Concatenate all transformed columns along the columns axis to create a combined dataframe\n",
    "        scores = pd.concat(result, axis=1).reset_index()\n",
    "        scores.columns = scores.columns.str.replace(\"Momentum\", \"AM\").str.replace(\"Peer\", self.peer)\n",
    "        scores.rename(columns = {\"AM\" : \"ShortAM\", f\"{self.peer}AM\" : f\"Short{self.peer}AM\"}, inplace = True)\n",
    "\n",
    "        self.x = scores.copy()\n",
    "\n",
    "        # Sort the final dataframe by \"Symbol\" and \"Date\" columns for organized viewing and reset the index\n",
    "        scores = scores.sort_values([\"Symbol\", \"Date\"]).reset_index(drop=True)\n",
    "\n",
    "        ## Store the Factor Data in dictionary.\n",
    "        self.factorData[\"ShortAM\"] = scores.copy()\n",
    "\n",
    "        ## Filter the data for latest update or values.\n",
    "        self.recentFactorUpdate[\"ShortAM\"] = scores[scores[\"Date\"] == scores[\"Date\"].max()].reset_index(drop = True)\n",
    "        \n",
    "        ## Combining the Factors in one Dataframe\n",
    "        self.stockFactors = pd.concat([self.recentFactorUpdate[key].set_index([\"Date\", \"Symbol\"]) for key in self.recentFactorUpdate.keys()], axis = 1)\n",
    "        self.stockFactors.reset_index(inplace = True)\n",
    "\n",
    "        print(\"### SHORT MOMENTUM COMPLETE ###\")\n",
    "\n",
    "        del priceData, result, temp, scores\n",
    "    \n",
    "    def generate_MidAM(self,):\n",
    "\n",
    "        # Function to calculate log returns\n",
    "        def calculate_log_returns(df):\n",
    "            df['LogReturn'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "            return df.dropna()\n",
    "\n",
    "        # Function to calculate annualized standard deviation\n",
    "        def calculate_annualized_std(df, window=252):\n",
    "            return df['LogReturn'].rolling(window).std() * np.sqrt(window)\n",
    "\n",
    "        # Function to calculate momentum ratios\n",
    "        def calculate_momentum_ratios(series, period):\n",
    "            return series / series.shift(period) - 1\n",
    "\n",
    "        ################\n",
    "        ## PRICE DATA ##\n",
    "        ################\n",
    "        # Check if stock price data is loaded; if not, read it\n",
    "        if self.stockPriceData is None:\n",
    "            self.__readPriceData()\n",
    "\n",
    "        ## Drop the Unnecessary COlumns\n",
    "        priceData = self.stockPriceData.drop(columns = [\"Open\", \"High\", \"Low\", \"Volume\", \"Mcap\"]).copy()\n",
    "        \n",
    "        ## Filter the Symbol which are present in strategy universe\n",
    "        priceData = priceData[priceData[\"Symbol\"].isin(self.strategyUniverse[\"Symbol\"].unique())].reset_index(drop = True)\n",
    "\n",
    "        # Define periods for momentum ratios (in trading days)\n",
    "        periods = {\n",
    "            'MR3': 63,   # 3 months\n",
    "            'MR6': 6*22,   # 3 months\n",
    "        }\n",
    "\n",
    "        # Apply log return calculation\n",
    "        priceData = priceData.groupby('Symbol', group_keys=False).apply(calculate_log_returns)\n",
    "    \n",
    "        # Calculate momentum ratios for each period\n",
    "        for label, period in periods.items():\n",
    "            priceData[label] = priceData.groupby('Symbol')['Close'].transform(lambda x: calculate_momentum_ratios(x, period))\n",
    "\n",
    "        # Calculate annualized standard deviation\n",
    "        priceData['AnnualizedStd'] = priceData.groupby('Symbol', group_keys=False).apply(calculate_annualized_std)\n",
    "\n",
    "        # Normalize the momentum ratios by dividing by the annualized standard deviation\n",
    "        for label in periods.keys():\n",
    "            priceData[label] /= priceData['AnnualizedStd']\n",
    "\n",
    "        priceData = priceData.sort_values([\"Date\", \"Symbol\"]).reset_index(drop = True)\n",
    "\n",
    "        # Calculate the mean and std deviation of each momentum ratio across the universe\n",
    "        for label in periods.keys():\n",
    "            priceData[f'mu_{label}'] = priceData.groupby('Date')[label].transform(lambda x: x.mean())\n",
    "            priceData[f'sigma_{label}'] = priceData.groupby('Date')[label].transform(lambda x: x.std())\n",
    "\n",
    "        # Calculate Z-scores for each period\n",
    "        for label in periods.keys():\n",
    "            priceData[f'Z_{label}'] = (priceData[label] - priceData[f'mu_{label}']) / priceData[f'sigma_{label}']\n",
    "\n",
    "        # Define specific combinations for which to calculate the final Z-scores\n",
    "        metrics = ['Z_MR3', 'Z_MR6']\n",
    "\n",
    "        # Weighted average Z-score\n",
    "        priceData[\"WtdZScore\"] =priceData[metrics].mean(axis= 1)\n",
    "\n",
    "        # Normalized momentum score\n",
    "        priceData[f'Momentum'] = np.where(priceData[f'WtdZScore'] >= 0,\n",
    "                                                1 + priceData[f'WtdZScore'],\n",
    "                                                (1 - priceData[f'WtdZScore']) ** -1)\n",
    "\n",
    "        ## Filter the Required Columns  \n",
    "        priceData = priceData.filter(items = [\"Date\", \"Symbol\", \"Momentum\"])\n",
    "\n",
    "        ## Merge the Computed Metrics against the symbol on each day's universe\n",
    "        priceData = pd.merge(self.strategyUniverse, priceData, on = [\"Date\", \"Symbol\"], how = \"left\")\n",
    "\n",
    "        ## Computing the Percentile Score of the Symbols on each Date\n",
    "        priceData[\"Momentum\"] = priceData.groupby([\"Date\"])[\"Momentum\"].rank(ascending = True, pct = True)\n",
    "\n",
    "        ## Computing the Aggreate rank of Peer (Theme / Sector / GICS) on each date using Symbol\n",
    "        priceData[\"PeerMomentum\"] = priceData.groupby([\"Date\", \"Peer\"])[\"Momentum\"].transform(lambda x: x.mean())\n",
    "\n",
    "        ## Re-Ranki The Agg.Score of Peer (Theme / Sector / GICS) on each Date.\n",
    "        priceData[\"PeerMomentum\"] = priceData.groupby([\"Date\"])[\"PeerMomentum\"].rank(ascending = True, pct = True)\n",
    "\n",
    "        ## Filter the data after year 2006\n",
    "        priceData = priceData[priceData[\"Date\"] >= \"2006-01-01\"].reset_index(drop = True)\n",
    "         \n",
    "        # Initialize an empty list to store the transformed data for each volatility column\n",
    "        result = list()\n",
    "\n",
    "        # Iterate over each volatility-related column\n",
    "        for col in [\"Momentum\", \"PeerMomentum\"]:\n",
    "\n",
    "            # Filter the dataframe to keep only the \"Symbol\", \"Date\", and current column\n",
    "            temp = priceData.filter([\"Symbol\", \"Date\", col])\n",
    "\n",
    "            # Pivot the table to have dates as rows and symbols as columns, with values from the current column\n",
    "            temp = temp.pivot_table(index='Date', columns='Symbol', values=col).reset_index()\n",
    "\n",
    "            # Shift the \"Date\" column up by one row to align data with the next date\n",
    "            temp[\"Date\"] = temp[\"Date\"].shift(-1)\n",
    "\n",
    "            # Set the last row's \"Date\" to today's date to capture current data\n",
    "            temp.iloc[-1, temp.columns.get_loc(\"Date\")] = pd.to_datetime(date.today())\n",
    "\n",
    "            # Unpivot the table to return it to long format with \"Date\", \"Symbol\", and current column values\n",
    "            temp = temp.melt(id_vars='Date', value_name=col)\n",
    "\n",
    "            # Drop rows with missing values and reset the index\n",
    "            temp = temp.dropna().reset_index(drop=True)\n",
    "\n",
    "            # Filter data to include only records from January 1, 2006, onwards and reset the index\n",
    "            temp = temp[temp[\"Date\"] >= \"2006-01-01\"].reset_index(drop=True).copy()\n",
    "\n",
    "            # Set the index to \"Date\" and \"Symbol\" for each transformed column and add to the result list\n",
    "            result.append(temp.set_index([\"Date\", \"Symbol\"]))\n",
    "\n",
    "        \n",
    "        # Concatenate all transformed columns along the columns axis to create a combined dataframe\n",
    "        scores = pd.concat(result, axis=1).reset_index()\n",
    "        scores.columns = scores.columns.str.replace(\"Momentum\", \"AM\").str.replace(\"Peer\", self.peer)\n",
    "        scores.rename(columns = {\"AM\" : \"MidAM\", f\"{self.peer}AM\" : f\"Mid{self.peer}AM\"}, inplace = True)\n",
    "\n",
    "        self.x = scores.copy()\n",
    "\n",
    "        # Sort the final dataframe by \"Symbol\" and \"Date\" columns for organized viewing and reset the index\n",
    "        scores = scores.sort_values([\"Symbol\", \"Date\"]).reset_index(drop=True)\n",
    "\n",
    "        ## Store the Factor Data in dictionary.\n",
    "        self.factorData[\"MidAM\"] = scores.copy()\n",
    "\n",
    "        ## Filter the data for latest update or values.\n",
    "        self.recentFactorUpdate[\"MidAM\"] = scores[scores[\"Date\"] == scores[\"Date\"].max()].reset_index(drop = True)\n",
    "        \n",
    "        ## Combining the Factors in one Dataframe\n",
    "        self.stockFactors = pd.concat([self.recentFactorUpdate[key].set_index([\"Date\", \"Symbol\"]) for key in self.recentFactorUpdate.keys()], axis = 1)\n",
    "        self.stockFactors.reset_index(inplace = True)\n",
    "\n",
    "        print(\"### MID MOMENTUM COMPLETE ###\")\n",
    "\n",
    "        del priceData, result, temp, scores\n",
    "\n",
    "    def generate_LongAM(self,):\n",
    "\n",
    "            # Function to calculate log returns\n",
    "            def calculate_log_returns(df):\n",
    "                df['LogReturn'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "                return df.dropna()\n",
    "\n",
    "            # Function to calculate annualized standard deviation\n",
    "            def calculate_annualized_std(df, window=252):\n",
    "                return df['LogReturn'].rolling(window).std() * np.sqrt(window)\n",
    "\n",
    "            # Function to calculate momentum ratios\n",
    "            def calculate_momentum_ratios(series, period):\n",
    "                return series / series.shift(period) - 1\n",
    "\n",
    "            ################\n",
    "            ## PRICE DATA ##\n",
    "            ################\n",
    "            # Check if stock price data is loaded; if not, read it\n",
    "            if self.stockPriceData is None:\n",
    "                self.__readPriceData()\n",
    "\n",
    "            ## Drop the Unnecessary COlumns\n",
    "            priceData = self.stockPriceData.drop(columns = [\"Open\", \"High\", \"Low\", \"Volume\", \"Mcap\"]).copy()\n",
    "            \n",
    "            ## Filter the Symbol which are present in strategy universe\n",
    "            priceData = priceData[priceData[\"Symbol\"].isin(self.strategyUniverse[\"Symbol\"].unique())].reset_index(drop = True)\n",
    "\n",
    "            # Define periods for momentum ratios (in trading days)\n",
    "            periods = {\n",
    "                'MR12': 252,   # 12 months\n",
    "                'MR6': 6*22,   # 6 months\n",
    "            }\n",
    "\n",
    "            # Apply log return calculation\n",
    "            priceData = priceData.groupby('Symbol', group_keys=False).apply(calculate_log_returns)\n",
    "        \n",
    "            # Calculate momentum ratios for each period\n",
    "            for label, period in periods.items():\n",
    "                priceData[label] = priceData.groupby('Symbol')['Close'].transform(lambda x: calculate_momentum_ratios(x, period))\n",
    "\n",
    "            # Calculate annualized standard deviation\n",
    "            priceData['AnnualizedStd'] = priceData.groupby('Symbol', group_keys=False).apply(calculate_annualized_std)\n",
    "\n",
    "            # Normalize the momentum ratios by dividing by the annualized standard deviation\n",
    "            for label in periods.keys():\n",
    "                priceData[label] /= priceData['AnnualizedStd']\n",
    "\n",
    "            priceData = priceData.sort_values([\"Date\", \"Symbol\"]).reset_index(drop = True)\n",
    "\n",
    "            # Calculate the mean and std deviation of each momentum ratio across the universe\n",
    "            for label in periods.keys():\n",
    "                priceData[f'mu_{label}'] = priceData.groupby('Date')[label].transform(lambda x: x.mean())\n",
    "                priceData[f'sigma_{label}'] = priceData.groupby('Date')[label].transform(lambda x: x.std())\n",
    "\n",
    "            # Calculate Z-scores for each period\n",
    "            for label in periods.keys():\n",
    "                priceData[f'Z_{label}'] = (priceData[label] - priceData[f'mu_{label}']) / priceData[f'sigma_{label}']\n",
    "\n",
    "            # Define specific combinations for which to calculate the final Z-scores\n",
    "            metrics = ['Z_MR12', 'Z_MR6']\n",
    "\n",
    "            # Weighted average Z-score\n",
    "            priceData[\"WtdZScore\"] =priceData[metrics].mean(axis= 1)\n",
    "\n",
    "            # Normalized momentum score\n",
    "            priceData[f'Momentum'] = np.where(priceData[f'WtdZScore'] >= 0,\n",
    "                                                    1 + priceData[f'WtdZScore'],\n",
    "                                                    (1 - priceData[f'WtdZScore']) ** -1)\n",
    "\n",
    "            ## Filter the Required Columns  \n",
    "            priceData = priceData.filter(items = [\"Date\", \"Symbol\", \"Momentum\"])\n",
    "\n",
    "            ## Merge the Computed Metrics against the symbol on each day's universe\n",
    "            priceData = pd.merge(self.strategyUniverse, priceData, on = [\"Date\", \"Symbol\"], how = \"left\")\n",
    "\n",
    "            ## Computing the Percentile Score of the Symbols on each Date\n",
    "            priceData[\"Momentum\"] = priceData.groupby([\"Date\"])[\"Momentum\"].rank(ascending = True, pct = True)\n",
    "\n",
    "            ## Computing the Aggreate rank of Peer (Theme / Sector / GICS) on each date using Symbol\n",
    "            priceData[\"PeerMomentum\"] = priceData.groupby([\"Date\", \"Peer\"])[\"Momentum\"].transform(lambda x: x.mean())\n",
    "\n",
    "            ## Re-Ranki The Agg.Score of Peer (Theme / Sector / GICS) on each Date.\n",
    "            priceData[\"PeerMomentum\"] = priceData.groupby([\"Date\"])[\"PeerMomentum\"].rank(ascending = True, pct = True)\n",
    "\n",
    "            ## Filter the data after year 2006\n",
    "            priceData = priceData[priceData[\"Date\"] >= \"2006-01-01\"].reset_index(drop = True)\n",
    "            \n",
    "            # Initialize an empty list to store the transformed data for each volatility column\n",
    "            result = list()\n",
    "\n",
    "            # Iterate over each volatility-related column\n",
    "            for col in [\"Momentum\", \"PeerMomentum\"]:\n",
    "\n",
    "                # Filter the dataframe to keep only the \"Symbol\", \"Date\", and current column\n",
    "                temp = priceData.filter([\"Symbol\", \"Date\", col])\n",
    "\n",
    "                # Pivot the table to have dates as rows and symbols as columns, with values from the current column\n",
    "                temp = temp.pivot_table(index='Date', columns='Symbol', values=col).reset_index()\n",
    "\n",
    "                # Shift the \"Date\" column up by one row to align data with the next date\n",
    "                temp[\"Date\"] = temp[\"Date\"].shift(-1)\n",
    "\n",
    "                # Set the last row's \"Date\" to today's date to capture current data\n",
    "                temp.iloc[-1, temp.columns.get_loc(\"Date\")] = pd.to_datetime(date.today())\n",
    "\n",
    "                # Unpivot the table to return it to long format with \"Date\", \"Symbol\", and current column values\n",
    "                temp = temp.melt(id_vars='Date', value_name=col)\n",
    "\n",
    "                # Drop rows with missing values and reset the index\n",
    "                temp = temp.dropna().reset_index(drop=True)\n",
    "\n",
    "                # Filter data to include only records from January 1, 2006, onwards and reset the index\n",
    "                temp = temp[temp[\"Date\"] >= \"2006-01-01\"].reset_index(drop=True).copy()\n",
    "\n",
    "                # Set the index to \"Date\" and \"Symbol\" for each transformed column and add to the result list\n",
    "                result.append(temp.set_index([\"Date\", \"Symbol\"]))\n",
    "\n",
    "            \n",
    "            # Concatenate all transformed columns along the columns axis to create a combined dataframe\n",
    "            scores = pd.concat(result, axis=1).reset_index()\n",
    "            scores.columns = scores.columns.str.replace(\"Momentum\", \"AM\").str.replace(\"Peer\", self.peer)\n",
    "            scores.rename(columns = {\"AM\" : \"LongAM\", f\"{self.peer}AM\" : f\"Long{self.peer}AM\"}, inplace = True)\n",
    "\n",
    "            self.x = scores.copy()\n",
    "\n",
    "            # Sort the final dataframe by \"Symbol\" and \"Date\" columns for organized viewing and reset the index\n",
    "            scores = scores.sort_values([\"Symbol\", \"Date\"]).reset_index(drop=True)\n",
    "\n",
    "            ## Store the Factor Data in dictionary.\n",
    "            self.factorData[\"LongAM\"] = scores.copy()\n",
    "\n",
    "            ## Filter the data for latest update or values.\n",
    "            self.recentFactorUpdate[\"LongAM\"] = scores[scores[\"Date\"] == scores[\"Date\"].max()].reset_index(drop = True)\n",
    "            \n",
    "            ## Combining the Factors in one Dataframe\n",
    "            self.stockFactors = pd.concat([self.recentFactorUpdate[key].set_index([\"Date\", \"Symbol\"]) for key in self.recentFactorUpdate.keys()], axis = 1)\n",
    "            self.stockFactors.reset_index(inplace = True)\n",
    "\n",
    "            print(\"### LONG MOMENTUM COMPLETE ###\")\n",
    "\n",
    "            del priceData, result, temp, scores\n",
    "\n",
    "    def generate_MultiAM(self,):\n",
    "\n",
    "            # Function to calculate log returns\n",
    "            def calculate_log_returns(df):\n",
    "                df['LogReturn'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "                return df.dropna()\n",
    "\n",
    "            # Function to calculate annualized standard deviation\n",
    "            def calculate_annualized_std(df, window=252):\n",
    "                return df['LogReturn'].rolling(window).std() * np.sqrt(window)\n",
    "\n",
    "            # Function to calculate momentum ratios\n",
    "            def calculate_momentum_ratios(series, period):\n",
    "                return series / series.shift(period) - 1\n",
    "\n",
    "            ################\n",
    "            ## PRICE DATA ##\n",
    "            ################\n",
    "            # Check if stock price data is loaded; if not, read it\n",
    "            if self.stockPriceData is None:\n",
    "                self.__readPriceData()\n",
    "\n",
    "            ## Drop the Unnecessary COlumns\n",
    "            priceData = self.stockPriceData.drop(columns = [\"Open\", \"High\", \"Low\", \"Volume\", \"Mcap\"]).copy()\n",
    "            \n",
    "            ## Filter the Symbol which are present in strategy universe\n",
    "            priceData = priceData[priceData[\"Symbol\"].isin(self.strategyUniverse[\"Symbol\"].unique())].reset_index(drop = True)\n",
    "\n",
    "            # Define periods for momentum ratios (in trading days)\n",
    "            periods = {\n",
    "                'MR3': 63,   # 3 months\n",
    "                'MR6': 6*22,   # 3 months\n",
    "            }\n",
    "\n",
    "            # Apply log return calculation\n",
    "            priceData = priceData.groupby('Symbol', group_keys=False).apply(calculate_log_returns)\n",
    "        \n",
    "            # Calculate momentum ratios for each period\n",
    "            for label, period in periods.items():\n",
    "                priceData[label] = priceData.groupby('Symbol')['Close'].transform(lambda x: calculate_momentum_ratios(x, period))\n",
    "\n",
    "            # Calculate annualized standard deviation\n",
    "            priceData['AnnualizedStd'] = priceData.groupby('Symbol', group_keys=False).apply(calculate_annualized_std)\n",
    "\n",
    "            # Normalize the momentum ratios by dividing by the annualized standard deviation\n",
    "            for label in periods.keys():\n",
    "                priceData[label] /= priceData['AnnualizedStd']\n",
    "\n",
    "            priceData = priceData.sort_values([\"Date\", \"Symbol\"]).reset_index(drop = True)\n",
    "\n",
    "            # Calculate the mean and std deviation of each momentum ratio across the universe\n",
    "            for label in periods.keys():\n",
    "                priceData[f'mu_{label}'] = priceData.groupby('Date')[label].transform(lambda x: x.mean())\n",
    "                priceData[f'sigma_{label}'] = priceData.groupby('Date')[label].transform(lambda x: x.std())\n",
    "\n",
    "            # Calculate Z-scores for each period\n",
    "            for label in periods.keys():\n",
    "                priceData[f'Z_{label}'] = (priceData[label] - priceData[f'mu_{label}']) / priceData[f'sigma_{label}']\n",
    "\n",
    "            # Define specific combinations for which to calculate the final Z-scores\n",
    "            metrics = ['Z_MR3', 'Z_MR6']\n",
    "\n",
    "            # Weighted average Z-score\n",
    "            priceData[\"WtdZScore\"] =priceData[metrics].mean(axis= 1)\n",
    "\n",
    "            # Normalized momentum score\n",
    "            priceData[f'Momentum'] = np.where(priceData[f'WtdZScore'] >= 0,\n",
    "                                                    1 + priceData[f'WtdZScore'],\n",
    "                                                    (1 - priceData[f'WtdZScore']) ** -1)\n",
    "\n",
    "            ## Filter the Required Columns  \n",
    "            priceData = priceData.filter(items = [\"Date\", \"Symbol\", \"Momentum\"])\n",
    "\n",
    "            ## Merge the Computed Metrics against the symbol on each day's universe\n",
    "            priceData = pd.merge(self.strategyUniverse, priceData, on = [\"Date\", \"Symbol\"], how = \"left\")\n",
    "\n",
    "            ## Computing the Percentile Score of the Symbols on each Date\n",
    "            priceData[\"Momentum\"] = priceData.groupby([\"Date\"])[\"Momentum\"].rank(ascending = True, pct = True)\n",
    "\n",
    "            ## Computing the Aggreate rank of Peer (Theme / Sector / GICS) on each date using Symbol\n",
    "            priceData[\"PeerMomentum\"] = priceData.groupby([\"Date\", \"Peer\"])[\"Momentum\"].transform(lambda x: x.mean())\n",
    "\n",
    "            ## Re-Ranki The Agg.Score of Peer (Theme / Sector / GICS) on each Date.\n",
    "            priceData[\"PeerMomentum\"] = priceData.groupby([\"Date\"])[\"PeerMomentum\"].rank(ascending = True, pct = True)\n",
    "\n",
    "            ## Filter the data after year 2006\n",
    "            priceData = priceData[priceData[\"Date\"] >= \"2006-01-01\"].reset_index(drop = True)\n",
    "            \n",
    "            # Initialize an empty list to store the transformed data for each volatility column\n",
    "            result = list()\n",
    "\n",
    "            # Iterate over each volatility-related column\n",
    "            for col in [\"Momentum\", \"PeerMomentum\"]:\n",
    "\n",
    "                # Filter the dataframe to keep only the \"Symbol\", \"Date\", and current column\n",
    "                temp = priceData.filter([\"Symbol\", \"Date\", col])\n",
    "\n",
    "                # Pivot the table to have dates as rows and symbols as columns, with values from the current column\n",
    "                temp = temp.pivot_table(index='Date', columns='Symbol', values=col).reset_index()\n",
    "\n",
    "                # Shift the \"Date\" column up by one row to align data with the next date\n",
    "                temp[\"Date\"] = temp[\"Date\"].shift(-1)\n",
    "\n",
    "                # Set the last row's \"Date\" to today's date to capture current data\n",
    "                temp.iloc[-1, temp.columns.get_loc(\"Date\")] = pd.to_datetime(date.today())\n",
    "\n",
    "                # Unpivot the table to return it to long format with \"Date\", \"Symbol\", and current column values\n",
    "                temp = temp.melt(id_vars='Date', value_name=col)\n",
    "\n",
    "                # Drop rows with missing values and reset the index\n",
    "                temp = temp.dropna().reset_index(drop=True)\n",
    "\n",
    "                # Filter data to include only records from January 1, 2006, onwards and reset the index\n",
    "                temp = temp[temp[\"Date\"] >= \"2006-01-01\"].reset_index(drop=True).copy()\n",
    "\n",
    "                # Set the index to \"Date\" and \"Symbol\" for each transformed column and add to the result list\n",
    "                result.append(temp.set_index([\"Date\", \"Symbol\"]))\n",
    "\n",
    "            \n",
    "            # Concatenate all transformed columns along the columns axis to create a combined dataframe\n",
    "            scores = pd.concat(result, axis=1).reset_index()\n",
    "            scores.columns = scores.columns.str.replace(\"Momentum\", \"AM\").str.replace(\"Peer\", self.peer)\n",
    "            scores.rename(columns = {\"AM\" : \"MidAM\", f\"{self.peer}AM\" : f\"Mid{self.peer}AM\"}, inplace = True)\n",
    "\n",
    "            self.x = scores.copy()\n",
    "\n",
    "            # Sort the final dataframe by \"Symbol\" and \"Date\" columns for organized viewing and reset the index\n",
    "            scores = scores.sort_values([\"Symbol\", \"Date\"]).reset_index(drop=True)\n",
    "\n",
    "            ## Store the Factor Data in dictionary.\n",
    "            self.factorData[\"MidAM\"] = scores.copy()\n",
    "\n",
    "            ## Filter the data for latest update or values.\n",
    "            self.recentFactorUpdate[\"MidAM\"] = scores[scores[\"Date\"] == scores[\"Date\"].max()].reset_index(drop = True)\n",
    "            \n",
    "            ## Combining the Factors in one Dataframe\n",
    "            self.stockFactors = pd.concat([self.recentFactorUpdate[key].set_index([\"Date\", \"Symbol\"]) for key in self.recentFactorUpdate.keys()], axis = 1)\n",
    "            self.stockFactors.reset_index(inplace = True)\n",
    "\n",
    "            print(\"### MID MOMENTUM COMPLETE ###\")\n",
    "\n",
    "            del priceData, result, temp, scores\n",
    "\n",
    "    def generate_UltraShortAM(self,):\n",
    " \n",
    "        # Function to calculate log returns\n",
    "        def calculate_log_returns(df):\n",
    "            df['LogReturn'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "            return df.dropna()\n",
    " \n",
    "        # Function to calculate annualized standard deviation\n",
    "        def calculate_annualized_std(df, window=252):\n",
    "            return df['LogReturn'].rolling(window).std() * np.sqrt(window)\n",
    " \n",
    "        # Function to calculate momentum ratios\n",
    "        def calculate_momentum_ratios(series, period):\n",
    "            return series / series.shift(period) - 1\n",
    " \n",
    "        ################\n",
    "        ## PRICE DATA ##\n",
    "        ################\n",
    "        # Check if stock price data is loaded; if not, read it\n",
    "        if self.stockPriceData is None:\n",
    "            self.__readPriceData()\n",
    " \n",
    "        ## Drop the Unnecessary COlumns\n",
    "        priceData = self.stockPriceData.drop(columns = [\"Open\", \"High\", \"Low\", \"Volume\", \"Mcap\"]).copy()\n",
    "        \n",
    "        ## Filter the Symbol which are present in strategy universe\n",
    "        priceData = priceData[priceData[\"Symbol\"].isin(self.strategyUniverse[\"Symbol\"].unique())].reset_index(drop = True)\n",
    " \n",
    "        # Define periods for momentum ratios (in trading days)\n",
    "        periods = {\n",
    "            'MR4': 5*4,   # 4 Weeks\n",
    "            'MR2': 5*2,   # 2 Weeks\n",
    "            'MR1': 5*1,   # 1 Weeks\n",
    "        }\n",
    " \n",
    "        # Apply log return calculation\n",
    "        priceData = priceData.groupby('Symbol', group_keys=False).apply(calculate_log_returns)\n",
    "    \n",
    "        # Calculate momentum ratios for each period\n",
    "        for label, period in periods.items():\n",
    "            priceData[label] = priceData.groupby('Symbol')['Close'].transform(lambda x: calculate_momentum_ratios(x, period))\n",
    " \n",
    "        # Calculate annualized standard deviation\n",
    "        priceData['AnnualizedStd'] = priceData.groupby('Symbol', group_keys=False).apply(calculate_annualized_std)\n",
    " \n",
    "        # Normalize the momentum ratios by dividing by the annualized standard deviation\n",
    "        for label in periods.keys():\n",
    "            priceData[label] /= priceData['AnnualizedStd']\n",
    " \n",
    "        priceData = priceData.sort_values([\"Date\", \"Symbol\"]).reset_index(drop = True)\n",
    " \n",
    "        # Calculate the mean and std deviation of each momentum ratio across the universe\n",
    "        for label in periods.keys():\n",
    "            priceData[f'mu_{label}'] = priceData.groupby('Date')[label].transform(lambda x: x.mean())\n",
    "            priceData[f'sigma_{label}'] = priceData.groupby('Date')[label].transform(lambda x: x.std())\n",
    " \n",
    "        # Calculate Z-scores for each period\n",
    "        for label in periods.keys():\n",
    "            priceData[f'Z_{label}'] = (priceData[label] - priceData[f'mu_{label}']) / priceData[f'sigma_{label}']\n",
    " \n",
    "        # Define specific combinations for which to calculate the final Z-scores\n",
    "        metrics = ['Z_MR1', 'Z_MR2', \"Z_MR4\"]\n",
    " \n",
    "        # Weighted average Z-score\n",
    "        priceData[\"WtdZScore\"] =priceData[metrics].mean(axis= 1)\n",
    " \n",
    "        # Normalized momentum score\n",
    "        priceData[f'Momentum'] = np.where(priceData[f'WtdZScore'] >= 0,\n",
    "                                                1 + priceData[f'WtdZScore'],\n",
    "                                                (1 - priceData[f'WtdZScore']) ** -1)\n",
    " \n",
    "        ## Filter the Required Columns  \n",
    "        priceData = priceData.filter(items = [\"Date\", \"Symbol\", \"Momentum\"])\n",
    " \n",
    "        ## Merge the Computed Metrics against the symbol on each day's universe\n",
    "        priceData = pd.merge(self.strategyUniverse, priceData, on = [\"Date\", \"Symbol\"], how = \"left\")\n",
    " \n",
    "        ## Computing the Percentile Score of the Symbols on each Date\n",
    "        priceData[\"Momentum\"] = priceData.groupby([\"Date\"])[\"Momentum\"].rank(ascending = True, pct = True)\n",
    " \n",
    "        ## Computing the Aggreate rank of Peer (Theme / Sector / GICS) on each date using Symbol\n",
    "        priceData[\"PeerMomentum\"] = priceData.groupby([\"Date\", \"Peer\"])[\"Momentum\"].transform(lambda x: x.mean())\n",
    " \n",
    "        ## Re-Ranki The Agg.Score of Peer (Theme / Sector / GICS) on each Date.\n",
    "        priceData[\"PeerMomentum\"] = priceData.groupby([\"Date\"])[\"PeerMomentum\"].rank(ascending = True, pct = True)\n",
    " \n",
    "        ## Filter the data after year 2006\n",
    "        priceData = priceData[priceData[\"Date\"] >= \"2006-01-01\"].reset_index(drop = True)\n",
    "         \n",
    "        # Initialize an empty list to store the transformed data for each volatility column\n",
    "        result = list()\n",
    " \n",
    "        # Iterate over each volatility-related column\n",
    "        for col in [\"Momentum\", \"PeerMomentum\"]:\n",
    " \n",
    "            # Filter the dataframe to keep only the \"Symbol\", \"Date\", and current column\n",
    "            temp = priceData.filter([\"Symbol\", \"Date\", col])\n",
    " \n",
    "            # Pivot the table to have dates as rows and symbols as columns, with values from the current column\n",
    "            temp = temp.pivot_table(index='Date', columns='Symbol', values=col).reset_index()\n",
    " \n",
    "            # Shift the \"Date\" column up by one row to align data with the next date\n",
    "            temp[\"Date\"] = temp[\"Date\"].shift(-1)\n",
    " \n",
    "            # Set the last row's \"Date\" to today's date to capture current data\n",
    "            temp.iloc[-1, temp.columns.get_loc(\"Date\")] = pd.to_datetime(date.today())\n",
    " \n",
    "            # Unpivot the table to return it to long format with \"Date\", \"Symbol\", and current column values\n",
    "            temp = temp.melt(id_vars='Date', value_name=col)\n",
    " \n",
    "            # Drop rows with missing values and reset the index\n",
    "            temp = temp.dropna().reset_index(drop=True)\n",
    " \n",
    "            # Filter data to include only records from January 1, 2006, onwards and reset the index\n",
    "            temp = temp[temp[\"Date\"] >= \"2006-01-01\"].reset_index(drop=True).copy()\n",
    " \n",
    "            # Set the index to \"Date\" and \"Symbol\" for each transformed column and add to the result list\n",
    "            result.append(temp.set_index([\"Date\", \"Symbol\"]))\n",
    " \n",
    "        \n",
    "        # Concatenate all transformed columns along the columns axis to create a combined dataframe\n",
    "        scores = pd.concat(result, axis=1).reset_index()\n",
    "        scores.columns = scores.columns.str.replace(\"Momentum\", \"AM\").str.replace(\"Peer\", self.peer)\n",
    "        scores.rename(columns = {\"AM\" : \"UltraShortAM\", f\"{self.peer}AM\" : f\"UltraShort{self.peer}AM\"}, inplace = True)\n",
    " \n",
    "        self.x = scores.copy()\n",
    " \n",
    "        # Sort the final dataframe by \"Symbol\" and \"Date\" columns for organized viewing and reset the index\n",
    "        scores = scores.sort_values([\"Symbol\", \"Date\"]).reset_index(drop=True)\n",
    " \n",
    "        ## Store the Factor Data in dictionary.\n",
    "        self.factorData[\"UltraShortAM\"] = scores.copy()\n",
    " \n",
    "        ## Filter the data for latest update or values.\n",
    "        self.recentFactorUpdate[\"UltraShortAM\"] = scores[scores[\"Date\"] == scores[\"Date\"].max()].reset_index(drop = True)\n",
    "        \n",
    "        ## Combining the Factors in one Dataframe\n",
    "        self.stockFactors = pd.concat([self.recentFactorUpdate[key].set_index([\"Date\", \"Symbol\"]) for key in self.recentFactorUpdate.keys()], axis = 1)\n",
    "        self.stockFactors.reset_index(inplace = True)\n",
    " \n",
    "        print(\"### ULTRA SHORT MOMENTUM COMPLETE ###\")\n",
    " \n",
    "        del priceData, result, temp, scores\n",
    "\n",
    "    def generate_Beta(self):\n",
    "        \n",
    "        ################\n",
    "        ## PRICE DATA ##\n",
    "        ################\n",
    "        if self.stockPriceData is None:\n",
    "            self.__readPriceData()\n",
    "\n",
    "        ## Drop the Unnecessary COlumns\n",
    "        priceData = self.stockPriceData.drop(columns = [\"Open\", \"High\", \"Low\", \"Volume\", \"Mcap\"]).copy()\n",
    "        \n",
    "        ## Filter the Symbol which are present in strategy universe\n",
    "        # priceData = priceData[priceData[\"Symbol\"].isin(self.strategyUniverse[\"Symbol\"].unique())].reset_index(drop = True)\n",
    "        # priceData = priceData.sort_values([\"Symbol\", \"Date\"]).reset_index(drop = True)\n",
    "\n",
    "        # data = consol_factor.stockPriceData.copy()\n",
    "        priceData.rename(columns={'Close': 'Price'}, inplace=True)\n",
    "        \n",
    "        # nifty = d.fetch_data_from_database(table_name='Nifty', no_of_years=25)\n",
    "        nifty = pd.read_csv('Nifty.csv', parse_dates=['Date'])\n",
    "        nifty.drop(columns={'NIFTY'}, inplace=True)\n",
    "        nifty.rename(columns={'NIFTY500': 'Nifty'}, inplace=True)\n",
    "\n",
    "        bench1 = nifty.copy()\n",
    "        bench1[\"BenchReturn\"] = bench1[\"Nifty\"].pct_change()\n",
    "        \n",
    "        priceData = pd.merge(priceData, bench1, on = [\"Date\"], how = \"left\")\n",
    "\n",
    "        def rollBeta(group, window = 252):\n",
    "        \n",
    "            group[\"Change\"] = group[\"Price\"].pct_change()\n",
    "            group[\"1Y_Beta\"] = group[\"Change\"].rolling(window = window).cov(group[\"BenchReturn\"]) / group[\"BenchReturn\"].rolling(window = window).var()\n",
    "        \n",
    "            return group\n",
    "        \n",
    "        priceData = priceData.groupby(\"Symbol\").apply(rollBeta, window = 250, include_groups=False).reset_index().drop(columns='level_1')\n",
    "        priceData = priceData[[\"Date\", \"Symbol\", \"1Y_Beta\"]]\n",
    "        # df_beta = beta.copy()\n",
    "        # beta_latest = df_beta[df_beta['Date'] == df_beta['Date'].max()].reset_index(drop=True)\n",
    "        # beta_latest = beta_latest[['Symbol', '1Y_Beta']]\n",
    "        # df_beta = df_beta[['Date', 'Symbol', '1Y_Beta']]\n",
    "        # df_beta = date_shift(df_beta)\n",
    "\n",
    "        # appender_final = pd.merge(appender, df_beta, how='left', on=['Date', 'Symbol'])\n",
    "        \n",
    "        ## Merge the Computed Metrics against the symbol on each day's universe\n",
    "        priceData = pd.merge(self.strategyUniverse, priceData, on = [\"Date\", \"Symbol\"], how = \"left\")\n",
    "\n",
    "        #high beta, low beta\n",
    "        # print(priceData.groupby('Date')['Symbol'].count().tail(10))\n",
    "        priceData['HighBeta'] = priceData.groupby('Date')['1Y_Beta'].rank(pct=True)\n",
    "        priceData['LowBeta'] = 1- priceData['HighBeta']\n",
    "        priceData.drop(columns=['1Y_Beta'], inplace=True)\n",
    "\n",
    "         ## Filter the data after year 2006\n",
    "        priceData = priceData[priceData[\"Date\"] >= \"2006-01-01\"].reset_index(drop = True)\n",
    "         \n",
    "        # Initialize an empty list to store the transformed data for each beta column\n",
    "        result = list()\n",
    "\n",
    "        # Iterate over each beta-related column\n",
    "        for col in [\"HighBeta\", \"LowBeta\"]:\n",
    "\n",
    "            # Filter the dataframe to keep only the \"Symbol\", \"Date\", and current column\n",
    "            temp = priceData.filter([\"Symbol\", \"Date\", col])\n",
    "\n",
    "            # Pivot the table to have dates as rows and symbols as columns, with values from the current column\n",
    "            temp = temp.pivot_table(index='Date', columns='Symbol', values=col).reset_index()\n",
    "\n",
    "            # Shift the \"Date\" column up by one row to align data with the next date\n",
    "            temp[\"Date\"] = temp[\"Date\"].shift(-1)\n",
    "\n",
    "            # Set the last row's \"Date\" to today's date to capture current data\n",
    "            temp.iloc[-1, temp.columns.get_loc(\"Date\")] = pd.to_datetime(date.today())\n",
    "\n",
    "            # Unpivot the table to return it to long format with \"Date\", \"Symbol\", and current column values\n",
    "            temp = temp.melt(id_vars='Date', value_name=col)\n",
    "\n",
    "            # Drop rows with missing values and reset the index\n",
    "            temp = temp.dropna().reset_index(drop=True)\n",
    "\n",
    "            # Filter data to include only records from January 1, 2006, onwards and reset the index\n",
    "            temp = temp[temp[\"Date\"] >= \"2006-01-01\"].reset_index(drop=True).copy()\n",
    "\n",
    "            # Set the index to \"Date\" and \"Symbol\" for each transformed column and add to the result list\n",
    "            result.append(temp.set_index([\"Date\", \"Symbol\"]))\n",
    "\n",
    "        \n",
    "        # Concatenate all transformed columns along the columns axis to create a combined dataframe\n",
    "        scores = pd.concat(result, axis=1).reset_index()\n",
    "        # scores.columns = scores.columns.str.replace(\"Momentum\", \"AM\").str.replace(\"Peer\", self.peer)\n",
    "\n",
    "        # Sort the final dataframe by \"Symbol\" and \"Date\" columns for organized viewing and reset the index\n",
    "        scores = scores.sort_values([\"Symbol\", \"Date\"]).reset_index(drop=True)\n",
    "\n",
    "        ## Store the Factor Data in dictionary.\n",
    "        self.factorData[\"Beta\"] = scores.copy()\n",
    "\n",
    "        ## Filter the data for latest update or values.\n",
    "        self.recentFactorUpdate[\"Beta\"] = scores[scores[\"Date\"] == scores[\"Date\"].max()].reset_index(drop = True)\n",
    "        \n",
    "        ## Combining the Factors in one Dataframe\n",
    "        self.stockFactors = pd.concat([self.recentFactorUpdate[key].set_index([\"Date\", \"Symbol\"]) for key in self.recentFactorUpdate.keys()], axis = 1)\n",
    "        self.stockFactors.reset_index(inplace = True)\n",
    "\n",
    "        print(\"### BETA COMPLETE ###\")\n",
    "\n",
    "        del priceData, result, temp, scores\n",
    "        \n",
    "\n",
    "        # ## Filter the data after year 2006 and sor the dataframe.\n",
    "        # priceData = priceData[priceData[\"Date\"] >= \"2006-01-01\"].copy()\n",
    "        # priceData = priceData.sort_values([\"Symbol\", \"Date\"]).reset_index(drop = True)\n",
    "    \n",
    "        # ## Convert the long from Dataframe to wide form dataframe\n",
    "        # priceData = priceData.pivot_table(index='Date',columns='Symbol',values='AM_New').reset_index()\n",
    "        # ## Shifting the Date column\n",
    "        # priceData[\"Date\"] = priceData[\"Date\"].shift(-1)\n",
    "        # ## Fill NaN value of date with todays date.\n",
    "        # priceData.iloc[-1, priceData.columns.get_loc(\"Date\")] = pd.to_datetime(date.today())\n",
    "        # ## Re-converting the wide form dataframe to long form dataframe\n",
    "        # priceData = priceData.melt(id_vars='Date', value_name= \"EM\")\n",
    "        # ## Drop na and reset the index\n",
    "        # priceData = priceData.dropna().reset_index(drop = True)\n",
    "\n",
    "        # ## Store the Factor Data in dictionary.\n",
    "        # self.factorData[\"EM\"] = priceData.copy()\n",
    "        # ## Filter the data for latest update or values.\n",
    "        # self.recentFactorUpdate[\"EM\"] = priceData[priceData[\"Date\"] == priceData[\"Date\"].max()].reset_index(drop = True)\n",
    "        # ## Combining the Factors in one Dataframe\n",
    "        # self.stockFactors = pd.concat([self.recentFactorUpdate[key].set_index([\"Date\", \"Symbol\"]) for key in self.recentFactorUpdate.keys()], axis = 1)\n",
    "        # self.stockFactors.reset_index(inplace = True)\n",
    "\n",
    "        # print(\"### EM COMPLETE ###\")\n",
    "\n",
    "        # del priceData\n",
    "\n",
    "    def generate_TrendAntitrendMR(self):\n",
    "        ################\n",
    "        ## PRICE DATA ##\n",
    "        ################\n",
    "        if self.stockPriceData is None:\n",
    "            self.__readPriceData()\n",
    "\n",
    "        ## Drop the Unnecessary COlumns\n",
    "        priceData = self.stockPriceData.drop(columns = [\"Open\", \"High\", \"Low\", \"Volume\"]).copy()\n",
    "\n",
    "        price_data = priceData.copy()\n",
    "        price_data_500 = price_data.groupby('Date', group_keys=False).apply(lambda x: x.sort_values(by='Mcap', ascending=False).head(500))\n",
    "        price_data_500 = price_data[price_data['Symbol'].isin(price_data_500['Symbol'])]\n",
    "        # theme_df = pd.read_excel('SectorMapping.xlsx')\n",
    "        # theme_df = d.fetch_data_from_database(table_name = 'SectorThemeGics')\n",
    "        theme_df = pd.read_csv('gics.csv')\n",
    "        theme_df = theme_df[['Symbol', 'Theme']]  # Ensure only relevant columns are kept\n",
    "        df = price_data_500[['Date', 'Symbol', 'Close', 'Mcap']].merge(theme_df, on='Symbol', how='inner')\n",
    "        df['Theme'] = df['Theme'].fillna('Others')\n",
    "        df.set_index('Date', inplace=True)\n",
    "\n",
    "        # Function to calculate log returns\n",
    "        def calculate_log_returns(df):\n",
    "            df['LogReturn'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "            return df.dropna()\n",
    "\n",
    "        # Function to calculate annualized standard deviation\n",
    "        def calculate_annualized_std(df, window=252):\n",
    "            return df['LogReturn'].rolling(window).std() * np.sqrt(window)\n",
    "\n",
    "        # Function to calculate momentum ratios\n",
    "        def calculate_momentum_ratios(series, period):\n",
    "            return series / series.shift(period) - 1\n",
    "\n",
    "        # Define periods for momentum ratios (in trading days)\n",
    "        periods = {\n",
    "            'MR1': 21,   # 1 month\n",
    "            'MR3': 63,   # 3 months\n",
    "            'MR6': 126,  # 6 months\n",
    "            'MR12': 252, # 12 months\n",
    "        }\n",
    "        # Apply log return calculation\n",
    "        df = df.groupby('Symbol', group_keys=False).apply(calculate_log_returns)\n",
    "\n",
    "        # Calculate momentum ratios for each period\n",
    "        for label, period in periods.items():\n",
    "            df[label] = df.groupby('Symbol')['Close'].transform(lambda x: calculate_momentum_ratios(x, period))\n",
    "\n",
    "        # Calculate annualized standard deviation\n",
    "        df['AnnualizedStd'] = df.groupby('Symbol', group_keys=False).apply(calculate_annualized_std)\n",
    "\n",
    "        # Normalize the momentum ratios by dividing by the annualized standard deviation\n",
    "        for label in periods.keys():\n",
    "            df[label] /= df['AnnualizedStd']\n",
    "\n",
    "        # Reset index\n",
    "        df = df.reset_index()\n",
    "\n",
    "        # Calculate the mean and std deviation of each momentum ratio across the universe\n",
    "        for label in periods.keys():\n",
    "            df[f'mu_{label}'] = df.groupby('Date')[label].transform(lambda x: x.mean())\n",
    "            df[f'sigma_{label}'] = df.groupby('Date')[label].transform(lambda x: x.std())\n",
    "\n",
    "        # Calculate Z-scores for each period\n",
    "        for label in periods.keys():\n",
    "            df[f'Z_{label}'] = (df[label] - df[f'mu_{label}']) / df[f'sigma_{label}']\n",
    "\n",
    "        #For short, medium and long run cells 1,2,3 and the last cell(this one)\n",
    "\n",
    "        # Define the combinations for different momentum strategies\n",
    "        momentum_strategies = {\n",
    "            'MomentumShort': ['MR1', 'MR3'],\n",
    "            'MomentumLong': ['MR6', 'MR12']\n",
    "        }\n",
    "\n",
    "        # Calculate weighted average Z-scores and normalized momentum scores for each strategy\n",
    "        for strategy_name, combination in momentum_strategies.items():\n",
    "            # Create Z-score column labels\n",
    "            z_columns = [f'Z_{label}' for label in combination]\n",
    "            \n",
    "            # Calculate equal weights\n",
    "            weights = np.ones(len(z_columns)) / len(z_columns)\n",
    "            \n",
    "            # Calculate weighted average Z-score\n",
    "            df[f'WeightedAvgZ_{strategy_name}'] = df[z_columns].dot(weights)\n",
    "            \n",
    "            # Calculate normalized momentum score\n",
    "            df[f'Score_{strategy_name}'] = np.where(\n",
    "                df[f'WeightedAvgZ_{strategy_name}'] >= 0,\n",
    "                1 + df[f'WeightedAvgZ_{strategy_name}'],\n",
    "                (1 - df[f'WeightedAvgZ_{strategy_name}']) ** -1\n",
    "            )\n",
    "\n",
    "        # Process each strategy and create CSV files\n",
    "        for strategy_name in momentum_strategies.keys():\n",
    "            # Create a copy of the dataframe with only relevant columns\n",
    "            strategy_df = df.copy()\n",
    "            \n",
    "            # Filter for top 500 stocks by market cap\n",
    "            strategy_df = strategy_df.groupby('Date', group_keys=False).apply(\n",
    "                lambda x: x.sort_values(by='Mcap', ascending=False).head(500)\n",
    "            )\n",
    "        strategy_df = strategy_df[['Date', 'Symbol', 'Score_MomentumShort', 'Score_MomentumLong']]\n",
    "        strategy_df = strategy_df.groupby('Date', group_keys=False).apply(\n",
    "            lambda x: x.assign(\n",
    "                TrendMR=x['Score_MomentumLong'].rank(pct=True, method='min', ascending=False),\n",
    "                Short_Rank=x['Score_MomentumShort'].rank(pct=True, method='min', ascending=True),\n",
    "                Long_Rank=x['Score_MomentumLong'].rank(pct=True, method='min', ascending=True)\n",
    "            ).assign(\n",
    "                AntiTrend_MR=lambda df: (df['Short_Rank'] - df['Long_Rank']) / 2,\n",
    "                AntiTrendMR=lambda df: df['AntiTrend_MR'].rank(pct=True, method='min', ascending=False)  # Lower scores get higher ranks\n",
    "            )\n",
    "        )\n",
    "        strategy_df = strategy_df[['Date','Symbol','TrendMR','AntiTrendMR']].reset_index(drop=True)\n",
    "        # strategy_df = date_shift(strategy_df)\n",
    "        # return strategy_df\n",
    "\n",
    "        # trend_antitrend_MR_df = trend_antitrend_MR(consol_factor.stockPriceData.copy())\n",
    "        # latest_trend_mr_df = strategy_df[strategy_df['Date'] == strategy_df['Date'].max()].reset_index(drop=True)\n",
    "        # strategy_df = strategy_df[['Date', 'Symbol','TrendMR','AntiTrendMR']].reset_index(drop=True)\n",
    "        # appender_final = pd.merge(appender_final, trend_antitrend_MR_df, how='left', on=['Date', 'Symbol'])\n",
    "\n",
    "        priceData = strategy_df.copy()\n",
    "\n",
    "         ## Filter the data after year 2006\n",
    "        priceData = priceData[priceData[\"Date\"] >= \"2006-01-01\"].reset_index(drop = True)\n",
    "         \n",
    "        # Initialize an empty list to store the transformed data for each beta column\n",
    "        result = list()\n",
    "\n",
    "        # Iterate over each beta-related column\n",
    "        for col in [\"TrendMR\", \"AntiTrendMR\"]:\n",
    "\n",
    "            # Filter the dataframe to keep only the \"Symbol\", \"Date\", and current column\n",
    "            temp = priceData.filter([\"Symbol\", \"Date\", col])\n",
    "\n",
    "            # Pivot the table to have dates as rows and symbols as columns, with values from the current column\n",
    "            temp = temp.pivot_table(index='Date', columns='Symbol', values=col).reset_index()\n",
    "\n",
    "            # Shift the \"Date\" column up by one row to align data with the next date\n",
    "            temp[\"Date\"] = temp[\"Date\"].shift(-1)\n",
    "\n",
    "            # Set the last row's \"Date\" to today's date to capture current data\n",
    "            temp.iloc[-1, temp.columns.get_loc(\"Date\")] = pd.to_datetime(date.today())\n",
    "\n",
    "            # Unpivot the table to return it to long format with \"Date\", \"Symbol\", and current column values\n",
    "            temp = temp.melt(id_vars='Date', value_name=col)\n",
    "\n",
    "            # Drop rows with missing values and reset the index\n",
    "            temp = temp.dropna().reset_index(drop=True)\n",
    "\n",
    "            # Filter data to include only records from January 1, 2006, onwards and reset the index\n",
    "            temp = temp[temp[\"Date\"] >= \"2006-01-01\"].reset_index(drop=True).copy()\n",
    "\n",
    "            # Set the index to \"Date\" and \"Symbol\" for each transformed column and add to the result list\n",
    "            result.append(temp.set_index([\"Date\", \"Symbol\"]))\n",
    "\n",
    "        \n",
    "        # Concatenate all transformed columns along the columns axis to create a combined dataframe\n",
    "        scores = pd.concat(result, axis=1).reset_index()\n",
    "        # scores.columns = scores.columns.str.replace(\"Momentum\", \"AM\").str.replace(\"Peer\", self.peer)\n",
    "\n",
    "        # Sort the final dataframe by \"Symbol\" and \"Date\" columns for organized viewing and reset the index\n",
    "        scores = scores.sort_values([\"Symbol\", \"Date\"]).reset_index(drop=True)\n",
    "\n",
    "        ## Store the Factor Data in dictionary.\n",
    "        self.factorData[\"Trend\"] = scores.copy()\n",
    "\n",
    "        ## Filter the data for latest update or values.\n",
    "        self.recentFactorUpdate[\"Trend\"] = scores[scores[\"Date\"] == scores[\"Date\"].max()].reset_index(drop = True)\n",
    "        \n",
    "        ## Combining the Factors in one Dataframe\n",
    "        self.stockFactors = pd.concat([self.recentFactorUpdate[key].set_index([\"Date\", \"Symbol\"]) for key in self.recentFactorUpdate.keys()], axis = 1)\n",
    "        self.stockFactors.reset_index(inplace = True)\n",
    "\n",
    "        print(\"### TREND COMPLETE ###\")\n",
    "\n",
    "        del priceData, result, temp, scores, strategy_df\n",
    "        \n",
    "    def generate_ShiftedAM(self):\n",
    "        ################\n",
    "        ## PRICE DATA ##\n",
    "        ################\n",
    "        if self.stockPriceData is None:\n",
    "            self.__readPriceData()\n",
    "\n",
    "        ## Drop the Unnecessary COlumns\n",
    "        priceData = self.stockPriceData.drop(columns = [\"Open\", \"High\", \"Low\", \"Volume\"]).copy()\n",
    "        \n",
    "        ## Filter the Symbol which are present in strategy universe\n",
    "        priceData = priceData[priceData[\"Symbol\"].isin(self.strategyUniverse[\"Symbol\"].unique())].reset_index(drop = True)\n",
    "        priceData = priceData.sort_values([\"Symbol\", \"Date\"]).reset_index(drop = True)\n",
    "\n",
    "        top_1000 =  priceData.groupby('Date', group_keys= False).apply(lambda x: x.sort_values(by='Mcap', ascending=False).head(500))\n",
    "        price_data_500 = priceData[priceData['Symbol'].isin(top_1000['Symbol'])]\n",
    "        df = top_1000[['Date', 'Symbol', 'Close', 'Mcap']]\n",
    "        df.set_index('Date', inplace=True)\n",
    "\n",
    "        # Shift the Close price by 21 days to get the starting point for the annualized standard deviation calculation\n",
    "        df['Close_shifted'] = df.groupby('Symbol')['Close'].shift(21)\n",
    "\n",
    "        # Function to calculate log returns based on the shifted Close (21 days back)\n",
    "        def calculate_log_returns_shifted(df):\n",
    "            df['LogReturn_shifted'] = np.log(df['Close_shifted'] / df['Close_shifted'].shift(1))\n",
    "            return df\n",
    "\n",
    "        # Apply the shifted log return calculation for each symbol group\n",
    "        df = df.groupby('Symbol', group_keys=False).apply(calculate_log_returns_shifted)\n",
    "\n",
    "        # Function to calculate annualized standard deviation over a 252-day rolling window from the 21-day shifted close\n",
    "        def calculate_annualized_std_shifted(df):\n",
    "            df['AnnualizedStd_Shifted'] = df['LogReturn_shifted'].rolling(window=252).std() * np.sqrt(252)\n",
    "            return df\n",
    "\n",
    "        # Calculate annualized standard deviation based on the 21-day shifted close prices for each symbol group\n",
    "        df = df.groupby('Symbol', group_keys=False).apply(calculate_annualized_std_shifted)\n",
    "\n",
    "        # Drop any NaN values generated from the shifting process\n",
    "        # df = df.dropna(subset=['AnnualizedStd_Shifted'])\n",
    "\n",
    "        # Display the DataFrame\n",
    "        df.head()\n",
    "\n",
    "        # Define custom momentum calculation based on rebalancing months with custom intervals\n",
    "        def calculate_custom_momentum_intervals(df, label, start_month, end_month):\n",
    "            shifted_df = df.copy()\n",
    "            \n",
    "            # Calculate close prices shifted by the specified start and end months\n",
    "            shifted_df[f'{label}_start'] = shifted_df.groupby('Symbol')['Close'].shift(start_month * 21)  # T-1 month back\n",
    "            shifted_df[f'{label}_end'] = shifted_df.groupby('Symbol')['Close'].shift(end_month * 21)      # T-7 or T-13 months back\n",
    "            \n",
    "            # Calculate the momentum ratio based on specified months and normalize by annualized standard deviation\n",
    "            shifted_df[label] = (\n",
    "                (shifted_df[f'{label}_start'] / shifted_df[f'{label}_end'] - 1) / shifted_df['AnnualizedStd_Shifted']\n",
    "            )\n",
    "\n",
    "            return shifted_df\n",
    "            # return shifted_df.dropna()  # Keep shifted columns by not dropping them here\n",
    "\n",
    "        # Define pairs for each momentum ratio (start and end month)\n",
    "        momentum_intervals = {'MR6': (1, 7), 'MR12': (1, 13)}\n",
    "\n",
    "        # Calculate custom momentum ratios based on the specified intervals\n",
    "        for label, (start_month, end_month) in momentum_intervals.items():\n",
    "            df = calculate_custom_momentum_intervals(df, label, start_month, end_month)\n",
    "\n",
    "        # Reset index for further calculations\n",
    "        df = df.reset_index()\n",
    "\n",
    "        # Calculate the mean and standard deviation of each momentum ratio across the universe on each date\n",
    "        for label in momentum_intervals.keys():\n",
    "            df[f'mu_{label}'] = df.groupby('Date')[label].transform('mean')\n",
    "            df[f'sigma_{label}'] = df.groupby('Date')[label].transform('std')\n",
    "\n",
    "        # Calculate Z-scores for each period\n",
    "        for label in momentum_intervals.keys():\n",
    "            df[f'Z_{label}'] = (df[label] - df[f'mu_{label}']) / df[f'sigma_{label}']\n",
    "        comb_labels = ['Z_MR6', 'Z_MR12'] \n",
    "        comb_weights = [0.5, 0.5] \n",
    "        # comb_labels = ['Z_MR6'] \n",
    "        # comb_weights = [1] \n",
    "        # Calculate weighted average Z-score based on combination of momentum scores\n",
    "        df['WeightedAvgZ_comb'] = df[comb_labels].dot(comb_weights)\n",
    "        # Calculate Normalized Momentum Score based on Weighted Average Z-score\n",
    "        df['NormalizedMomentumScore'] = np.where(\n",
    "            df['WeightedAvgZ_comb'] >= 0,\n",
    "            1 + df['WeightedAvgZ_comb'],\n",
    "            (1 - df['WeightedAvgZ_comb']) ** -1\n",
    "        )\n",
    "\n",
    "        # Filter for the top 1000 stocks by market cap on each date\n",
    "        df = df.groupby('Date', group_keys=False).apply(lambda x: x.sort_values(by='Mcap', ascending=False).head(500))\n",
    "\n",
    "        # Reset index after grouping\n",
    "        df = df.reset_index(drop=True)\n",
    "        momdf = df[['Date','Symbol','Mcap','NormalizedMomentumScore']]\n",
    "        momdf['Shifted_Mom_Rank'] = df.groupby('Date')['NormalizedMomentumScore'].rank(pct=True, ascending=True)\n",
    "        momdf = momdf.sort_values(by=['Date','Shifted_Mom_Rank'], ascending=[True,False])\n",
    "        momdf['Date'] = momdf.groupby('Symbol')['Date'].shift(-1)\n",
    "        momdf = momdf.dropna(subset=['Date'])\n",
    "        momdf = momdf[['Date','Symbol', 'Shifted_Mom_Rank']]\n",
    "        momdf = momdf.dropna()\n",
    "        momdf.rename(columns={'Shifted_Mom_Rank': 'ShiftedAM'}, inplace=True)\n",
    "\n",
    "        ## Filter the data after year 2006 and sor the dataframe.\n",
    "        priceData = momdf.copy()\n",
    "        priceData = priceData[priceData[\"Date\"] >= \"2006-01-01\"].copy()\n",
    "        priceData = priceData.sort_values([\"Symbol\", \"Date\"]).reset_index(drop = True)\n",
    "    \n",
    "        ## Convert the long from Dataframe to wide form dataframe\n",
    "        priceData = priceData.pivot_table(index='Date',columns='Symbol',values='ShiftedAM').reset_index()\n",
    "        ## Shifting the Date column\n",
    "        priceData[\"Date\"] = priceData[\"Date\"].shift(-1)\n",
    "        ## Fill NaN value of date with todays date.\n",
    "        priceData.iloc[-1, priceData.columns.get_loc(\"Date\")] = pd.to_datetime(date.today())\n",
    "        ## Re-converting the wide form dataframe to long form dataframe\n",
    "        priceData = priceData.melt(id_vars='Date', value_name= \"ShiftedAM\")\n",
    "        ## Drop na and reset the index\n",
    "        priceData = priceData.dropna().reset_index(drop = True)\n",
    "\n",
    "        ## Store the Factor Data in dictionary.\n",
    "        self.factorData[\"ShiftedAM\"] = priceData.copy()\n",
    "        ## Filter the data for latest update or values.\n",
    "        self.recentFactorUpdate[\"ShiftedAM\"] = priceData[priceData[\"Date\"] == priceData[\"Date\"].max()].reset_index(drop = True)\n",
    "        ## Combining the Factors in one Dataframe\n",
    "        self.stockFactors = pd.concat([self.recentFactorUpdate[key].set_index([\"Date\", \"Symbol\"]) for key in self.recentFactorUpdate.keys()], axis = 1)\n",
    "        self.stockFactors.reset_index(inplace = True)\n",
    "\n",
    "        print(\"### SHIFTED AM COMPLETE ###\")\n",
    "\n",
    "        del priceData, momdf, df, top_1000, price_data_500\n",
    "\n",
    "    def generate_ValuePrice(self):\n",
    "        ################\n",
    "        ## PRICE DATA ##\n",
    "        ################\n",
    "        if self.stockPriceData is None:\n",
    "            self.__readPriceData()\n",
    "\n",
    "        ## Drop the Unnecessary COlumns\n",
    "        priceData = self.stockPriceData.drop(columns = [\"Open\", \"High\", \"Low\", \"Volume\", \"Mcap\"]).copy()\n",
    "        \n",
    "        # print(priceData.columns)\n",
    "        # 52 week low\n",
    "        price_data = priceData.copy()\n",
    "        # price_data.rename(columns={'Price': 'Close'}, inplace=True)\n",
    "        price_data = price_data[['Date', 'Symbol', 'Close']].reset_index(drop=True)\n",
    "\n",
    "        def calculate_distance52Low(df):  \n",
    "            df['52W_Low'] = df.groupby('Symbol')['Close'].transform(lambda x: x.rolling(window=252, min_periods=1).min())\n",
    "            df['percentage_change_52WLow'] = ((df['52W_Low'] / df['Close']) - 1).mul(100)\n",
    "            # df = df[df['Date'] == df['Date'].iloc[-1]].reset_index(drop=True)\n",
    "            df = df[['Date', 'Symbol', 'percentage_change_52WLow']]\n",
    "            return df\n",
    "\n",
    "        priceData = calculate_distance52Low(price_data.copy(deep=True))\n",
    "        # latest_52WLow = date_shift(latest_52WLow)\n",
    "\n",
    "        # print(priceData.columns)\n",
    "        # priceData = latest_52WLow.copy()\n",
    "        # ValuePrice\n",
    "        # appender_final = pd.merge(appender_final, latest_52WLow, how='left', on=['Date', 'Symbol'])\n",
    "        priceData['ValuePrice'] = priceData.groupby('Date')['percentage_change_52WLow'].rank(pct=True, ascending=True)\n",
    "        # appender_final['ValuePrice'] = appender_final['percentage_change_52WLow'].rank(pct=True, ascending=True)\n",
    "        priceData.drop(columns={'percentage_change_52WLow'}, inplace=True)\n",
    "\n",
    "        ## Merge the Computed Metrics against the symbol on each day's universe\n",
    "        priceData = pd.merge(self.strategyUniverse, priceData, on = [\"Date\", \"Symbol\"], how = \"left\")\n",
    "\n",
    "        ## Computing the Percentile Score of the Symbols on each Date\n",
    "        # priceData[\"Momentum\"] = priceData.groupby([\"Date\"])[\"Momentum\"].rank(ascending = True, pct = True)\n",
    "\n",
    "        ## Computing the Aggreate rank of Peer (Theme / Sector / GICS) on each date using Symbol\n",
    "        priceData[\"SectorValuePrice\"] = priceData.groupby([\"Date\", \"Sector\"])[\"ValuePrice\"].transform(lambda x: x.mean())\n",
    "\n",
    "        ## Re-Ranki The Agg.Score of Peer (Theme / Sector / GICS) on each Date.\n",
    "        priceData[\"SectorValuePrice\"] = priceData.groupby([\"Date\"])[\"SectorValuePrice\"].rank(ascending = True, pct = True)\n",
    "\n",
    "        ## Computing the Aggreate rank of Peer (Theme / Sector / GICS) on each date using Symbol\n",
    "        priceData[\"ThemeValuePrice\"] = priceData.groupby([\"Date\", \"Theme\"])[\"ValuePrice\"].transform(lambda x: x.mean())\n",
    "\n",
    "        ## Re-Ranki The Agg.Score of Peer (Theme / Sector / GICS) on each Date.\n",
    "        priceData[\"ThemeValuePrice\"] = priceData.groupby([\"Date\"])[\"ThemeValuePrice\"].rank(ascending = True, pct = True)\n",
    "        \n",
    "         ## Filter the data after year 2006\n",
    "        priceData = priceData[priceData[\"Date\"] >= \"2006-01-01\"].reset_index(drop = True)\n",
    "         \n",
    "        # Initialize an empty list to store the transformed data for each beta column\n",
    "        result = list()\n",
    "\n",
    "        # Iterate over each beta-related column\n",
    "        for col in [\"ValuePrice\", \"SectorValuePrice\", \"ThemeValuePrice\"]:\n",
    "\n",
    "            # Filter the dataframe to keep only the \"Symbol\", \"Date\", and current column\n",
    "            temp = priceData.filter([\"Symbol\", \"Date\", col])\n",
    "\n",
    "            # Pivot the table to have dates as rows and symbols as columns, with values from the current column\n",
    "            temp = temp.pivot_table(index='Date', columns='Symbol', values=col).reset_index()\n",
    "\n",
    "            # Shift the \"Date\" column up by one row to align data with the next date\n",
    "            temp[\"Date\"] = temp[\"Date\"].shift(-1)\n",
    "\n",
    "            # Set the last row's \"Date\" to today's date to capture current data\n",
    "            temp.iloc[-1, temp.columns.get_loc(\"Date\")] = pd.to_datetime(date.today())\n",
    "\n",
    "            # Unpivot the table to return it to long format with \"Date\", \"Symbol\", and current column values\n",
    "            temp = temp.melt(id_vars='Date', value_name=col)\n",
    "\n",
    "            # Drop rows with missing values and reset the index\n",
    "            temp = temp.dropna().reset_index(drop=True)\n",
    "\n",
    "            # Filter data to include only records from January 1, 2006, onwards and reset the index\n",
    "            temp = temp[temp[\"Date\"] >= \"2006-01-01\"].reset_index(drop=True).copy()\n",
    "\n",
    "            # Set the index to \"Date\" and \"Symbol\" for each transformed column and add to the result list\n",
    "            result.append(temp.set_index([\"Date\", \"Symbol\"]))\n",
    "\n",
    "        \n",
    "        # Concatenate all transformed columns along the columns axis to create a combined dataframe\n",
    "        scores = pd.concat(result, axis=1).reset_index()\n",
    "        # scores.columns = scores.columns.str.replace(\"Momentum\", \"AM\").str.replace(\"Peer\", self.peer)\n",
    "\n",
    "        # Sort the final dataframe by \"Symbol\" and \"Date\" columns for organized viewing and reset the index\n",
    "        scores = scores.sort_values([\"Symbol\", \"Date\"]).reset_index(drop=True)\n",
    "\n",
    "        ## Store the Factor Data in dictionary.\n",
    "        self.factorData[\"ValuePrice\"] = scores.copy()\n",
    "\n",
    "        ## Filter the data for latest update or values.\n",
    "        self.recentFactorUpdate[\"ValuePrice\"] = scores[scores[\"Date\"] == scores[\"Date\"].max()].reset_index(drop = True)\n",
    "        \n",
    "        ## Combining the Factors in one Dataframe\n",
    "        self.stockFactors = pd.concat([self.recentFactorUpdate[key].set_index([\"Date\", \"Symbol\"]) for key in self.recentFactorUpdate.keys()], axis = 1)\n",
    "        self.stockFactors.reset_index(inplace = True)\n",
    "\n",
    "        print(\"### VALUEPRICE COMPLETE ###\")\n",
    "\n",
    "        del priceData, result, temp, scores\n",
    "\n",
    "    def generate_3DQuality(self):\n",
    "\n",
    "        #########################\n",
    "        ## COMPANY MASTER DATA ##\n",
    "        #########################\n",
    "        if self.companyMaster is None:\n",
    "            self.__readCompanyMaster()\n",
    "\n",
    "        if self.profitLossGrowth is None:\n",
    "            self.__readProfitLossGrowth()\n",
    "\n",
    "        if self.financeBalanceSheet is None:\n",
    "            self.__readFinanceBalanceSheet()\n",
    "\n",
    "        if self.stockPriceData is None:\n",
    "            self.__readPriceData()\n",
    "\n",
    "        if self.sectorData is None:\n",
    "            self.__readSectorData()\n",
    "\n",
    "\n",
    "        price_data = self.stockPriceData.copy()\n",
    "        top_500 = self.strategyUniverse.copy()\n",
    "\n",
    "        # mapping = pd.read_excel(\"company_master_mapping.xlsx\")\n",
    "        # price_data = d.fetch_price_data(no_of_years=20)\n",
    "        # price_data = pd.read_csv('stockPriceData-3.csv')\n",
    "        # Convert 'Date' to datetime type\n",
    "        # price_data['Date'] = pd.to_datetime(price_data['Date'])\n",
    "        # Create a dictionary mapping from SYMBOL_NSE to SYMBOL_CM\n",
    "        # mapping = dict(zip(mapping[\"SYMBOL_NSE\"], mapping[\"SYMBOL_CM\"]))\n",
    "        # price_data[\"Symbol\"] = price_data[\"Symbol\"].replace(mapping)\n",
    "        # top_500 =  price_data.groupby('Date', group_keys= False).apply(lambda x: x.sort_values(by='Mcap', ascending=False).head(500))\n",
    "        # Filter price data for dates after '2006-01-01' and drop duplicates\n",
    "        # price_data.drop_duplicates(['Date', 'Symbol'], inplace=True)\n",
    "\n",
    "        #company master swapping\n",
    "        # company_master = d.fetch_data_from_database(table_name='Companymaster')\n",
    "        # company_master = pd.read_csv('Companymaster_3_2_2025.csv')\n",
    "        # company_master[\"SYMBOL\"] = company_master[\"SYMBOL\"].replace(mapping)\n",
    "        company_master = self.companyMaster.copy()\n",
    "        company_master.columns = company_master.columns.str.upper()\n",
    "        # yearly = pd.read_csv('Finance_pl_3_2_2025.csv')\n",
    "        # Quaterly = pd.read_csv('Quarterly_3_2_2025.csv')\n",
    "        # Finance_bs =  pd.read_csv('Finance_bs_3_2_2025.csv')[[ 'Year_end', 'Fincode', 'Share_Capital', 'Reserve']]\n",
    "        Finance_bs = self.financeBalanceSheet[[\"Year_end\", \"FINCODE\", \"Share_Capital\", \"Reserve\"]].copy()\n",
    "        Finance_bs.rename(columns={'FINCODE': 'Fincode'}, inplace=True)\n",
    "        # rebal_dates = self.__dataCursor.fetch_data_from_database(table_name=\"GrowthDate\", no_of_years=50)\n",
    "        rebal_dates = pd.read_csv('growth_date_latest.csv')\n",
    "        rebal_dates[\"Date\"] = pd.to_datetime(rebal_dates[\"Date\"])\n",
    "        rebal=rebal_dates['Date']\n",
    "        Rebalance_Qtr=list(rebal_dates['Quarter'])\n",
    "        Rebalance_Date=list(rebal_dates['Date'])\n",
    "        Qtr_date_dict=dict(zip(Rebalance_Date,Rebalance_Qtr))\n",
    "        date_Qtr_dict=dict(zip(Rebalance_Qtr,Rebalance_Date))\n",
    "        top_500['Quarter']=top_500['Date'].map(Qtr_date_dict)\n",
    "        index_constituents_data= top_500\n",
    "        # X = Quaterly[Quaterly['Result_Type'] == 'Q']\n",
    "        # print(X.columns)\n",
    "        # Calculating Networth\n",
    "        # X = self.profitLossGrowth[[\"Date_End\", \"FINCODE\", \"PAT\"]].copy()\n",
    "        X = self.profitLossGrowth.copy()\n",
    "        X.rename(columns={'FINCODE': 'Fincode'}, inplace=True)\n",
    "\n",
    "        Finance_bs['NetWorth'] = Finance_bs['Share_Capital'] + Finance_bs['Reserve']\n",
    "        Finance_bs2 = Finance_bs[['Year_end', 'Fincode', 'NetWorth']]\n",
    "        Finance_bs3 = pd.merge(Finance_bs2, company_master[['FINCODE', 'SYMBOL']], left_on='Fincode', right_on='FINCODE')\n",
    "        Finance_bs3 = Finance_bs3[['Year_end', 'FINCODE', 'SYMBOL', 'NetWorth']]\n",
    "\n",
    "        # Revisd PAT in units\n",
    "        pat_df= X[['Date_End', 'Fincode', 'PAT']]\n",
    "        pat_df['PAT'] = pat_df['PAT']/10\n",
    "\n",
    "        # Calculating Networth on quarterly basis\n",
    "        p =pd.merge(pat_df, Finance_bs3, left_on=['Date_End','Fincode'], right_on=['Year_end','FINCODE'], how='left')\n",
    "        p['SYMBOL'] = p.groupby('Fincode', group_keys=False)['SYMBOL'].apply(lambda x: x.fillna(method='ffill'))\n",
    "        p = p.drop(columns=['Year_end', 'FINCODE'])\n",
    "\n",
    "        # Quarterly Networth Calculation\n",
    "        def fill_networth(row, prev_networth):\n",
    "            if pd.isna(row['NetWorth']):\n",
    "                return prev_networth + row['PAT']\n",
    "            else:\n",
    "                return row['NetWorth']\n",
    "\n",
    "        def fill_networth_column(df):\n",
    "            df = df.sort_values(by=['Fincode', 'Date_End'])\n",
    "            df['NetWorth_Filled'] = np.nan\n",
    "            for fincode in df['Fincode'].unique():\n",
    "                prev_networth = None\n",
    "                for i, row in df[df['Fincode'] == fincode].iterrows():\n",
    "                    if prev_networth is None:\n",
    "                        prev_networth = row['NetWorth']\n",
    "                    else:\n",
    "                        df.at[i, 'NetWorth_Filled'] = fill_networth(row, prev_networth)\n",
    "                        prev_networth = df.at[i, 'NetWorth_Filled']\n",
    "            return df\n",
    "\n",
    "        # Apply the function\n",
    "        p = fill_networth_column(p)\n",
    "        # Fill any remaining NaN in the original NetWorth column with the filled values\n",
    "        p['NetWorth'] = p['NetWorth'].combine_first(p['NetWorth_Filled'])\n",
    "        p.drop(columns=['NetWorth_Filled'], inplace=True)\n",
    "        p = p.dropna()\n",
    "\n",
    "        # Calculating ROE from scratch\n",
    "        p['ROE'] = p.groupby('SYMBOL', group_keys=False).apply(lambda x: x['PAT']/x['NetWorth'])\n",
    "        p['ROE_ttm'] = p.groupby('SYMBOL', group_keys=False)['ROE'].apply(lambda x: x.rolling(window=4).sum())\n",
    "        roe_ttm = p[['Fincode', 'Date_End', 'ROE_ttm']]\n",
    "\n",
    "        quality = X[['Fincode','Date_End','Debt/Equity Ratio', 'Adj_eps_abs']]\n",
    "        quality = pd.merge(quality, roe_ttm, on=['Fincode', 'Date_End'])\n",
    "        pl_quality = pd.merge(quality, company_master, how='inner', left_on='Fincode', right_on='FINCODE')\n",
    "        a = pl_quality[['Fincode', 'Date_End', 'Debt/Equity Ratio', 'ROE_ttm', 'SYMBOL', 'Adj_eps_abs']].reset_index(drop=True)\n",
    "\n",
    "        # Removing the rows with NaN Symbol Names\n",
    "        a = a[a.SYMBOL.notna()]\n",
    "\n",
    "        # Filtering Stocks which have been historically in top 500 universe only.\n",
    "        b = a[a.SYMBOL.isin(top_500.Symbol.unique())]\n",
    "\n",
    "        # Filter out stocks with fewer than 4 quarters of history\n",
    "        symbol_quarter_counts = b.groupby('SYMBOL').size()\n",
    "        valid_symbols = symbol_quarter_counts[symbol_quarter_counts >= 4].index\n",
    "        b = b[b['SYMBOL'].isin(valid_symbols)]\n",
    "\n",
    "        # Define a function to check for 2 consecutive quarters of negative EPS within a 1-year period\n",
    "        def has_2_consecutive_negative_in_last_1_year(series):\n",
    "            return series.rolling(window=4, min_periods=4).apply(\n",
    "                lambda x: any((x[i] < 0 and x[i + 1] < 0) for i in range(len(x) - 1)), raw=True\n",
    "            )\n",
    "\n",
    "        # Apply the function to create a flag for exclusion\n",
    "        b['exclude_flag'] = b.groupby('SYMBOL')['Adj_eps_abs'].transform(has_2_consecutive_negative_in_last_1_year)\n",
    "        # Filter out rows based on the exclude flag\n",
    "        b_filtered = b[b['exclude_flag'] != 1].reset_index(drop=True)\n",
    "        # Drop the intermediate column used for filtering\n",
    "        b_filtered = b_filtered.drop(columns=['exclude_flag'])\n",
    "        # Calculate Adjusted EPS TTM\n",
    "        b_filtered['Adj_eps_abs_TTM'] = b_filtered.groupby('SYMBOL')['Adj_eps_abs'].transform(lambda x: x.rolling(4).sum())\n",
    "        # SectorThemeGICS = pd.read_excel('SectorMapping.xlsx')\n",
    "        SectorThemeGICS = self.sectorData.copy()\n",
    "        df = pd.merge(b_filtered, SectorThemeGICS[['Symbol', 'Sector']], left_on='SYMBOL', right_on='Symbol', how='left')\n",
    "        # Calculating ROE for TTM for the past 3 Years years\n",
    "        df['ROE_ttm'] = df.groupby('Symbol')['ROE_ttm'].transform(lambda x : x.rolling(12).mean())\n",
    "        # Create a new column for quarter information\n",
    "        df['Quarter'] = pd.to_datetime(df['Date_End'], format='%Y%m').dt.quarter\n",
    "\n",
    "        #  EPS Growth Calculation\n",
    "        def calculate_yoy_eps_growth(eps):\n",
    "            \"\"\"Calculates YoY EPS Growth based on the provided rules.\"\"\"\n",
    "            growth = []\n",
    "            for i in range(len(eps)):\n",
    "                if i < 4:\n",
    "                    growth.append(np.nan)\n",
    "                else:\n",
    "                    prev_eps = eps.iloc[i - 4] \n",
    "                    curr_eps = eps.iloc[i]     \n",
    "                    if prev_eps > 0:\n",
    "                        growth.append((curr_eps - prev_eps) / prev_eps)\n",
    "                    elif prev_eps < 0:\n",
    "                        growth.append(-(curr_eps - prev_eps) / prev_eps)\n",
    "                    else:\n",
    "                        growth.append(np.nan) \n",
    "            return pd.Series(growth, index=eps.index) \n",
    "\n",
    "        # Calculating YoY EPS Growth for each quarter\n",
    "        df['YoY_EPS_Growth'] = df.groupby('SYMBOL')['Adj_eps_abs'].transform(calculate_yoy_eps_growth)\n",
    "\n",
    "        # Calculate 3-year mean and std deviation for each quarter separately\n",
    "        def calc_quarterly_stats(group):\n",
    "            group = group.sort_values('Date_End')\n",
    "            group['Mean_YoY_EPS_Growth'] = group['YoY_EPS_Growth'].rolling(window=3, min_periods=1).mean()\n",
    "            group['Std_YoY_EPS_Growth'] = group['YoY_EPS_Growth'].rolling(window=3, min_periods=1).std()\n",
    "            return group\n",
    "\n",
    "        # Apply function to each SYMBOL and Quarter group\n",
    "        df = df.groupby(['SYMBOL', 'Quarter'], group_keys=False).apply(calc_quarterly_stats)\n",
    "\n",
    "        # Dropping Quarter column after calculation to keep the original dataframe structure\n",
    "        df.drop(columns=['Quarter'], inplace=True)\n",
    "\n",
    "        # Calculating rolling 3-year mean and std deviation for EPS growth\n",
    "        df['Mean_YoY_EPS_Growth'] = df.groupby('SYMBOL')['YoY_EPS_Growth'].transform(lambda x: x.rolling(window=12,min_periods=4).mean())\n",
    "        df['Std_YoY_EPS_Growth'] = df.groupby('SYMBOL')['YoY_EPS_Growth'].transform(lambda x: x.rolling(window=12,min_periods=4).std())\n",
    "\n",
    "        # Calulating EPS Growth Variability\n",
    "        df['EPS_Growth_Variability'] = df['Mean_YoY_EPS_Growth'].div(df['Std_YoY_EPS_Growth'])\n",
    "\n",
    "        # Split data into three sectors: Bank, Finance, and Others\n",
    "        bank_df = df[df['Sector'] == 'Bank']\n",
    "        finance_df = df[df['Sector'] == 'Finance']\n",
    "        non_financials_df = df[~df['Sector'].isin(['Bank', 'Finance'])]\n",
    "\n",
    "        # Calculate metrics for Bank sector\n",
    "        bank_df['mean_roe_bank'] = bank_df.groupby('Date_End')['ROE_ttm'].transform('mean')\n",
    "        bank_df['std_roe_bank'] = bank_df.groupby('Date_End')['ROE_ttm'].transform('std')\n",
    "        bank_df['mean_eps_growth_variability_bank'] = bank_df.groupby('Date_End')['EPS_Growth_Variability'].transform('mean')\n",
    "        bank_df['std_eps_growth_variability_bank'] = bank_df.groupby('Date_End')['EPS_Growth_Variability'].transform('std')\n",
    "\n",
    "        # Calculate metrics for Finance sector\n",
    "        finance_df['mean_roe_fin'] = finance_df.groupby('Date_End')['ROE_ttm'].transform('mean')\n",
    "        finance_df['std_roe_fin'] = finance_df.groupby('Date_End')['ROE_ttm'].transform('std')\n",
    "        finance_df['mean_eps_growth_variability_fin'] = finance_df.groupby('Date_End')['EPS_Growth_Variability'].transform('mean')\n",
    "        finance_df['std_eps_growth_variability_fin'] = finance_df.groupby('Date_End')['EPS_Growth_Variability'].transform('std')\n",
    "\n",
    "        # Calculate metrics for non-financials\n",
    "        non_financials_df['mean_roe'] = non_financials_df.groupby('Date_End')['ROE_ttm'].transform('mean')\n",
    "        non_financials_df['std_roe'] = non_financials_df.groupby('Date_End')['ROE_ttm'].transform('std')\n",
    "        non_financials_df['mean_de'] = non_financials_df.groupby('Date_End')['Debt/Equity Ratio'].transform('mean')\n",
    "        non_financials_df['std_de'] = non_financials_df.groupby('Date_End')['Debt/Equity Ratio'].transform('std')\n",
    "        non_financials_df['mean_eps_growth_variability'] = non_financials_df.groupby('Date_End')['EPS_Growth_Variability'].transform('mean')\n",
    "        non_financials_df['std_eps_growth_variability'] = non_financials_df.groupby('Date_End')['EPS_Growth_Variability'].transform('std')\n",
    "\n",
    "        # Z-Score calculations for Bank sector\n",
    "        bank_df['Z_ROE_ttm_bank'] = (bank_df['ROE_ttm'] - bank_df['mean_roe_bank']) / bank_df['std_roe_bank']\n",
    "        bank_df['Z_EPS_Growth_Variability_bank'] = (bank_df['EPS_Growth_Variability'] - bank_df['mean_eps_growth_variability_bank']) / bank_df['std_eps_growth_variability_bank']\n",
    "\n",
    "        # Z-Score calculations for Finance sector\n",
    "        finance_df['Z_ROE_ttm_fin'] = (finance_df['ROE_ttm'] - finance_df['mean_roe_fin']) / finance_df['std_roe_fin']\n",
    "        finance_df['Z_EPS_Growth_Variability_fin'] = (finance_df['EPS_Growth_Variability'] - finance_df['mean_eps_growth_variability_fin']) / finance_df['std_eps_growth_variability_fin']\n",
    "\n",
    "        # Z-Score calculations for non-financials\n",
    "        non_financials_df['Z_ROE_ttm'] = (non_financials_df['ROE_ttm'] - non_financials_df['mean_roe']) / non_financials_df['std_roe']\n",
    "        non_financials_df['Z_DE'] = (non_financials_df['Debt/Equity Ratio'] - non_financials_df['mean_de']) / non_financials_df['std_de']\n",
    "        non_financials_df['Z_EPS_Growth_Variability'] = (non_financials_df['EPS_Growth_Variability'] - non_financials_df['mean_eps_growth_variability']) / non_financials_df['std_eps_growth_variability']\n",
    "\n",
    "        # Weighted Average Z Quality Score calculation\n",
    "        def calculate_weighted_avg_z(row):\n",
    "            if row['Sector'] == 'Bank':\n",
    "                return (1/2) * row['Z_ROE_ttm_bank'] - (1/2) * abs(row['Z_EPS_Growth_Variability_bank'])\n",
    "            elif row['Sector'] == 'Finance':\n",
    "                return (1/2) * row['Z_ROE_ttm_fin'] - (1/2) * abs(row['Z_EPS_Growth_Variability_fin'])\n",
    "            else:\n",
    "                return (1/3) * row['Z_ROE_ttm'] - (1/3) * abs(row['Z_DE']) - (1/3) * abs(row['Z_EPS_Growth_Variability'])\n",
    "            \n",
    "        # Combine all dataframes\n",
    "        df_combined = pd.concat([bank_df, finance_df, non_financials_df]).reset_index(drop=True)\n",
    "        # Calculate weighted average Z-score\n",
    "        df_combined['WeightedAvgZ'] = df_combined.apply(calculate_weighted_avg_z, axis=1)\n",
    "        # Calculate Normalized Quality Score\n",
    "        df_combined['Quality_Score'] = np.where(df_combined['WeightedAvgZ'] >= 0,\n",
    "                                            1 + df_combined['WeightedAvgZ'],\n",
    "                                            (1 - df_combined['WeightedAvgZ'])**-1)\n",
    "        # df_combined\n",
    "        # Final dataframe with required columns\n",
    "        # df = df_combined[['Date_End', 'Symbol', 'Sector', 'Quality_Score']].dropna().reset_index(drop=True)\n",
    "        # First, let's calculate within-sector quality scores\n",
    "        # We need to recalculate Z-scores within each sector for each date\n",
    "        def calculate_within_sector_quality_score(df):\n",
    "            df_result = df.copy()\n",
    "            # Calculate within-sector Z-scores for each sector\n",
    "            for sector in df['Sector'].unique():\n",
    "                sector_mask = df['Sector'] == sector\n",
    "                sector_data = df[sector_mask].copy()\n",
    "                if sector in ['Bank', 'Finance']:\n",
    "                    # For Bank and Finance sectors - calculate within-sector means and stds\n",
    "                    sector_data['mean_roe_sector'] = sector_data.groupby('Date_End')['ROE_ttm'].transform('mean')\n",
    "                    sector_data['std_roe_sector'] = sector_data.groupby('Date_End')['ROE_ttm'].transform('std')\n",
    "                    sector_data['mean_eps_growth_variability_sector'] = sector_data.groupby('Date_End')['EPS_Growth_Variability'].transform('mean')\n",
    "                    sector_data['std_eps_growth_variability_sector'] = sector_data.groupby('Date_End')['EPS_Growth_Variability'].transform('std')\n",
    "                    # Calculate within-sector Z-scores\n",
    "                    sector_data['Z_ROE_ttm_sector'] = (sector_data['ROE_ttm'] - sector_data['mean_roe_sector']) / sector_data['std_roe_sector']\n",
    "                    sector_data['Z_EPS_Growth_Variability_sector'] = (sector_data['EPS_Growth_Variability'] - sector_data['mean_eps_growth_variability_sector']) / sector_data['std_eps_growth_variability_sector']\n",
    "                    # Calculate within-sector quality score\n",
    "                    sector_data['Within_Sector_Quality_Score'] = (1/2) * sector_data['Z_ROE_ttm_sector'] - (1/2) * abs(sector_data['Z_EPS_Growth_Variability_sector'])\n",
    "                else:  # Non-financial sectors\n",
    "                    # Calculate within-sector means and stds for non-financials\n",
    "                    sector_data['mean_roe_sector'] = sector_data.groupby('Date_End')['ROE_ttm'].transform('mean')\n",
    "                    sector_data['std_roe_sector'] = sector_data.groupby('Date_End')['ROE_ttm'].transform('std')\n",
    "                    sector_data['mean_de_sector'] = sector_data.groupby('Date_End')['Debt/Equity Ratio'].transform('mean')\n",
    "                    sector_data['std_de_sector'] = sector_data.groupby('Date_End')['Debt/Equity Ratio'].transform('std')\n",
    "                    sector_data['mean_eps_growth_variability_sector'] = sector_data.groupby('Date_End')['EPS_Growth_Variability'].transform('mean')\n",
    "                    sector_data['std_eps_growth_variability_sector'] = sector_data.groupby('Date_End')['EPS_Growth_Variability'].transform('std')                \n",
    "                    # Calculate within-sector Z-scores\n",
    "                    sector_data['Z_ROE_ttm_sector'] = (sector_data['ROE_ttm'] - sector_data['mean_roe_sector']) / sector_data['std_roe_sector']\n",
    "                    sector_data['Z_DE_sector'] = (sector_data['Debt/Equity Ratio'] - sector_data['mean_de_sector']) / sector_data['std_de_sector']\n",
    "                    sector_data['Z_EPS_Growth_Variability_sector'] = (sector_data['EPS_Growth_Variability'] - sector_data['mean_eps_growth_variability_sector']) / sector_data['std_eps_growth_variability_sector']\n",
    "                    # Calculate within-sector quality score\n",
    "                    sector_data['Within_Sector_Quality_Score'] = (1/3) * sector_data['Z_ROE_ttm_sector'] - (1/3) * abs(sector_data['Z_DE_sector']) - (1/3) * abs(sector_data['Z_EPS_Growth_Variability_sector'])\n",
    "                # Update the result dataframe\n",
    "                df_result.loc[sector_mask, 'Within_Sector_Quality_Score'] = sector_data['Within_Sector_Quality_Score']\n",
    "            return df_result\n",
    "\n",
    "        # Calculate within-sector quality scores\n",
    "        df_combined = calculate_within_sector_quality_score(df_combined)\n",
    "        # Handle any NaN or infinite values that might arise from division by zero\n",
    "        df_combined['Within_Sector_Quality_Score'] = df_combined['Within_Sector_Quality_Score'].replace([np.inf, -np.inf], np.nan)\n",
    "        # Normalize within-sector quality scores to create within-sector normalized scores\n",
    "        def normalize_within_sector_scores(df):\n",
    "            df_result = df.copy()\n",
    "            df_result['Within_Sector_Normalized_Score'] = np.where(\n",
    "                df_result['Within_Sector_Quality_Score'] >= 0,\n",
    "                1 + df_result['Within_Sector_Quality_Score'],\n",
    "                (1 - df_result['Within_Sector_Quality_Score'])**-1\n",
    "            )\n",
    "            return df_result\n",
    "\n",
    "        df_combined = normalize_within_sector_scores(df_combined)\n",
    "        df = df_combined[['Date_End', 'Symbol', 'Sector', 'Quality_Score', 'Within_Sector_Normalized_Score']].dropna().reset_index(drop=True)\n",
    "        df['MeanQScore'] = ((1/2) * df['Quality_Score'] + (1/2) * (df['Within_Sector_Normalized_Score']))\n",
    "        # df\n",
    "\n",
    "        # GrowthDate = self.__dataCursor.fetch_data_from_database(table_name=\"GrowthDate\", no_of_years=50)[['Date', 'Qtr']]\n",
    "        GrowthDate = pd.read_csv('growth_date_latest.csv', parse_dates=['Date'])[['Date', 'Qtr']]\n",
    "        GrowthDate['Qtr'] = GrowthDate['Qtr'].astype('int')\n",
    "        # GrowthDate Mapping \n",
    "        final_df =  pd.merge(df, GrowthDate, left_on='Date_End', right_on='Qtr').drop(columns=['Date_End', 'Qtr'])\n",
    "        final_df = final_df[['Date', 'Symbol', 'Sector', 'MeanQScore']].sort_values(by='Date').reset_index(drop=True)\n",
    "        # final_df\n",
    "\n",
    "        # Merging Quality Ranks with price data universe\n",
    "        price_data['Date'] = pd.to_datetime(price_data['Date'])\n",
    "        final_df['Date'] = pd.to_datetime(final_df['Date'])\n",
    "        merged_df = pd.merge(final_df[['Date', 'Symbol', 'MeanQScore']], price_data, on=['Date', 'Symbol'], how='outer')\n",
    "        merged_df =  merged_df.groupby('Date', group_keys= False).apply(lambda x: x.sort_values(by='Mcap', ascending=False).head(500))\n",
    "        merged_df['Quality_pct_rank'] = merged_df.groupby('Date', group_keys=False)['MeanQScore'].apply(lambda x : x.rank(pct=True))\n",
    "        merged_df[['Quality_pct_rank']] = merged_df.groupby('Symbol', group_keys=False)[['Quality_pct_rank']].apply(lambda x: x.fillna(method='ffill'))\n",
    "        qualityfinal = merged_df[['Date','Symbol','Quality_pct_rank']]\n",
    "        qualityfinal.rename(columns={'Quality_pct_rank': '3DQuality'}, inplace=True)\n",
    "        # qualityfinal\n",
    "        top500 = qualityfinal.filter([\"Symbol\", \"Date\",\"3DQuality\"])\n",
    "\n",
    "        ## Convert the long from Dataframe to wide form dataframe\n",
    "        top500 = top500.pivot_table(index='Date',columns='Symbol',values='3DQuality').reset_index()\n",
    "        ## Shifting the Date column\n",
    "        top500[\"Date\"] = top500[\"Date\"].shift(-1)\n",
    "        ## Fill NaN value of date with todays date.\n",
    "        top500.iloc[-1, top500.columns.get_loc(\"Date\")] = pd.to_datetime(date.today())\n",
    "\n",
    "        ## Re-converting the wide form dataframe to long form dataframe\n",
    "        top500 = top500.melt(id_vars='Date', value_name = \"3DQuality\")\n",
    "        ## Drop na and reset the index\n",
    "        top500 = top500.dropna().reset_index(drop = True)\n",
    "        top500 = top500[top500[\"Date\"] >= \"2006-01-01\"].reset_index(drop = True).copy()\n",
    "        # top500\n",
    "        ## Store the Factor Data in dictionary.\n",
    "        self.factorData[\"3DQuality\"] = top500.copy()\n",
    "        ## Filter the data for latest update or values.\n",
    "        self.recentFactorUpdate[\"3DQuality\"] = top500[top500[\"Date\"] == top500[\"Date\"].max()].reset_index(drop = True)\n",
    "        ## Combining the Factors in one Dataframe\n",
    "        self.stockFactors = pd.concat([self.recentFactorUpdate[key].set_index([\"Date\", \"Symbol\"]) for key in self.recentFactorUpdate.keys()], axis = 1)\n",
    "        self.stockFactors.reset_index(inplace = True)\n",
    "\n",
    "        print(\"### 3D QUALITY COMPLETE ###\")\n",
    "\n",
    "        del price_data, top500, company_master, Finance_bs, rebal_dates, pat_df, p, roe_ttm, quality, pl_quality, a, b, b_filtered, SectorThemeGICS, df, bank_df, finance_df, non_financials_df, df_combined, merged_df, qualityfinal\n",
    "\n",
    "    def generate_3DValue(self):\n",
    "        ## Fetch the Stock Value Data\n",
    "        if self.stockValueData is None:\n",
    "            self.__readValueData()\n",
    "\n",
    "        if self.stockPriceData is None:\n",
    "            self.__readPriceData()\n",
    "\n",
    "        if self.sectorData is None:\n",
    "            self.__readSectorData()\n",
    "\n",
    "\n",
    "        price_data = self.stockPriceData.copy()\n",
    "        top_500 = self.strategyUniverse.copy()\n",
    "\n",
    "        # table = pd.read_csv('value_raw_data_latest.csv')\n",
    "        # table['Date'] = pd.to_datetime(table['Date'])\n",
    "        table = self.stockValueData.copy()\n",
    "\n",
    "        # price_data = pd.read_csv('stockPriceData-3.csv')\n",
    "        price_data=price_data[price_data['Date']>='2006']\n",
    "        price_data.drop_duplicates(['Date','Symbol'],inplace=True)\n",
    "        # price_data['Date'] = pd.to_datetime(price_data['Date'])\n",
    "        \n",
    "        table = pd.merge(price_data,table, on= ['Date','Symbol'])\n",
    "        # Filter top 500 companies by market cap for each date\n",
    "        table = table.groupby('Date', group_keys=False).apply(lambda x: x.sort_values(by='Mcap', ascending=False).head(500))\n",
    "        # Keep only the relevant columns\n",
    "        table = table[['Date', 'Symbol', 'PB', 'PE','Mcap']]\n",
    "        # Replace non-positive values with NaN for factors\n",
    "        factor = ['PB', 'PE']\n",
    "        table[factor] = table[factor].where(table[factor] > 0, np.nan)\n",
    "\n",
    "        # Calculate mean and standard deviation for each factor grouped by date\n",
    "        table['mean_PB'] = table.groupby('Date')['PB'].transform('mean')\n",
    "        table['mean_PE'] = table.groupby('Date')['PE'].transform('mean')\n",
    "        table['STD_PB'] = table.groupby('Date')['PB'].transform('std')\n",
    "        table['STD_PE'] = table.groupby('Date')['PE'].transform('std')\n",
    "\n",
    "        # Calculate inverted Z-Scores (higher values -> lower Z-scores)\n",
    "        table['Z_PE'] = -(table['PE'] - table['mean_PE']) / table['STD_PE']\n",
    "        table['Z_PB'] = -(table['PB'] - table['mean_PB']) / table['STD_PB']\n",
    "\n",
    "        # Calculate Weighted Average Z-Score\n",
    "        table['WeightedAvgZ'] = ((1/2) * table['Z_PE']) + ((1/2) * table['Z_PB'])\n",
    "\n",
    "        # Calculate Normalized Value Score\n",
    "        table['Value_score_cross'] = np.where(table['WeightedAvgZ'] >= 0,\n",
    "                                        1 + table['WeightedAvgZ'],\n",
    "                                        (1 - table['WeightedAvgZ'])**-1)\n",
    "\n",
    "        # Filter, rank, and finalize the table\n",
    "        table = table[['Date', 'Symbol', 'Value_score_cross', 'PB', 'PE', 'Mcap']].reset_index(drop=True)\n",
    "        table['Value_cross_rank'] = table.groupby('Date', group_keys=False)['Value_score_cross'].apply(lambda x: x.rank(pct=True, ascending=True))\n",
    "        # Filter top 500 companies by market cap for each date\n",
    "        table = table.groupby('Date', group_keys=False).apply(\n",
    "            lambda x: x.sort_values(by='Mcap', ascending=False).head(500)\n",
    "        )\n",
    "        # Keep only relevant columns\n",
    "        table = table[['Date', 'Symbol', 'PB', 'PE','Mcap','Value_score_cross','Value_cross_rank']]\n",
    "        # Replace non-positive values with NaN for factors\n",
    "        factors = ['PB', 'PE']\n",
    "        table[factors] = table[factors].where(table[factors] > 0, np.nan)\n",
    "        # Merge with sector mapping data\n",
    "        # SectorThemeGICS = pd.read_excel('SectorMapping.xlsx')\n",
    "        SectorThemeGICS = self.sectorData.copy()\n",
    "        table = pd.merge(table, SectorThemeGICS[['Symbol', 'Sector']], on='Symbol', how='left')\n",
    "        # Calculate sector-wise mean and std\n",
    "        for factor in factors:\n",
    "            table[f'mean_{factor}Peer'] = table.groupby(['Date', 'Sector'])[factor].transform('mean')\n",
    "            table[f'STD_{factor}Peer'] = table.groupby(['Date', 'Sector'])[factor].transform('std')\n",
    "\n",
    "        # Calculate inverted Z-scores for sector peers\n",
    "        table['Z_PBPeer'] = -(table['PB'] - table['mean_PBPeer']) / table['STD_PBPeer']\n",
    "        table['Z_PEPeer'] = -(table['PE'] - table['mean_PEPeer']) / table['STD_PEPeer']\n",
    "\n",
    "        # Calculate weighted average Z-score (equal weights)\n",
    "        table['WeightedAvgZPeer'] = ((1/2) * table['Z_PEPeer'] + (1/2) * table['Z_PBPeer'])\n",
    "\n",
    "        # Calculate normalized value score for sector peers\n",
    "        table['Value_score_Peer'] = np.where(\n",
    "            table['WeightedAvgZPeer'] >= 0,\n",
    "            1 + table['WeightedAvgZPeer'],\n",
    "            (1 - table['WeightedAvgZPeer'])**-1\n",
    "        )\n",
    "\n",
    "        # Calculate percentile rank within sector for each date\n",
    "        table['Value_pct_rank_Peer'] = table.groupby(['Date', 'Sector'], group_keys=False)['Value_score_Peer'].apply(lambda x: x.rank(pct=True, ascending=True))\n",
    "\n",
    "        # Finalize and clean table\n",
    "        table = table[['Date', 'Symbol', 'Sector', 'PB', 'PE','Value_score_cross','Value_cross_rank','Value_score_Peer', 'Value_pct_rank_Peer','Mcap']].reset_index(drop=True)\n",
    "        table = table.sort_values(['Symbol', 'Date'])\n",
    "        window = 252*3  # rolling window size (adjust for your data frequency)\n",
    "        # Replace non-positive values\n",
    "        for factor in ['PB', 'PE']:\n",
    "            table[factor] = table[factor].where(table[factor] > 0, np.nan)\n",
    "\n",
    "        # Rolling metrics per stock\n",
    "        for factor in ['PB', 'PE']:\n",
    "            table[f'mean_{factor}'] = table.groupby('Symbol')[factor].transform(lambda x: x.rolling(window).mean())\n",
    "            table[f'STD_{factor}'] = table.groupby('Symbol')[factor].transform(lambda x: x.rolling(window).std())\n",
    "\n",
    "        # Calculate Z-scores (inverted, so lower valuation  higher score)\n",
    "        for factor in ['PB', 'PE']:\n",
    "            table[f'Z_{factor}'] = -(table[factor] - table[f'mean_{factor}']) / table[f'STD_{factor}']\n",
    "\n",
    "        # Weighted average Z\n",
    "        table['WeightedAvgZ_time'] = 0.5 * table['Z_PE'] + 0.5 * table['Z_PB']\n",
    "\n",
    "        # Normalized Value Score (same as cross-section formula)\n",
    "        table['Value_score_time'] = np.where(table['WeightedAvgZ_time'] >= 0,\n",
    "                                            1 + table['WeightedAvgZ_time'],\n",
    "                                            (1 - table['WeightedAvgZ_time'])**-1)\n",
    "        table['Value_time_rank'] = table.groupby('Date', group_keys=False)['Value_score_cross'].apply(lambda x: x.rank(pct=True, ascending=True))\n",
    "\n",
    "        # Finalize: drop NaNs from rolling window\n",
    "        table = table[['Date', 'Symbol', 'Sector','Value_score_cross','Value_cross_rank','Value_score_Peer', 'Value_pct_rank_Peer','Value_score_time','Value_time_rank','Mcap']].reset_index(drop=True)\n",
    "        # Step 1: Calculate mean score\n",
    "        table['mean_score'] = table[['Value_score_cross', 'Value_score_Peer', 'Value_score_time']].mean(axis=1)\n",
    "        # Step 2: Calculate percentile rank (higher is better)\n",
    "        table['mean_score_percentile'] = table['mean_score'].rank(pct=True)\n",
    "        table = table[['Date','Symbol','mean_score_percentile']]\n",
    "        table.rename(columns={'mean_score_percentile': '3DValue'}, inplace=True)\n",
    "\n",
    "        # value final\n",
    "        top500 = table.filter([\"Symbol\", \"Date\",\"3DValue\"]).copy()\n",
    "\n",
    "        ## Convert the long from Dataframe to wide form dataframe\n",
    "        top500 = top500.pivot_table(index='Date',columns='Symbol',values='3DValue').reset_index()\n",
    "        ## Shifting the Date column\n",
    "        top500[\"Date\"] = top500[\"Date\"].shift(-1)\n",
    "        ## Fill NaN value of date with todays date.\n",
    "        top500.iloc[-1, top500.columns.get_loc(\"Date\")] = pd.to_datetime(date.today())\n",
    "\n",
    "        ## Re-converting the wide form dataframe to long form dataframe\n",
    "        top500 = top500.melt(id_vars='Date', value_name = \"3DValue\")\n",
    "        ## Drop na and reset the index\n",
    "        top500 = top500.dropna().reset_index(drop = True)\n",
    "        top500 = top500[top500[\"Date\"] >= \"2006-01-01\"].reset_index(drop = True).copy()\n",
    "        # top500\n",
    "        ## Store the Factor Data in dictionary.\n",
    "        self.factorData[\"3DValue\"] = top500.copy()\n",
    "        ## Filter the data for latest update or values.\n",
    "        self.recentFactorUpdate[\"3DValue\"] = top500[top500[\"Date\"] == top500[\"Date\"].max()].reset_index(drop = True)\n",
    "        ## Combining the Factors in one Dataframe\n",
    "        self.stockFactors = pd.concat([self.recentFactorUpdate[key].set_index([\"Date\", \"Symbol\"]) for key in self.recentFactorUpdate.keys()], axis = 1)\n",
    "        self.stockFactors.reset_index(inplace = True)\n",
    "\n",
    "        print(\"### 3D VALUE COMPLETE ###\")\n",
    "\n",
    "        del price_data, top500, table, SectorThemeGICS\n",
    "\n",
    "    def generate_AllFactors(self,save_appender = False):\n",
    "        # FUnction call for LTM Factor\n",
    "        self.generate_LTM()\n",
    "        # ## Function call for Momentum Factor\n",
    "        # self.generate_Momentum()\n",
    "        # ## Function call for Theme Factor\n",
    "        # self.generate_Theme()\n",
    "        # ## Function call for Volatility Factor\n",
    "        # self.generate_Volatility()\n",
    "        ## Function call for Value Factor\n",
    "        self.generate_ValueYield()\n",
    "        # Function call for Growth Factor\n",
    "        self.generate_Growth()\n",
    "        ## Function call for Quality Factor\n",
    "        self.generate_QualityAnnual()\n",
    "        ## Function call for Quality Factor\n",
    "        self.generate_QualityQuarter()\n",
    "        ## Function call for EM Factor\n",
    "        self.generate_EM()\n",
    "        # Function call for Dividend Factor\n",
    "        self.generate_Dividend()\n",
    "        ## Function call for LTMA Factor\n",
    "        self.generate_LTMA()\n",
    "        # # Function call for Low Vol Factor\n",
    "        self.generate_LowVol()\n",
    "        ## Function call for AM Factor\n",
    "        self.generate_AM()\n",
    "        ## Function call for UltraShort AM Factor\n",
    "        self.generate_UltraShortAM()\n",
    "        ## Function call for Short AM Factor\n",
    "        self.generate_ShortAM()\n",
    "        ## Function call for AM Factor\n",
    "        self.generate_ValueYieldNoPeg()\n",
    "        ## Function call for AM Factor\n",
    "        self.generate_ValueYieldExDiv()\n",
    "        ## Function call for Mid AM Factor\n",
    "        self.generate_MidAM()\n",
    "        ## Function call for Long AM Factor\n",
    "        self.generate_LongAM()\n",
    "        ## Function call for Multi AM Factor\n",
    "        self.generate_MultiAM()\n",
    "\n",
    "        self.generate_Beta()\n",
    "\n",
    "        self.generate_TrendAntitrendMR()\n",
    "\n",
    "        self.generate_ShiftedAM()\n",
    "\n",
    "        self.generate_3DQuality()\n",
    "\n",
    "        self.generate_3DValue()\n",
    "\n",
    "        # Extract unique dates from the 'Date' column of the strategyUniverse DataFrame.\n",
    "        # Drop duplicates and reset the index to create a clean, unique list of dates.\n",
    "        dates = self.strategyUniverse[[\"Date\"]].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "        # Create a new column 'DateO' which is the next date (shifted upwards by one row).\n",
    "        # This maps each date to the following date in the dataset.\n",
    "        dates[\"DateO\"] = dates[\"Date\"].shift(-1)\n",
    "\n",
    "        # Create a dictionary to map each date to its corresponding next date ('DateO').\n",
    "        dateMapper = dict(zip(dates[\"Date\"], dates[\"DateO\"]))\n",
    "\n",
    "        # Make a copy of the original strategyUniverse DataFrame to avoid modifying it directly.\n",
    "        strategyUniverse = self.strategyUniverse.copy()\n",
    "\n",
    "        # Map the 'DateO' column in the copied DataFrame using the dateMapper.\n",
    "        strategyUniverse[\"DateO\"] = strategyUniverse[\"Date\"].replace(dateMapper)\n",
    "\n",
    "        # Fill any missing values in 'DateO' (last row) with today's date.\n",
    "        strategyUniverse[\"DateO\"] = strategyUniverse[\"DateO\"].fillna(pd.to_datetime(date.today()))\n",
    "\n",
    "        # Drop the original 'Date' column as 'DateO' will now replace it.\n",
    "        strategyUniverse.drop(columns=[\"Date\"], inplace=True)\n",
    "\n",
    "        # Rename the 'DateO' column back to 'Date' to maintain consistency in column names.\n",
    "        strategyUniverse.rename(columns={'DateO': \"Date\"}, inplace=True)\n",
    "\n",
    "        # Reorder the columns so that the 'Date' column is the last one in the DataFrame.\n",
    "        strategyUniverse = strategyUniverse[list(strategyUniverse.columns[-1:]) + list(strategyUniverse.columns[:-2])]\n",
    "\n",
    "        ## Generating the Appender file only when all the factors are generated.\n",
    "        self.appender = pd.DataFrame()\n",
    "        for key in self.factorData.keys():\n",
    "            if len(self.appender) == 0:\n",
    "                self.appender = pd.merge(strategyUniverse,  self.factorData[key], on = [\"Date\", \"Symbol\"], how = \"left\")\n",
    "            else:\n",
    "                self.appender = pd.merge(self.appender, self.factorData[key], on = [\"Date\", \"Symbol\"], how= \"left\")\n",
    "\n",
    "        ## Save the appender file to csv only if the flag is True\n",
    "        if save_appender:\n",
    "            ## If there is no directory then create the directory\n",
    "            if not os.path.isdir(\"./DailyAppender\"):\n",
    "                os.mkdir(\"./DailyAppender\")\n",
    "\n",
    "            # self.appender.to_csv(self.uniqueFileName(f\"./DailyAppender/Appender_{date.today().strftime('%d%b%Y')}.csv\"), index = False)\n",
    "            self.appender.to_csv(\"./DailyAppender/Appender_latest.csv\", index = False)\n",
    "\n",
    "        print(\"### ALL THE FACTORS ARE GENERATED ###\")    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QualityDate = d.fetch_data_from_database(table_name = \"QualityDate\", no_of_years = 100)\n",
    "# QualityDate.to_csv('QualityDate.csv', index=False)\n",
    "# growth_date_latest = d.fetch_data_from_database(table_name = \"GrowthDate\", no_of_years=100)\n",
    "# growth_date_latest.to_csv('growth_date_latest.csv', index=False)\n",
    "# Finance_bs = d.fetch_data_from_database(table_name = \"Finance_bs\")\n",
    "# Finance_bs.to_csv(\"./Finance_bs.csv\", index=False)\n",
    "# Finance_fr = d.fetch_data_from_database(table_name = \"Finance_fr\")\n",
    "# Finance_fr.to_csv(\"./Finance_fr.csv\", index=False)\n",
    "# Finance_pl = d.fetch_data_from_database(table_name = \"Finance_pl\")\n",
    "# Finance_pl.to_csv('Finance_pl.csv', index=False)\n",
    "# # self.profitLossQuality = pd.read_csv(\"./Finance_pl.csv\")\n",
    "# Nifty_PE_PB = d.fetch_data_from_database(table_name = \"Nifty_PE_PB\", no_of_years = 30)\n",
    "# Nifty_PE_PB.to_csv('Nifty_PE_PB.csv', index=False)\n",
    "# # self.benchmarkValueData = pd.read_csv(\"./RAW_DATA/Nifty_PE_PB.csv\")\n",
    "# Value_Data_test = d.fetch_data_from_database(table_name = \"Value_RawData_Test\", no_of_years = 30)\n",
    "# Value_Data_test.to_csv('Value_Data.csv', index=False)\n",
    "# Finance_Cf = d.fetch_data_from_database(table_name = \"Finance_cf\")\n",
    "# Finance_Cf.to_csv('Finance_Cf.csv', index=False)\n",
    "# Quarterly = d.fetch_data_from_database(table_name = \"Quarterly\")\n",
    "# Quarterly.to_csv('Quarterly.csv', index=False)\n",
    "# CM = d.fetch_data_from_database(table_name = \"Companymaster\")\n",
    "# CM.to_csv('CM.csv', index=False)\n",
    "# price_data = d.fetch_price_data(no_of_years = 30, inc_indices=True)\n",
    "# price_data.to_csv('price_data.csv', index=False)\n",
    "# etf_indices = d.fetch_data_from_database(table_name=\"Etf_Indices\")\n",
    "# etf_indices.to_csv('etf_indices.csv', index=False)\n",
    "# Universe = d.fetch_data_from_database(table_name=\"universe_mcap_500\", no_of_years=50)\n",
    "# Universe.to_csv('Universe.csv', index=False)\n",
    "# gics = d.fetch_data_from_database(table_name=\"SectorThemeGics\")\n",
    "# gics.to_csv('gics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "import functools as ft\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "periodMapping = {\n",
    "                    'LTM'        : {\"1Y\" : {\"S\" : 0,   \"F\" : 252},\n",
    "                                    \"3Y\" : {\"S\" : 252, \"F\" : 500},\n",
    "                                    \"5Y\" : {\"S\" : 750, \"F\" : 500}},\n",
    "\n",
    "                    'Momentum'   : {'1W' : 5, \"1M\" : 20, \n",
    "                                    \"3M\" : 62, \"6M\" : 125, \n",
    "                                    \"1Y\" : 250,\"2Y\" : 500,  \n",
    "                                    \"3Y\" : 750},\n",
    "\n",
    "                    'Volatility' : {'1W' : 5, \"2W\" : 10, \n",
    "                                    \"1M\" : 20, \"3M\" : 62, \n",
    "                                    \"6M\" : 125, \"1Y\" : 250, \n",
    "                                    \"3Y\" : 750},\n",
    "                        \n",
    "                    \"Theme\"      : {'1W' : 5, \"2W\" : 10, \"2M\" : 40,\n",
    "                                    \"1M\" : 20, \"3M\" : 62,\n",
    "                                    \"1Y\" : 250, \"6M\" : 125,\n",
    "                                    \"3Y\" : 750}\n",
    "                }\n",
    "\n",
    "\n",
    "class EquityFactorsConsol:\n",
    "\n",
    "    def __init__(self, noOfYears, correct, peer = \"Theme\"):\n",
    "        ## Initialize the variables\n",
    "        self.stockPriceData = None\n",
    "        self.stockValueData = None\n",
    "        self.sectorData = None\n",
    "        self.benchmark = None\n",
    "        self.stockFactors = None\n",
    "        self.themeFactor = None\n",
    "        self.companyMaster = None\n",
    "        self.cashFlow = None\n",
    "        self.profitLossGrowth = None\n",
    "        self.profitLossQuality = None\n",
    "        self.financialRatio = None\n",
    "        self.appender = None\n",
    "        self.financeBalanceSheet = None\n",
    "\n",
    "        self.factorData = dict()\n",
    "        self.recentFactorUpdate = dict()\n",
    "        # self.__dataCursor = Data()\n",
    "\n",
    "        self.noOfYears = noOfYears\n",
    "        self.correct = correct\n",
    "        self.peer = peer\n",
    "\n",
    "    def uniqueFileName(self, filename):\n",
    "        \"\"\"\n",
    "        Generates a unique filename by appending a counter to the base name if a file with \n",
    "        the specified filename already exists in the current directory.\n",
    "\n",
    "        Args:\n",
    "            filename (str): The desired filename.\n",
    "\n",
    "        Returns:\n",
    "            str: A unique filename that does not already exist in the current directory.\n",
    "        \"\"\"\n",
    "\n",
    "        # Split the path into directory and filename\n",
    "        directory, file_name = os.path.split(filename)\n",
    "        \n",
    "        # Split the filename into name and extension\n",
    "        name, ext = os.path.splitext(file_name)\n",
    "        \n",
    "        # Initialize counter\n",
    "        counter = 1\n",
    "        \n",
    "        # Generate the full path\n",
    "        full_path = os.path.join(directory, file_name)\n",
    "        \n",
    "        # Check if the file already exists\n",
    "        while os.path.exists(full_path):\n",
    "            # If it does, create a new filename with the counter\n",
    "            new_filename = f\"{name}_{counter}{ext}\"\n",
    "            full_path = os.path.join(directory, new_filename)\n",
    "            counter += 1\n",
    "            \n",
    "        return full_path\n",
    "\n",
    "    def __readPriceData(self,):\n",
    "        ''' Function  is used to read the price data '''\n",
    "        ## Read the price Data.\n",
    "        # self.stockPriceData = self.__dataCursor.fetch_price_data(no_of_years = self.noOfYears, inc_indices=True)\n",
    "        self.stockPriceData = pd.read_csv(\"./price_data.csv\")\n",
    "\n",
    "        self.stockPriceData[\"Date\"] = pd.to_datetime(self.stockPriceData[\"Date\"])\n",
    "\n",
    "        # if self.correct:\n",
    "        #     # Replace The Old Symbol With the New Symbol\n",
    "        #     mapping = pd.read_excel(\"company_master_mapping.xlsx\")\n",
    "        #     mapping = dict(zip(mapping[\"SYMBOL_NSE\"], mapping[\"SYMBOL_CM\"]))\n",
    "        #     self.stockPriceData[\"Symbol\"] = self.stockPriceData[\"Symbol\"].replace(mapping)\n",
    "\n",
    "        self.stockPriceData[\"Symbol\"] = self.stockPriceData[\"Symbol\"].str.strip()\n",
    "        self.stockPriceData1 = self.stockPriceData.copy()\n",
    "        self.stockPriceDataMom = self.stockPriceData.copy()\n",
    "        ## Drop the Rows with NaN Values\n",
    "        self.stockPriceData = self.stockPriceData.dropna()\n",
    "        ## Remove the data of Erroneous Date.\n",
    "        self.stockPriceData = self.stockPriceData[self.stockPriceData[\"Date\"] != \"2022-03-07\"]#.reset_index(drop = True)\n",
    "\n",
    "        ## Filter the data for the Benchmark data\n",
    "        self.benchmark = self.stockPriceData[self.stockPriceData[\"Symbol\"] == \"NIFTY500\"].reset_index(drop = True)\n",
    "        ## Filter the necessary columns for benchmark\n",
    "        self.benchmark = self.benchmark.filter([\"Date\",\"Close\"])\n",
    "        ## Calculate the daily percentage change for the benchmark and renaming the columns.\n",
    "        self.benchmark[\"BenchmarkReturn\"] = self.benchmark[\"Close\"].pct_change()\n",
    "        self.benchmark.rename(columns = {'Close' : \"BenchmarkPrice\"}, inplace = True)\n",
    "\n",
    "        ## Fetch the ETF Indices List\n",
    "        # self.etf = self.__dataCursor.fetch_data_from_database(table_name=\"Etf_Indices\")\n",
    "        self.etf = pd.read_csv(\"./etf_indices.csv\")\n",
    "        ## Remove the ETF Indices data from the stock data\n",
    "        self.stockPriceData = self.stockPriceData[~self.stockPriceData['Symbol'].isin(self.etf['Symbol'])]\n",
    "\n",
    "        self.priceDataLTM = self.stockPriceData.copy()\n",
    "        ## Remove the data for the Saturday and Sunday and Sort the dataframe\n",
    "        self.stockPriceData = self.stockPriceData[~self.stockPriceData[\"Date\"].dt.day_name().isin(['Sunday', \"Saturday\"])]\n",
    "        self.stockPriceData = self.stockPriceData.sort_values([\"Symbol\", \"Date\"]).reset_index(drop = True)\n",
    "\n",
    "        # ## Identify Companies which have been in Top-500 historically.\n",
    "        # top500Companies  = self.stockPriceData.groupby([\"Date\"]).apply(lambda x: x.sort_values(\"Mcap\", ascending=False).head(500), include_groups = False).reset_index(level = 0).reset_index(drop = True)\n",
    "        # self.top500Companies = top500Companies[\"Symbol\"].unique().tolist()\n",
    "        # ## Identify Companies which have been in TOP-1000 historically.\n",
    "        # top1000Companies  = self.stockPriceData.groupby([\"Date\"]).apply(lambda x: x.sort_values(\"Mcap\", ascending=False).head(1000), include_groups=False).reset_index(level = 0).reset_index(drop = True)\n",
    "        # self.top1000Companies = top1000Companies[\"Symbol\"].unique().tolist()\n",
    "\n",
    "        self.strategyUniverse = pd.read_csv(\"Universe.csv\")\n",
    "        # self.strategyUniverse = self.__dataCursor.fetch_data_from_database(table_name=\"universe_mcap_500\", no_of_years=50)\n",
    "        self.strategyUniverse[\"Date\"] = pd.to_datetime(self.strategyUniverse[\"Date\"])\n",
    "        self.strategyUniverse[\"Peer\"] = self.strategyUniverse[self.peer].copy()\n",
    "        self.strategyUniverse.rename(columns = {\"MCAP\" : \"Mcap\"}, inplace = True)\n",
    "\n",
    "        # sectordata = self.__dataCursor.fetch_data_from_database(table_name=\"SectorThemeGics\")\n",
    "        # self.strategyUniverse = pd.merge(self.strategyUniverse, sectordata, on = \"Symbol\", how = \"left\")\n",
    "        # self.strategyUniverse[[\"Theme\", \"Sector\"]] = self.strategyUniverse[[\"Theme\", \"Sector\"]].fillna(\"Others\")\n",
    "        # self.strategyUniverse[\"Peer\"] = self.strategyUniverse[self.peer].copy()\n",
    "\n",
    "    def __readSectorData(self,):\n",
    "        ## Inout the Sector Mapping data\n",
    "        # self.sectorData = self.__dataCursor.fetch_data_from_database(table_name=\"SectorThemeGics\")\n",
    "        self.sectorData = pd.read_csv(\"./gics.csv\")\n",
    "\n",
    "    def __readCompanyMaster(self,):\n",
    "        # self.companyMaster = self.__dataCursor.fetch_data_from_database(table_name = \"Companymaster\")\n",
    "        self.companyMaster = pd.read_csv(\"./CM.csv\")\n",
    "        self.companyMaster = self.companyMaster[self.companyMaster[\"SERIES\"] == \"EQ\"]\n",
    "\n",
    "        self.companyMaster = self.companyMaster[[\"FINCODE\", \"SYMBOL\"]].copy()\n",
    "        self.companyMaster = self.companyMaster.dropna().reset_index(drop = True)\n",
    "        self.companyMaster.rename(columns = {'SYMBOL' : \"Symbol\"}, inplace = True)\n",
    "\n",
    "    def __readProfitLossGrowth(self,):\n",
    "        # self.profitLossGrowth = self.__dataCursor.fetch_data_from_database(table_name = \"Quarterly_Cons\")\n",
    "        self.profitLossGrowth = pd.read_csv(\"./Quarterly_Cons.csv\")\n",
    "        self.profitLossGrowth = self.profitLossGrowth[self.profitLossGrowth[\"Result_Type\"] == \"Q\"].reset_index(drop = True)\n",
    "        self.profitLossGrowth = self.profitLossGrowth.filter(items = [\"Fincode\", \"Date_End\",  \"PAT\", \"OPERATING_PROFIT\", \"GROSS_PROFIT\",\n",
    "                                                                      \"NET_SALES\", \"EPS_DILUTED\", \"PBT\", 'Debt/Equity Ratio', 'Adj_eps_abs', \"Dividend payout ratio\"])\n",
    "        self.profitLossGrowth.rename(columns = {'Fincode' : \"FINCODE\"}, inplace = True)\n",
    "\n",
    "    def __readCashFlow(self, ):\n",
    "        # self.cashFlow = self.__dataCursor.fetch_data_from_database(table_name = \"Finance_cons_cf\")\n",
    "        self.cashFlow = pd.read_csv(\"./Finance_cons_cf.csv\")\n",
    "\n",
    "    def __readValueData(self):\n",
    "            ## Input the Company Valuation Data.\n",
    "\n",
    "            # if self.correct:\n",
    "            #     self.stockValueData = self.__dataCursor.fetch_data_from_database(table_name = \"Value_RawData_Test\", no_of_years = self.noOfYears)\n",
    "            # else:\n",
    "            #     self.stockValueData = self.__dataCursor.fetch_data_from_database(table_name = \"Value_RawData\", no_of_years = self.noOfYears)\n",
    "            \n",
    "            self.stockValueData = pd.read_csv(\"./Value_Data.csv\")\n",
    "            ## COnvert the Date column from string to Datetime format\n",
    "            self.stockValueData[\"Date\"] = pd.to_datetime(self.stockValueData[\"Date\"])\n",
    "\n",
    "            # Import the Nifty Valuation Fields\n",
    "            # self.benchmarkValueData = self.__dataCursor.fetch_data_from_database(table_name = \"Nifty_PE_PB\", no_of_years = self.noOfYears)\n",
    "            self.benchmarkValueData = pd.read_csv(\"./Nifty_PE_PB.csv\")\n",
    "            self.benchmarkValueData[\"Date\"] = pd.to_datetime(self.benchmarkValueData[\"Date\"])\n",
    "\n",
    "    def __readProfitLossQuality(self,):\n",
    "        # self.profitLossQuality = self.__dataCursor.fetch_data_from_database(table_name = \"Finance_cons_pl\")\n",
    "        self.profitLossQuality = pd.read_csv(\"./Finance_cons_pl.csv\")\n",
    "        self.profitLossQuality = self.profitLossQuality[[\"FINCODE\", \"Year_end\", \"Profit_after_tax\", \"Net_sales\", \n",
    "                                                        \"Operating_profit\", \"Gross_profits\", \"Adj_Eps\"]]\n",
    "\n",
    "    def __readFinancialRatio(self,):\n",
    "        # self.financialRatio = self.__dataCursor.fetch_data_from_database(table_name = \"Finance_cons_fr\")\n",
    "\n",
    "        self.financialRatio = pd.read_csv(\"./Finance_cons_fr.csv\")\n",
    "        self.financialRatio = self.financialRatio[[\"FINCODE\", \"Year_end\", \"Inventory_Days\", \"Receivable_days\", \"Payable_days\", \"ROE\", \"ROCE\", \n",
    "                                                   \"Total_Debt_Equity\", \"Interest_Cover\", \"CEPS\", \"ROA\", \"FCF_Share\", \"Dividend_Payout_Per\"]]\n",
    "        self.financialRatio = self.financialRatio[self.financialRatio[\"Year_end\"] >= 200012]\n",
    "        self.financialRatio = self.financialRatio.reset_index(drop = True)\n",
    "\n",
    "    def __readFinanceBalanceSheet(self,):\n",
    "        # self.financeBalanceSheet = self.__dataCursor.fetch_data_from_database(table_name = \"Finance_cons_bs\")\n",
    "        self.financeBalanceSheet = pd.read_csv(\"./Finance_cons_bs.csv\")\n",
    "        # self.financeBalanceSheet = self.financeBalanceSheet[[\"FINCODE\", \"Year_end\", \"Profit_after_tax\", \"Net_sales\", \n",
    "        #                                                  \"Operating_profit\", \"Gross_profits\", \"Adj_Eps\"]]\n",
    "        self.financeBalanceSheet.rename(columns = {\"Fincode\" : \"FINCODE\"}, inplace = True)\n",
    "\n",
    "    def generate_LowVol(self,):\n",
    "\n",
    "        ################\n",
    "        ## PRICE DATA ##\n",
    "        ################\n",
    "        # Check if stock price data is loaded; if not, read it\n",
    "        if self.stockPriceData is None:\n",
    "            self.__readPriceData()\n",
    "\n",
    "        ## Drop the Unnecessary COlumns\n",
    "        priceData = self.stockPriceData.drop(columns = [\"Open\", \"High\", \"Low\", \"Volume\", \"Mcap\"]).copy()\n",
    "        \n",
    "        ## Filter the Symbol which are present in strategy universe\n",
    "        priceData = priceData[priceData[\"Symbol\"].isin(self.strategyUniverse[\"Symbol\"].unique())].reset_index(drop = True)\n",
    "\n",
    "        # Calculate daily returns for each stock by symbol, based on \"Close\" prices\n",
    "        priceData[\"Returns\"] = priceData.groupby(\"Symbol\")[\"Close\"].pct_change()\n",
    "        \n",
    "        # Calculate squared returns for down days only; store in \"DReturns\" (for downside volatility)\n",
    "        priceData[\"DReturns\"] = np.square(np.where(priceData[\"Returns\"] < 0, priceData[\"Returns\"], 0))\n",
    "\n",
    "        # Calculate 1-year rolling average of downside returns for each symbol and take the square root\n",
    "        priceData[\"DownVol\"] = priceData.groupby(\"Symbol\")[\"DReturns\"].transform(lambda x: x.rolling(252).mean())\n",
    "        priceData[\"DownVol\"] = np.sqrt(priceData[\"DownVol\"])\n",
    "\n",
    "        # Calculate 1-year rolling standard deviation of returns as low volatility measure\n",
    "        priceData['LowVol'] = priceData.groupby(\"Symbol\")[\"Returns\"].transform(lambda x: x.rolling(window=252).std())\n",
    "\n",
    "        # Calculate average volatility as the mean of LowVol and DownVol\n",
    "        priceData[\"AvgVol\"] = priceData[[\"LowVol\", \"DownVol\"]].mean(axis=1)\n",
    "\n",
    "        ## Merge the Computed Metrics against the symbol on each day's universe\n",
    "        priceData = pd.merge(self.strategyUniverse, priceData, on = [\"Date\", \"Symbol\"], how = \"left\")\n",
    "\n",
    "        # Rank stocks daily within the top 500 by LowVol, DownVol, and AvgVol, assigning percentile ranks\n",
    "        priceData[[\"LowVolRank\", \"DownVolRank\", \"AvgVolRank\"]] = priceData.groupby(\"Date\")[[\"LowVol\", \"DownVol\", \"AvgVol\"]].rank(ascending=False, pct=True)\n",
    "    \n",
    "        # Compute average volatilities for each theme on each date\n",
    "        themeScore = priceData.groupby([\"Peer\", \"Date\"])[[\"LowVol\", \"DownVol\", \"AvgVol\"]].mean()\n",
    "        \n",
    "        # Rename columns to reflect theme-based volatility measures\n",
    "        themeScore.columns = [\"PeerLowVol\", \"PeerDownVol\", \"PeerAvgVol\"]\n",
    "\n",
    "        # Merge theme scores with price data\n",
    "        priceData = pd.concat([priceData.set_index([\"Peer\", \"Date\"]), themeScore], axis=1).reset_index()\n",
    "\n",
    "        # Rank theme-based volatilities across all themes daily\n",
    "        priceData[[\"PeerLowVol\", \"PeerDownVol\", \"PeerAvgVol\"]] = priceData.groupby([\"Date\"])[[\"PeerLowVol\", \"PeerDownVol\", \"PeerAvgVol\"]].rank(ascending=False, pct=True)\n",
    "\n",
    "        # Select relevant columns and rename to remove \"Rank\" suffix for final output\n",
    "        df = priceData[[\"Date\", \"Symbol\", \"LowVolRank\", \"DownVolRank\", \"AvgVolRank\", \"PeerLowVol\", \"PeerDownVol\", \"PeerAvgVol\"]].copy()\n",
    "        df.columns = df.columns.str.replace(\"Rank\", \"\")\n",
    "\n",
    "        # Filter data from January 1, 2006, onwards and reset index\n",
    "        df = df[df[\"Date\"] >= \"2006-01-01\"].reset_index(drop=True)\n",
    " \n",
    "        # Initialize an empty list to store the transformed data for each volatility column\n",
    "        result = list()\n",
    "\n",
    "        # Iterate over each volatility-related column\n",
    "        for col in [\"LowVol\", \"DownVol\", \"AvgVol\", \"PeerLowVol\", \"PeerDownVol\", \"PeerAvgVol\"]:\n",
    "\n",
    "            # Filter the dataframe to keep only the \"Symbol\", \"Date\", and current column\n",
    "            temp = df.filter([\"Symbol\", \"Date\", col])\n",
    "\n",
    "            # Pivot the table to have dates as rows and symbols as columns, with values from the current column\n",
    "            temp = temp.pivot_table(index='Date', columns='Symbol', values=col).reset_index()\n",
    "\n",
    "            # Shift the \"Date\" column up by one row to align data with the next date\n",
    "            temp[\"Date\"] = temp[\"Date\"].shift(-1)\n",
    "\n",
    "            # Set the last row's \"Date\" to today's date to capture current data\n",
    "            temp.iloc[-1, temp.columns.get_loc(\"Date\")] = pd.to_datetime(date.today())\n",
    "\n",
    "            # Unpivot the table to return it to long format with \"Date\", \"Symbol\", and current column values\n",
    "            temp = temp.melt(id_vars='Date', value_name=col)\n",
    "\n",
    "            # Drop rows with missing values and reset the index\n",
    "            temp = temp.dropna().reset_index(drop=True)\n",
    "\n",
    "            # Set the index to \"Date\" and \"Symbol\" for each transformed column and add to the result list\n",
    "            result.append(temp.set_index([\"Date\", \"Symbol\"]))\n",
    "\n",
    "        # Concatenate all transformed columns along the columns axis to create a combined dataframe\n",
    "        scores = pd.concat(result, axis=1).reset_index()\n",
    "\n",
    "        # Sort the final dataframe by \"Symbol\" and \"Date\" columns for organized viewing and reset the index\n",
    "        scores = scores.sort_values([\"Symbol\", \"Date\"]).reset_index(drop=True)\n",
    "        scores.columns = scores.columns.str.replace(\"Peer\", self.peer)\n",
    "\n",
    "        ## Store the Factor Data in dictionary.\n",
    "        self.factorData[\"LowVol\"] = scores.copy()\n",
    "\n",
    "        ## Filter the data for latest update or values.\n",
    "        self.recentFactorUpdate[\"LowVol\"] = scores[scores[\"Date\"] == scores[\"Date\"].max()].reset_index(drop = True)\n",
    "        \n",
    "        ## Combining the Factors in one Dataframe\n",
    "        self.stockFactors = pd.concat([self.recentFactorUpdate[key].set_index([\"Date\", \"Symbol\"]) for key in self.recentFactorUpdate.keys()], axis = 1)\n",
    "        self.stockFactors.reset_index(inplace = True)\n",
    "\n",
    "        print(\"### LOWVOL COMPLETE ###\")\n",
    "\n",
    "        del priceData, themeScore, df, scores, result\n",
    "\n",
    "    def generate_AM(self,):\n",
    "\n",
    "        # Function to calculate log returns\n",
    "        def calculate_log_returns(df):\n",
    "            df['LogReturn'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "            return df.dropna()\n",
    "\n",
    "        # Function to calculate annualized standard deviation\n",
    "        def calculate_annualized_std(df, window=252):\n",
    "            return df['LogReturn'].rolling(window).std() * np.sqrt(window)\n",
    "\n",
    "        # Function to calculate momentum ratios\n",
    "        def calculate_momentum_ratios(series, period):\n",
    "            return series / series.shift(period) - 1\n",
    "\n",
    "        ################\n",
    "        ## PRICE DATA ##\n",
    "        ################\n",
    "        # Check if stock price data is loaded; if not, read it\n",
    "        if self.stockPriceData is None:\n",
    "            self.__readPriceData()\n",
    "\n",
    "        ## Drop the Unnecessary COlumns\n",
    "        priceData = self.stockPriceData.drop(columns = [\"Open\", \"High\", \"Low\", \"Volume\", \"Mcap\"]).copy()\n",
    "        \n",
    "        ## Filter the Symbol which are present in strategy universe\n",
    "        priceData = priceData[priceData[\"Symbol\"].isin(self.strategyUniverse[\"Symbol\"].unique())].reset_index(drop = True)\n",
    "\n",
    "        # Define periods for momentum ratios (in trading days)\n",
    "        periods = {\n",
    "            'MR1': 21,   # 1 month\n",
    "            'MR2': 42,   # 2 months\n",
    "            'MR3': 63,   # 3 months\n",
    "            'MR6': 126,  # 6 months\n",
    "            'MR12': 252, # 12 months\n",
    "        }\n",
    "\n",
    "        # Apply log return calculation\n",
    "        priceData = priceData.groupby('Symbol', group_keys=False).apply(calculate_log_returns)\n",
    "    \n",
    "        # Calculate momentum ratios for each period\n",
    "        for label, period in periods.items():\n",
    "            priceData[label] = priceData.groupby('Symbol')['Close'].transform(lambda x: calculate_momentum_ratios(x, period))\n",
    "\n",
    "        # Calculate annualized standard deviation\n",
    "        priceData['AnnualizedStd'] = priceData.groupby('Symbol', group_keys=False).apply(calculate_annualized_std)\n",
    "\n",
    "        # Normalize the momentum ratios by dividing by the annualized standard deviation\n",
    "        for label in periods.keys():\n",
    "            priceData[label] /= priceData['AnnualizedStd']\n",
    "\n",
    "        priceData = priceData.sort_values([\"Date\", \"Symbol\"]).reset_index(drop = True)\n",
    "\n",
    "        # Calculate the mean and std deviation of each momentum ratio across the universe\n",
    "        for label in periods.keys():\n",
    "            priceData[f'mu_{label}'] = priceData.groupby('Date')[label].transform(lambda x: x.mean())\n",
    "            priceData[f'sigma_{label}'] = priceData.groupby('Date')[label].transform(lambda x: x.std())\n",
    "\n",
    "        # Calculate Z-scores for each period\n",
    "        for label in periods.keys():\n",
    "            priceData[f'Z_{label}'] = (priceData[label] - priceData[f'mu_{label}']) / priceData[f'sigma_{label}']\n",
    "\n",
    "        # Define specific combinations for which to calculate the final Z-scores\n",
    "        metrics = ['Z_MR1', 'Z_MR2', 'Z_MR3', 'Z_MR6', 'Z_MR12']\n",
    "\n",
    "        # Weighted average Z-score\n",
    "        priceData[\"WtdZScore\"] =priceData[metrics].mean(axis= 1)\n",
    "\n",
    "        # Normalized momentum score\n",
    "        priceData[f'Momentum'] = np.where(priceData[f'WtdZScore'] >= 0,\n",
    "                                                1 + priceData[f'WtdZScore'],\n",
    "                                                (1 - priceData[f'WtdZScore']) ** -1)\n",
    "\n",
    "        ## Filter the Required Columns  \n",
    "        priceData = priceData.filter(items = [\"Date\", \"Symbol\", \"Momentum\"])\n",
    "\n",
    "        ## Merge the Computed Metrics against the symbol on each day's universe\n",
    "        priceData = pd.merge(self.strategyUniverse, priceData, on = [\"Date\", \"Symbol\"], how = \"left\")\n",
    "\n",
    "        ## Computing the Percentile Score of the Symbols on each Date\n",
    "        priceData[\"Momentum\"] = priceData.groupby([\"Date\"])[\"Momentum\"].rank(ascending = True, pct = True)\n",
    "\n",
    "        ## Computing the Aggreate rank of Peer (Theme / Sector / GICS) on each date using Symbol\n",
    "        priceData[\"PeerMomentum\"] = priceData.groupby([\"Date\", \"Peer\"])[\"Momentum\"].transform(lambda x: x.mean())\n",
    "\n",
    "        ## Re-Ranki The Agg.Score of Peer (Theme / Sector / GICS) on each Date.\n",
    "        priceData[\"PeerMomentum\"] = priceData.groupby([\"Date\"])[\"PeerMomentum\"].rank(ascending = True, pct = True)\n",
    "\n",
    "        ## Filter the data after year 2006\n",
    "        priceData = priceData[priceData[\"Date\"] >= \"2006-01-01\"].reset_index(drop = True)\n",
    "         \n",
    "        # Initialize an empty list to store the transformed data for each volatility column\n",
    "        result = list()\n",
    "\n",
    "        # Iterate over each volatility-related column\n",
    "        for col in [\"Momentum\", \"PeerMomentum\"]:\n",
    "\n",
    "            # Filter the dataframe to keep only the \"Symbol\", \"Date\", and current column\n",
    "            temp = priceData.filter([\"Symbol\", \"Date\", col])\n",
    "\n",
    "            # Pivot the table to have dates as rows and symbols as columns, with values from the current column\n",
    "            temp = temp.pivot_table(index='Date', columns='Symbol', values=col).reset_index()\n",
    "\n",
    "            # Shift the \"Date\" column up by one row to align data with the next date\n",
    "            temp[\"Date\"] = temp[\"Date\"].shift(-1)\n",
    "\n",
    "            # Set the last row's \"Date\" to today's date to capture current data\n",
    "            temp.iloc[-1, temp.columns.get_loc(\"Date\")] = pd.to_datetime(date.today())\n",
    "\n",
    "            # Unpivot the table to return it to long format with \"Date\", \"Symbol\", and current column values\n",
    "            temp = temp.melt(id_vars='Date', value_name=col)\n",
    "\n",
    "            # Drop rows with missing values and reset the index\n",
    "            temp = temp.dropna().reset_index(drop=True)\n",
    "\n",
    "            # Filter data to include only records from January 1, 2006, onwards and reset the index\n",
    "            temp = temp[temp[\"Date\"] >= \"2006-01-01\"].reset_index(drop=True).copy()\n",
    "\n",
    "            # Set the index to \"Date\" and \"Symbol\" for each transformed column and add to the result list\n",
    "            result.append(temp.set_index([\"Date\", \"Symbol\"]))\n",
    "\n",
    "        \n",
    "        # Concatenate all transformed columns along the columns axis to create a combined dataframe\n",
    "        scores = pd.concat(result, axis=1).reset_index()\n",
    "        scores.columns = scores.columns.str.replace(\"Momentum\", \"AM\").str.replace(\"Peer\", self.peer)\n",
    "\n",
    "        # Sort the final dataframe by \"Symbol\" and \"Date\" columns for organized viewing and reset the index\n",
    "        scores = scores.sort_values([\"Symbol\", \"Date\"]).reset_index(drop=True)\n",
    "\n",
    "        ## Store the Factor Data in dictionary.\n",
    "        self.factorData[\"AM\"] = scores.copy()\n",
    "\n",
    "        ## Filter the data for latest update or values.\n",
    "        self.recentFactorUpdate[\"AM\"] = scores[scores[\"Date\"] == scores[\"Date\"].max()].reset_index(drop = True)\n",
    "        \n",
    "        ## Combining the Factors in one Dataframe\n",
    "        self.stockFactors = pd.concat([self.recentFactorUpdate[key].set_index([\"Date\", \"Symbol\"]) for key in self.recentFactorUpdate.keys()], axis = 1)\n",
    "        self.stockFactors.reset_index(inplace = True)\n",
    "\n",
    "        print(\"### MOMENTUM COMPLETE ###\")\n",
    "\n",
    "        del priceData, result, temp, scores\n",
    "    \n",
    "    def generate_LTMA(self,):\n",
    "\n",
    "        ## If Price is ot imported form Databse, then read it from DB.\n",
    "        if self.stockPriceData is None:\n",
    "            self.__readPriceData()\n",
    "        \n",
    "        ## Drop the Unnecessary COlumns\n",
    "        priceData = self.stockPriceData.drop(columns = [\"Open\", \"High\", \"Low\", \"Volume\", \"Mcap\"]).copy()\n",
    "        \n",
    "        ## Filter the Symbol which are present in strategy universe\n",
    "        priceData = priceData[priceData[\"Symbol\"].isin(self.strategyUniverse[\"Symbol\"].unique())].reset_index(drop = True)\n",
    "\n",
    "        # Sort the Dataframe by Symbol and Date, Reset the index\n",
    "        priceData = priceData.sort_values([\"Symbol\", \"Date\"]).reset_index(drop = True)\n",
    "\n",
    "        # Calculate 1-year, 3-year, and 5-year percentage changes in stock closing prices.\n",
    "        for key in [3, 6, 12, 18]:\n",
    "            priceData[f\"{key}_return\"] = priceData.groupby('Symbol')[\"Close\"].pct_change(22*key)\n",
    "            priceData[f\"{key}_vol\"] = priceData.groupby('Symbol')[\"Close\"].transform(lambda x: x.pct_change().rolling(key*22).std())\n",
    "            priceData[f\"{key}_vol\"] *= np.sqrt(252)\n",
    "            priceData[f\"{key}_sharpe\"] = priceData[f\"{key}_return\"] / priceData[f\"{key}_vol\"]\n",
    "            priceData.drop(columns = [f\"{key}_return\", f\"{key}_vol\"], inplace = True)\n",
    "\n",
    "        ## Sort and reset the index\n",
    "        priceData.sort_values(\"Date\", inplace = True)\n",
    "    \n",
    "        ## Merge the Computed Metrics against the symbol on each day's universe\n",
    "        priceData = pd.merge(self.strategyUniverse, priceData, on = [\"Date\", \"Symbol\"], how = \"left\")\n",
    "\n",
    "        ## Assigning the rank to each 500 companies on particular date.\n",
    "        for key in [3, 6, 12, 18]:\n",
    "            priceData[f\"{key}_Rank\"]=priceData.groupby('Date')[f\"{key}_sharpe\"].rank(pct=True)\n",
    "\n",
    "        ## Aggregate the rank for composite score.\n",
    "        priceData[\"LTMA\"] = priceData[[col for col in priceData.columns if \"Rank\" in col]].mean(axis = 1, skipna = False)\n",
    "\n",
    "        ## Filter out the necessary columns\n",
    "        priceData = priceData.filter(items = [\"Date\", \"Symbol\", \"LTMA\"])\n",
    "\n",
    "        ## Convert the long from Dataframe to wide form dataframe\n",
    "        priceData = priceData.pivot_table(index='Date',columns='Symbol',values='LTMA').reset_index()\n",
    "        ## Shifting the Date column\n",
    "        priceData[\"Date\"] = priceData[\"Date\"].shift(-1)\n",
    "        ## Fill NaN value of date with todays date.\n",
    "        priceData.iloc[-1, priceData.columns.get_loc(\"Date\")] = pd.to_datetime(date.today())\n",
    "\n",
    "        ## Re-converting the wide form dataframe to long form dataframe\n",
    "        priceData = priceData.melt(id_vars='Date', value_name = \"LTMA\")\n",
    "        ## Drop na and reset the index\n",
    "        priceData = priceData.dropna().reset_index(drop = True)\n",
    "        priceData = priceData[priceData[\"Date\"] >= \"2006-01-01\"].reset_index(drop = True).copy()\n",
    "\n",
    "        ## Store the Factor Data in dictionary.\n",
    "        self.factorData[\"LTMA\"] = priceData.copy()\n",
    "        ## Filter the data for latest update or values.\n",
    "        self.recentFactorUpdate[\"LTMA\"] = priceData[priceData[\"Date\"] == priceData[\"Date\"].max()].reset_index(drop = True)\n",
    "        ## Combining the Factors in one Dataframe\n",
    "        self.stockFactors = pd.concat([self.recentFactorUpdate[key].set_index([\"Date\", \"Symbol\"]) for key in self.recentFactorUpdate.keys() if key != \"Theme\"], axis = 1)\n",
    "        self.stockFactors.reset_index(inplace = True)\n",
    "\n",
    "        print(\"### LTMA COMPLETE ###\")\n",
    "\n",
    "        del priceData\n",
    "\n",
    "    def generate_LTM(self,):\n",
    "\n",
    "        ## If Price is ot imported form Databse, then read it from DB.\n",
    "        if self.stockPriceData is None:\n",
    "            self.__readPriceData()\n",
    "\n",
    "        ## Set the maping dictionary for LTM Factor.\n",
    "        mapper = periodMapping[\"LTM\"]\n",
    "\n",
    "        ## Function to calculate 3-year return after shifting by 100 days\n",
    "        def rollingReturn(group, shift, period, key):\n",
    "            group[f'{key}_Return'] = group['Close'].ffill().shift(shift).pct_change(period, fill_method = None)\n",
    "            return group\n",
    "            \n",
    "        ## Drop the Unnecessary COlumns\n",
    "        priceData = self.stockPriceData.drop(columns = [\"Open\", \"High\", \"Low\", \"Volume\", \"Mcap\"]).copy()\n",
    "        \n",
    "        ## Filter the Symbol which are present in strategy universe\n",
    "        priceData = priceData[priceData[\"Symbol\"].isin(self.strategyUniverse[\"Symbol\"].unique())].reset_index(drop = True)\n",
    "\n",
    "        # Sort the Dataframe by Symbol and Date, Reset the index\n",
    "        priceData = priceData.sort_values([\"Symbol\", \"Date\"]).reset_index(drop = True)\n",
    "\n",
    "        # Calculate 1-year, 3-year, and 5-year percentage changes in stock closing prices.\n",
    "        for key in mapper.keys():\n",
    "            priceData = priceData.groupby('Symbol').apply(rollingReturn, shift = mapper[key][\"S\"], \n",
    "                                                            period = mapper[key][\"F\"], \n",
    "                                                            key = key, include_groups = False).reset_index(level = 0)\n",
    "\n",
    "        ## Sort and reset the index\n",
    "        priceData.sort_values(\"Date\", inplace = True)\n",
    "    \n",
    "        ## Merge the Computed Metrics against the symbol on each day's universe\n",
    "        priceData = pd.merge(self.strategyUniverse, priceData, on = [\"Date\", \"Symbol\"], how = \"left\")\n",
    "\n",
    "        ## Assigning the rank to each 500 companies on particular date.\n",
    "        for key in mapper.keys():\n",
    "            priceData[f\"{key}_Rank\"]=priceData.groupby('Date')[f\"{key}_Return\"].rank(pct=True)\n",
    "        \n",
    "        ## Aggregate the rank for composite score.\n",
    "        priceData[\"LTM\"] = priceData[[col for col in priceData.columns if \"Rank\" in col]].mean(axis = 1, skipna = False)\n",
    "\n",
    "        ## Filter out the necessary columns\n",
    "        priceData = priceData.filter(items = [\"Date\", \"Symbol\", \"LTM\"])\n",
    "\n",
    "        ## Convert the long from Dataframe to wide form dataframe\n",
    "        priceData = priceData.pivot_table(index='Date',columns='Symbol',values='LTM').reset_index()\n",
    "        \n",
    "        ## Shifting the Date column\n",
    "        priceData[\"Date\"] = priceData[\"Date\"].shift(-1)\n",
    "\n",
    "        ## Fill NaN value of date with todays date.\n",
    "        priceData.iloc[-1, priceData.columns.get_loc(\"Date\")] = pd.to_datetime(date.today())\n",
    "\n",
    "        ## Re-converting the wide form dataframe to long form dataframe\n",
    "        priceData = priceData.melt(id_vars='Date', value_name = \"LTM\")\n",
    "\n",
    "        ## Drop na and reset the index\n",
    "        priceData = priceData.dropna().reset_index(drop = True)\n",
    "        priceData = priceData[priceData[\"Date\"] >= \"2006-01-01\"].reset_index(drop = True).copy()\n",
    "\n",
    "        ## Store the Factor Data in dictionary.\n",
    "        self.factorData[\"LTM\"] = priceData.copy()\n",
    "\n",
    "        ## Filter the data for latest update or values.\n",
    "        self.recentFactorUpdate[\"LTM\"] = priceData[priceData[\"Date\"] == priceData[\"Date\"].max()].reset_index(drop = True)\n",
    "        \n",
    "        ## Combining the Factors in one Dataframe\n",
    "        self.stockFactors = pd.concat([self.recentFactorUpdate[key].set_index([\"Date\", \"Symbol\"]) for key in self.recentFactorUpdate.keys() if key != \"Theme\"], axis = 1)\n",
    "        self.stockFactors.reset_index(inplace = True)\n",
    "\n",
    "        print(\"### LTM COMPLETE ###\")\n",
    "\n",
    "        del priceData\n",
    "\n",
    "    def generate_EM(self):\n",
    "        # RSI function\n",
    "        def rsi(closes, n):\n",
    "            diff_serie = closes.diff()\n",
    "            gain = diff_serie.where(diff_serie > 0, 0)\n",
    "            loss = -diff_serie.where(diff_serie < 0, 0)\n",
    "            avg_gain = gain.rolling(window=n).mean()\n",
    "            avg_loss = loss.rolling(window=n).mean()\n",
    "            rs = avg_gain / avg_loss\n",
    "            rsi = 100 - (100 / (1 + rs))\n",
    "            return rsi.fillna(0)\n",
    "\n",
    "        ################\n",
    "        ## PRICE DATA ##\n",
    "        ################\n",
    "        if self.stockPriceData is None:\n",
    "            self.__readPriceData()\n",
    "\n",
    "        ## Drop the Unnecessary COlumns\n",
    "        priceData = self.stockPriceData.drop(columns = [\"Open\", \"High\", \"Low\", \"Volume\", \"Mcap\"]).copy()\n",
    "        \n",
    "        ## Filter the Symbol which are present in strategy universe\n",
    "        priceData = priceData[priceData[\"Symbol\"].isin(self.strategyUniverse[\"Symbol\"].unique())].reset_index(drop = True)\n",
    "        priceData = priceData.sort_values([\"Symbol\", \"Date\"]).reset_index(drop = True)\n",
    "        \n",
    "        ## Calculate the 50 and 200 day moving average\n",
    "        priceData['200_ma'] = priceData.groupby('Symbol')['Close'].transform(lambda x: x.rolling(200).mean())\n",
    "        priceData['50_ma'] = priceData.groupby('Symbol')['Close'].transform(lambda x: x.rolling(50).mean())\n",
    "\n",
    "        # Calculate Close to 200DMA and 50DMA to 200DMA Ratio.\n",
    "        priceData['200_ma_ratio'] = priceData['Close'] / priceData['200_ma']\n",
    "        priceData['50_200_ratio'] = priceData['50_ma'] / priceData['200_ma']\n",
    "\n",
    "        # Compute the RSI\n",
    "        priceData['rsi'] = priceData.groupby('Symbol')['Close'].transform(lambda x: rsi(x, 14))\n",
    "\n",
    "        ## Compute the rank for each of the sub-factors\n",
    "        priceData['200_ma_ratio_score'] = priceData.groupby('Date')['200_ma_ratio'].rank(ascending=False, pct=True)\n",
    "        priceData['50_200_score'] = priceData.groupby('Date')['50_200_ratio'].rank(ascending=False, pct=True)\n",
    "        priceData['rsi_score'] = priceData.groupby('Date')['rsi'].rank(ascending=False, pct=True)\n",
    "\n",
    "        # Calculate final scores and sor the dataframe\n",
    "        priceData['finalScore'] = priceData[['200_ma_ratio_score', '50_200_score', 'rsi_score']].sum(axis = 1, skipna = False)\n",
    "\n",
    "        ## Merge the Computed Metrics against the symbol on each day's universe\n",
    "        priceData = pd.merge(self.strategyUniverse, priceData, on = [\"Date\", \"Symbol\"], how = \"left\")\n",
    "\n",
    "        ## Rank the top500 companies and filter the necessary columns\n",
    "        priceData['AM_New'] = priceData.groupby('Date')['finalScore'].rank(ascending=False, pct=True)\n",
    "        priceData = priceData.filter(items= ['Date','Symbol','AM_New'])\n",
    "\n",
    "        ## Filter the data after year 2006 and sor the dataframe.\n",
    "        priceData = priceData[priceData[\"Date\"] >= \"2006-01-01\"].copy()\n",
    "        priceData = priceData.sort_values([\"Symbol\", \"Date\"]).reset_index(drop = True)\n",
    "    \n",
    "        ## Convert the long from Dataframe to wide form dataframe\n",
    "        priceData = priceData.pivot_table(index='Date',columns='Symbol',values='AM_New').reset_index()\n",
    "        ## Shifting the Date column\n",
    "        priceData[\"Date\"] = priceData[\"Date\"].shift(-1)\n",
    "        ## Fill NaN value of date with todays date.\n",
    "        priceData.iloc[-1, priceData.columns.get_loc(\"Date\")] = pd.to_datetime(date.today())\n",
    "        ## Re-converting the wide form dataframe to long form dataframe\n",
    "        priceData = priceData.melt(id_vars='Date', value_name= \"EM\")\n",
    "        ## Drop na and reset the index\n",
    "        priceData = priceData.dropna().reset_index(drop = True)\n",
    "\n",
    "        ## Store the Factor Data in dictionary.\n",
    "        self.factorData[\"EM\"] = priceData.copy()\n",
    "        ## Filter the data for latest update or values.\n",
    "        self.recentFactorUpdate[\"EM\"] = priceData[priceData[\"Date\"] == priceData[\"Date\"].max()].reset_index(drop = True)\n",
    "        ## Combining the Factors in one Dataframe\n",
    "        self.stockFactors = pd.concat([self.recentFactorUpdate[key].set_index([\"Date\", \"Symbol\"]) for key in self.recentFactorUpdate.keys()], axis = 1)\n",
    "        self.stockFactors.reset_index(inplace = True)\n",
    "\n",
    "        print(\"### EM COMPLETE ###\")\n",
    "\n",
    "        del priceData\n",
    "\n",
    "    def generate_Dividend(self, valueYieldCall = False):\n",
    "\n",
    "        def previousQuarter(currDate):\n",
    "            if pd.Period(currDate,freq = \"Q\").end_time.date() == currDate.date():\n",
    "                return currDate\n",
    "            else:\n",
    "                return currDate - pd.offsets.QuarterEnd()\n",
    "            \n",
    "        #########################\n",
    "        ## COMPANY MASTER DATA ##\n",
    "        #########################\n",
    "        if self.companyMaster is None:\n",
    "            self.__readCompanyMaster()\n",
    "\n",
    "        if self.profitLossGrowth is None:\n",
    "            self.__readProfitLossGrowth() \n",
    "    \n",
    "        if self.stockPriceData is None:\n",
    "            self.__readPriceData()\n",
    "            \n",
    "        # Fetch the rebalance dates for the last 100 years from the \"GrowthDate\" table in the database\n",
    "        # rebalanceDates = self.__dataCursor.fetch_data_from_database(table_name = \"GrowthDate\", no_of_years=100)\n",
    "        rebalanceDates = pd.read_csv('growth_date_latest.csv', parse_dates=['Date'])\n",
    "\n",
    "        # Create a copy of the strategy universe (likely a dataframe or similar structure) to work with for company composition\n",
    "        composition  = self.strategyUniverse.copy()\n",
    "\n",
    "        # Extract the quarter and date values from the rebalanceDates dataframe\n",
    "        rebalanceQtr = list(rebalanceDates['Quarter'])\n",
    "        rebalanceDate = list(rebalanceDates['Date'])\n",
    "\n",
    "        # Create a dictionary mapping each date to its corresponding quarter and vice versa\n",
    "        qtrDateDict = dict(zip(rebalanceDate, rebalanceQtr))\n",
    "        dateQtrDict = dict(zip(rebalanceQtr, rebalanceDate))\n",
    "\n",
    "        # Add a new column 'Quarter' to the composition dataframe by mapping the 'Date' column to the respective quarter using qtrDateDict\n",
    "        composition['Quarter'] = composition['Date'].map(qtrDateDict)\n",
    "\n",
    "        # Filter the necessary columns ('FINCODE', 'Date_End', 'Dividend payout ratio', 'PAT') from the profitLossGrowth dataframe\n",
    "        div_qtr = self.profitLossGrowth.filter(items=[\"FINCODE\", \"Date_End\", \"Dividend payout ratio\", \"PAT\"])\n",
    "\n",
    "        # Merge div_qtr with companyMaster on the 'FINCODE' column to include company information\n",
    "        div_qtr = pd.merge(div_qtr, self.companyMaster, on=\"FINCODE\", how=\"inner\")\n",
    "\n",
    "        # Convert the 'Date_End' column to datetime format\n",
    "        div_qtr[\"Date_End\"] = pd.to_datetime(div_qtr['Date_End'], format=\"%Y%m\")\n",
    "\n",
    "        # Shift 'Date_End' to the respective month-end date\n",
    "        div_qtr[\"MonthEnd\"] = div_qtr[\"Date_End\"] + pd.offsets.MonthEnd()\n",
    "\n",
    "        # Create a new column 'QuarterEnd' to calculate the end of the financial quarter based on the 'MonthEnd'\n",
    "        div_qtr[\"QuarterEnd\"] = div_qtr[\"MonthEnd\"].apply(previousQuarter) \n",
    "\n",
    "        # Convert the 'QuarterEnd' to the Indian Financial Year format, which ends in March\n",
    "        div_qtr[\"YearQuarter\"] = div_qtr[\"QuarterEnd\"].dt.to_period(\"Q-MAR\")\n",
    "\n",
    "        # Extract the quarter and year from 'YearQuarter' and create a new 'Quarter' column\n",
    "        div_qtr[\"Quarter\"] = div_qtr[\"YearQuarter\"].astype(str).str[-2:] + div_qtr[\"YearQuarter\"].astype(str).str[:4]\n",
    "\n",
    "        # Extract the year from 'YearQuarter' and create a 'Year' column\n",
    "        div_qtr[\"Year\"] = div_qtr[\"YearQuarter\"].astype(str).str[:4].astype(int)\n",
    "\n",
    "        # Sort the dataframe by 'YearQuarter' in ascending order for chronological order\n",
    "        div_qtr.sort_values(\"YearQuarter\", ascending=True, inplace=True)\n",
    "        div_qtr.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # Calculate the dividend by multiplying 'PAT' (Profit After Tax) by the 'Dividend payout ratio'\n",
    "        div_qtr['dividend'] = div_qtr['PAT'] * div_qtr['Dividend payout ratio']\n",
    "\n",
    "        # Sort by 'Symbol' and 'MonthEnd' for proper chronological order within each company\n",
    "        div_qtr.sort_values(['Symbol', 'MonthEnd'], inplace=True)\n",
    "\n",
    "        # Calculate the rolling dividend over the last 4 quarters for each company\n",
    "        div_qtr['Rollingdividend'] = div_qtr.groupby('Symbol')['dividend'].transform(lambda x: x.rolling(4).sum())\n",
    "\n",
    "        # Sort the dataframe again by 'Symbol' and 'YearQuarter', and reset the index\n",
    "        div_qtr.sort_values([\"Symbol\", \"YearQuarter\"], inplace=True)\n",
    "        div_qtr.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # Map the 'Quarter' to its respective 'Date' using the dateQtrDict for final data\n",
    "        div_qtr['Date'] = div_qtr['Quarter'].map(dateQtrDict)\n",
    "\n",
    "        # Merge the 'div_qtr' with the 'composition' dataframe on 'Symbol', 'Date', and 'Quarter' to get the final composition\n",
    "        div_qtr = pd.merge(div_qtr, composition, on=['Symbol', 'Date', 'Quarter'])\n",
    "\n",
    "        # Extract the year from the 'Date' column and assign it to a new 'Year' column\n",
    "        div_qtr['Year'] = div_qtr['Date'].dt.year\n",
    "\n",
    "        # Sort the dataframe by 'Date' and 'Symbol' for easier analysis and reset the index\n",
    "        div_qtr = div_qtr.sort_values([\"Date\", \"Symbol\"]).reset_index(drop=True)\n",
    "\n",
    "        # Create a dataframe 'divRank' containing only relevant columns like dividend, PAT, and Rollingdividend\n",
    "        divRank = div_qtr.filter(items=[\"Date\", \"Symbol\", \"dividend\", \"PAT\", 'Dividend payout ratio', 'Rollingdividend'])\n",
    "\n",
    "        # Map the 'divRank' to the 'composition' dataframe on 'Date' and 'Symbol' to align dividends with composition details\n",
    "        divRank = pd.merge(composition, divRank, on=['Date', 'Symbol'], how='left')\n",
    "\n",
    "        # Sort the 'divRank' dataframe by 'Symbol' and 'Date' for proper chronological order\n",
    "        divRank = divRank.sort_values([\"Symbol\", \"Date\"]).reset_index(drop=True)\n",
    "\n",
    "        # Forward fill missing values in 'dividend', 'PAT', 'Dividend payout ratio', and 'Rollingdividend' within each 'Symbol'\n",
    "        divRank[['dividend', 'PAT', 'Dividend payout ratio', 'Rollingdividend']] = divRank.groupby('Symbol')[['dividend', 'PAT', 'Dividend payout ratio', 'Rollingdividend']].ffill()\n",
    "\n",
    "        # Calculate the dividend yield for each company as (Rollingdividend / Market Capitalization) * 100\n",
    "        divRank['yield'] = (divRank['Rollingdividend'] / divRank['Mcap']) * 100\n",
    "\n",
    "        # Calculate the 4-quarter rolling average of the dividend yield for each company\n",
    "        divRank[\"yield\"] = divRank.groupby(\"Symbol\")[\"yield\"].transform(lambda x: x.rolling(4).mean())\n",
    "\n",
    "        # Final dataframe 'divRankFinal' contains 'Date', 'Symbol', and 'Dividend' columns, with no missing values\n",
    "        divRankFinal = divRank[['Date', 'Symbol', 'yield']].dropna()\n",
    "\n",
    "        # Rename the 'yield' column to 'Dividend' for better clarity\n",
    "        divRankFinal.rename(columns={'yield': 'Dividend'}, inplace=True)\n",
    "\n",
    "        if valueYieldCall:\n",
    "            return divRankFinal\n",
    "\n",
    "        ## Convert the long from Dataframe to wide form dataframe\n",
    "        divRankFinal = divRankFinal.pivot_table(index='Date',columns='Symbol',values='Dividend').reset_index()\n",
    "\n",
    "        ## Shifting the Date column\n",
    "        divRankFinal[\"Date\"] = divRankFinal[\"Date\"].shift(-1)\n",
    "        \n",
    "        ## Fill NaN value of date with todays date.\n",
    "        divRankFinal.iloc[-1, divRankFinal.columns.get_loc(\"Date\")] = pd.to_datetime(date.today())\n",
    "        \n",
    "        ## Re-converting the wide form dataframe to long form dataframe\n",
    "        divRankFinal = divRankFinal.melt(id_vars='Date', value_name= \"Dividend\")\n",
    "\n",
    "        ## Drop na and reset the index\n",
    "        divRankFinal = divRankFinal.dropna().reset_index(drop = True)\n",
    "        divRankFinal = divRankFinal[divRankFinal[\"Date\"] >= \"2006-01-01\"].reset_index(drop = True).copy()\n",
    "        divRankFinal['Dividend']=divRankFinal.groupby('Date')['Dividend'].rank(pct=True)\n",
    "\n",
    "        ## Store the Factor Data in dictionary.\n",
    "        self.factorData[\"Dividend\"] = divRankFinal.copy()\n",
    "        \n",
    "        ## Filter the data for latest update or values.\n",
    "        self.recentFactorUpdate[\"Dividend\"] = divRankFinal[divRankFinal[\"Date\"] == divRankFinal[\"Date\"].max()].reset_index(drop = True)\n",
    "        \n",
    "        ## Combining the Factors in one Dataframe\n",
    "        self.stockFactors = pd.concat([self.recentFactorUpdate[key].set_index([\"Date\", \"Symbol\"]) for key in self.recentFactorUpdate.keys()], axis = 1)\n",
    "        self.stockFactors.reset_index(inplace = True)\n",
    "\n",
    "        print(\"### DIVIDEND COMPLETE ###\")\n",
    "\n",
    "        del div_qtr, divRank, divRankFinal\n",
    "\n",
    "    def generate_Growth(self,valueYieldCall = False):\n",
    "\n",
    "        ## FUnction to get the previous Quarter Date\n",
    "        def previousQuarter(currDate):\n",
    "            if pd.Period(currDate,freq = \"Q\").end_time.date() == currDate.date():\n",
    "                return currDate\n",
    "            else:\n",
    "                return currDate - pd.offsets.QuarterEnd()\n",
    "\n",
    "        #########################\n",
    "        ## COMPANY MASTER DATA ##\n",
    "        #########################\n",
    "        if self.companyMaster is None:\n",
    "            self.__readCompanyMaster()\n",
    "\n",
    "        if self.profitLossGrowth is None:\n",
    "            self.__readProfitLossGrowth()\n",
    "\n",
    "        if self.cashFlow is None:\n",
    "            self.__readCashFlow()\n",
    "\n",
    "        if self.stockPriceData is None:\n",
    "            self.__readPriceData()\n",
    "\n",
    "        if self.sectorData is None:\n",
    "            self.__readSectorData()\n",
    "\n",
    "\n",
    "        ######################\n",
    "        ## DATA PREPARATION ##\n",
    "        ######################\n",
    "        ## List of the Rebalance Dates\n",
    "        # rebalanceDates = self.__dataCursor.fetch_data_from_database(table_name = \"GrowthDate\", no_of_years=100)\n",
    "        rebalanceDates = pd.read_csv('growth_date_latest.csv', parse_dates=['Date'])\n",
    "        \n",
    "        # Identify Companies which have been in Top-500 historically\n",
    "        composition  = self.strategyUniverse.copy()\n",
    "        \n",
    "        ## List of Quarter and Dates\n",
    "        rebalanceQtr=list(rebalanceDates['Quarter'])\n",
    "        rebalanceDate=list(rebalanceDates['Date'])\n",
    "        \n",
    "        ## Mappinf Dictionary of Date to Quarter and vice versa\n",
    "        qtrDateDict=dict(zip(rebalanceDate,rebalanceQtr))\n",
    "        dateQtrDict=dict(zip(rebalanceQtr,rebalanceDate))\n",
    "\n",
    "        ## Quarter column in PriceData\n",
    "        composition['Quarter']=composition['Date'].map(qtrDateDict)\n",
    "\n",
    "        ############################\n",
    "        ## OPERATION ON CASH DATA ##\n",
    "        ############################\n",
    "        \n",
    "        ## Import the Cash data\n",
    "        cf_yearly = self.cashFlow.filter(items = [\"FINCODE\", \"Year_end\", \"Cash_from_Operation\"])\n",
    "        ## Map the Company Names\n",
    "        cf_yearly = pd.merge(cf_yearly, self.companyMaster, how = \"inner\", on = \"FINCODE\")\n",
    "        ## Convert the string into Datetime\n",
    "        cf_yearly[\"Year_end\"] = pd.to_datetime(cf_yearly['Year_end'],format=\"%Y%m\")\n",
    "        cf_yearly[\"Year\"] = cf_yearly[\"Year_end\"].dt.year\n",
    "        ## Shift the Date Year\n",
    "        cf_yearly['Year'] = np.where(cf_yearly['Year_end'].dt.month == 12, \n",
    "                                    (cf_yearly['Year'] + 1), \n",
    "                                    (cf_yearly['Year']))\n",
    "        cf_yearly[\"Year\"] = cf_yearly[\"Year\"] + 1\n",
    "        ## Sort the dataframe\n",
    "        cf_yearly.sort_values('Year',inplace=True)\n",
    "        cf_yearly.reset_index(drop = True, inplace = True)\n",
    "\n",
    "        self.cf_yearly = cf_yearly.copy()\n",
    "\n",
    "        ###################################\n",
    "        ## OPERATION ON PROFIT/LOSS DATA ##\n",
    "        ###################################\n",
    "        ## Read the Fundamental data of the comapnies - Growth Related\n",
    "        pl_yearly = self.profitLossGrowth.drop(columns = [ 'Debt/Equity Ratio', 'Adj_eps_abs', 'Dividend payout ratio']).copy()\n",
    "        ## Convert the string into Datetime\n",
    "        pl_yearly[\"Date_End\"] = pd.to_datetime(pl_yearly['Date_End'],format=\"%Y%m\")\n",
    "\n",
    "        ## Merge the Fundamental data with master list of the companies \n",
    "        pl_yearly = pd.merge(pl_yearly, self.companyMaster, on = \"FINCODE\", how = \"inner\")\n",
    "\n",
    "        ## Calculate the Operating_Margin and Gross_Margin\n",
    "        pl_yearly[\"Operating_Margin\"] = pl_yearly[\"OPERATING_PROFIT\"].div(pl_yearly[\"NET_SALES\"])\n",
    "        pl_yearly[\"Gross_Margin\"] = pl_yearly[\"GROSS_PROFIT\"].div(pl_yearly[\"NET_SALES\"])\n",
    "\n",
    "        # ## Shift Date to respective Month End date.\n",
    "        pl_yearly[\"MonthEnd\"] = pl_yearly[\"Date_End\"] + pd.offsets.MonthEnd()\n",
    "        pl_yearly[\"QuarterEnd\"] = pl_yearly[\"MonthEnd\"].apply(previousQuarter) \n",
    "\n",
    "        ## Convert the Dates as per Indian Financial Quarter\n",
    "        pl_yearly[\"YearQuarter\"] = pl_yearly[\"QuarterEnd\"].dt.to_period(\"Q-MAR\")\n",
    "        pl_yearly[\"Quarter\"] = pl_yearly[\"YearQuarter\"].astype(str).str[-2:] + pl_yearly[\"YearQuarter\"].astype(str).str[:4]\n",
    "        pl_yearly[\"Year\"] =pl_yearly[\"YearQuarter\"].astype(str).str[:4].astype(int)\n",
    "\n",
    "        ## sort the Data Frame based on Quarter\n",
    "        pl_yearly.sort_values(\"YearQuarter\", ascending = True, inplace = True)\n",
    "        pl_yearly.reset_index(drop = True, inplace = True)\n",
    "\n",
    "        self.pl_yearly = pl_yearly.copy()\n",
    "\n",
    "        ###############\n",
    "        ## COMBINING ##\n",
    "        ###############\n",
    "        ## Combine the Profit loss and Cash data\n",
    "        data = pd.merge(pl_yearly, cf_yearly, on = ['FINCODE',\"Symbol\", \"Year\"])\n",
    "\n",
    "        ## List of Factors\n",
    "        factorUniverse = ['PAT','OPERATING_PROFIT','GROSS_PROFIT','NET_SALES','EPS_DILUTED','PBT','Operating_Margin','Gross_Margin','Cash_from_Operation']\n",
    "        ## SUbste the columns\n",
    "        data = data[[\"Year\", \"YearQuarter\", \"Quarter\", \"Symbol\", \"Date_End\"] + factorUniverse]\n",
    "\n",
    "        ## Sort the DataFrame\n",
    "        data.sort_values([\"Symbol\", \"YearQuarter\"], inplace = True)\n",
    "        data.reset_index(drop = True, inplace = True)\n",
    "\n",
    "        ## Calculating the Change in factors over a year\n",
    "        year_chg = lambda ser: (ser - ser.shift(4))/ser.abs().shift(4)\n",
    "        data[[f\"{c}_change\" for c in factorUniverse]] = data.groupby(\"Symbol\", group_keys=False)[factorUniverse].apply(year_chg)\n",
    "    \n",
    "        ## Mapping the Date column to Quarter Date\n",
    "        data['Date']=data['Quarter'].map(dateQtrDict)\n",
    "        \n",
    "        data=pd.merge(data,composition,on=['Symbol','Date','Quarter'])\n",
    "        data['Year']=data['Date'].dt.year\n",
    "\n",
    "        ## Compute the Absolute / Peer and Historical Rank\n",
    "        data = data.sort_values([\"Date\", \"Symbol\"]).reset_index(drop = True)\n",
    "\n",
    "        if valueYieldCall:\n",
    "            return data\n",
    "\n",
    "        data.drop(columns = [\"Date_End\"], inplace = True)\n",
    "\n",
    "        factorUniverse = [f\"{col}_change\" for col in factorUniverse]\n",
    "        data[\"AbsRank\"] = data.groupby(\"Date\")[factorUniverse].rank(pct = True).mean(axis = 1)\n",
    "        data[\"PeerRank\"] = data.groupby([\"Date\", \"Sector\"])[factorUniverse].rank(pct = True).mean(axis = 1)\n",
    "\n",
    "        data = data.sort_values([\"Symbol\", \"Date\"]).reset_index(drop = True)\n",
    "        data[\"HistRank\"] = data.groupby(\"Symbol\")[factorUniverse].rolling(window = 12, min_periods = 4).rank(pct = True).mean(axis = 1).reset_index(drop = True)\n",
    "        growthRank = data.filter(items = [\"Date\", \"Symbol\", \"AbsRank\", \"PeerRank\", \"HistRank\"])\n",
    "        growthRank[\"Rank\"] = growthRank[[ \"AbsRank\", \"PeerRank\", \"HistRank\"]].mean(axis = 1)\n",
    "        \n",
    "        ## Mapping the Growth score to each date\n",
    "        growthRank = pd.merge(composition,growthRank,on=['Date','Symbol'],how='left')\n",
    "        growthRank = growthRank.sort_values([\"Symbol\",  \"Date\"]).reset_index(drop = True)\n",
    "\n",
    "        ## Forward fill the growth score, as growth is for each quarter\n",
    "        growthRank[['AbsRank','PeerRank','HistRank','FinalRank']]=growthRank.groupby('Symbol')[['AbsRank','PeerRank','HistRank','Rank']].ffill()\n",
    "        growthRank['Growth'] = growthRank['PeerRank']*0.95 + growthRank['AbsRank']*0.05\n",
    "\n",
    "        ## Convert the long from Dataframe to wide form dataframe\n",
    "        growthRank = growthRank.pivot_table(index='Date',columns='Symbol',values='Growth').reset_index()\n",
    "        ## Shifting the Date column\n",
    "        growthRank[\"Date\"] = growthRank[\"Date\"].shift(-1)\n",
    "        ## Fill NaN value of date with todays date.\n",
    "        growthRank.iloc[-1, growthRank.columns.get_loc(\"Date\")] = pd.to_datetime(date.today())\n",
    "        ## Re-converting the wide form dataframe to long form dataframe\n",
    "        growthRank = growthRank.melt(id_vars='Date', value_name= \"Growth\")\n",
    "        ## Drop na and reset the index\n",
    "        growthRank = growthRank.dropna().reset_index(drop = True)\n",
    "        growthRank = growthRank[growthRank[\"Date\"] >= \"2006-01-01\"].reset_index(drop = True).copy()\n",
    "\n",
    "        ## Store the Factor Data in dictionary.\n",
    "        self.factorData[\"Growth\"] = growthRank.copy()\n",
    "        ## Filter the data for latest update or values.\n",
    "        self.recentFactorUpdate[\"Growth\"] = growthRank[growthRank[\"Date\"] == growthRank[\"Date\"].max()].reset_index(drop = True)\n",
    "        ## Combining the Factors in one Dataframe\n",
    "        self.stockFactors = pd.concat([self.recentFactorUpdate[key].set_index([\"Date\", \"Symbol\"]) for key in self.recentFactorUpdate.keys()], axis = 1)\n",
    "        self.stockFactors.reset_index(inplace = True)\n",
    "\n",
    "        print(\"### GROWTH COMPLETE ###\")\n",
    "\n",
    "    def generate_ValueYield(self):\n",
    "\n",
    "        ## Fetch the Stock Value Data\n",
    "        if self.stockValueData is None:\n",
    "            self.__readValueData()\n",
    "\n",
    "        ## Fetch the \n",
    "        if self.stockPriceData is None:\n",
    "            self.__readPriceData()\n",
    "\n",
    "        if self.sectorData is None:\n",
    "            self.__readSectorData()\n",
    "\n",
    "        \n",
    "        ### VALUE ###\n",
    "        # Filter the rows for Top-500 companies historically\n",
    "        valueData = self.stockValueData[self.stockValueData[\"Symbol\"].isin(self.strategyUniverse[\"Symbol\"].unique())].copy()\n",
    "\n",
    "        ## List of Fundamental Factor Value.\n",
    "        fundamentalValueFactor = [\"EV_EBITDA\", \"PS\", \"PB\", \"PE\"]\n",
    "        ## Replace -ve value with NaN value for all the fundamental value factor\n",
    "        for factor in fundamentalValueFactor:\n",
    "            valueData[factor] = np.where(valueData[factor] <= 0, np.nan, valueData[factor])\n",
    "            valueData[factor] = 1/valueData[factor]\n",
    "\n",
    "        ## Sort and reset the index\n",
    "        valueData.sort_values([\"Symbol\", \"Date\"], inplace = True)\n",
    "        valueData.reset_index(drop = True, inplace = True)\n",
    "\n",
    "\n",
    "        ### Dividend ###\n",
    "        dividendData = self.generate_Dividend(valueYieldCall = True)\n",
    "\n",
    "        ## EPS ###\n",
    "        epsData = self.generate_Growth(valueYieldCall = True)\n",
    "\n",
    "        ## ROE\n",
    "        roe = self.generate_QualityQuarter(valueYieldCall=True)\n",
    "        roe[\"Date_End\"] = pd.to_datetime(roe['Date_End'],format=\"%Y%m\")\n",
    "\n",
    "        growth = pd.merge(epsData[[\"Date\", \"Date_End\", \"Symbol\",\"EPS_DILUTED_change\"]], \n",
    "                          roe[[\"Date_End\", \"Symbol\", \"ROE_ttm\"]], on = [\"Date_End\", \"Symbol\"], how = \"left\")\n",
    "\n",
    "        final=pd.merge(valueData,dividendData,on=['Date','Symbol'])\n",
    "        final=pd.merge(final,growth,on=['Date','Symbol'],how='left')\n",
    "\n",
    "        final = final.sort_values([\"Symbol\",\"Date\"]).reset_index(drop = True)\n",
    "        final[['ROE_ttm', \"EPS_DILUTED_change\"]]=final.groupby('Symbol')[['ROE_ttm', \"EPS_DILUTED_change\"]].ffill()\n",
    "        final['PEG']=final['PE']/final['EPS_DILUTED_change']\n",
    "\n",
    "        factorUniverse = ['EV_EBITDA', 'PS','PB', 'PE', 'Dividend','PEG','ROE_ttm']\n",
    "\n",
    "        final = pd.merge(self.strategyUniverse, final[[\"Date\", \"Symbol\"] + factorUniverse], on = [\"Date\",\"Symbol\"], how = \"left\")\n",
    "\n",
    "        final[\"AbsRank\"] = final.groupby(\"Date\")[factorUniverse].rank(pct = True).mean(axis = 1)\n",
    "        final[\"PeerRank\"] = final.groupby([\"Date\", \"Sector\"])[factorUniverse].rank(pct = True).mean(axis = 1)\n",
    "\n",
    "        final['ValueYield'] = final['PeerRank']*0.95 + final['AbsRank']*0.05\n",
    "        final['ValueABS'] = final['PeerRank']*0.05 + final['AbsRank']*0.95        \n",
    "\n",
    "        result = list()\n",
    "\n",
    "        # Iterate over each volatility-related column\n",
    "        for col in [\"ValueYield\", \"ValueABS\"]:\n",
    "\n",
    "            # Filter the dataframe to keep only the \"Symbol\", \"Date\", and current column\n",
    "            temp = final.filter([\"Symbol\", \"Date\", col])\n",
    "\n",
    "            # Pivot the table to have dates as rows and symbols as columns, with values from the current column\n",
    "            temp = temp.pivot_table(index='Date', columns='Symbol', values=col).reset_index()\n",
    "\n",
    "            # Shift the \"Date\" column up by one row to align data with the next date\n",
    "            temp[\"Date\"] = temp[\"Date\"].shift(-1)\n",
    "\n",
    "            # Set the last row's \"Date\" to today's date to capture current data\n",
    "            temp.iloc[-1, temp.columns.get_loc(\"Date\")] = pd.to_datetime(date.today())\n",
    "\n",
    "            # Unpivot the table to return it to long format with \"Date\", \"Symbol\", and current column values\n",
    "            temp = temp.melt(id_vars='Date', value_name=col)\n",
    "\n",
    "            # Drop rows with missing values and reset the index\n",
    "            temp = temp.dropna().reset_index(drop=True)\n",
    "\n",
    "            # Filter data to include only records from January 1, 2006, onwards and reset the index\n",
    "            temp = temp[temp[\"Date\"] >= \"2006-01-01\"].reset_index(drop=True).copy()\n",
    "\n",
    "            # Set the index to \"Date\" and \"Symbol\" for each transformed column and add to the result list\n",
    "            result.append(temp.set_index([\"Date\", \"Symbol\"]))\n",
    "\n",
    "\n",
    "        # Concatenate all transformed columns along the columns axis to create a combined dataframe\n",
    "        scores = pd.concat(result, axis=1).reset_index()\n",
    "\n",
    "        ## Store the Factor Data in dictionary.\n",
    "        self.factorData[\"ValueYield\"] = scores.copy()\n",
    "        ## Filter the data for latest update or values.\n",
    "        self.recentFactorUpdate[\"ValueYield\"] = scores[scores[\"Date\"] == scores[\"Date\"].max()].reset_index(drop = True)\n",
    "        ## Combining the Factors in one Dataframe\n",
    "        self.stockFactors = pd.concat([self.recentFactorUpdate[key].set_index([\"Date\", \"Symbol\"]) for key in self.recentFactorUpdate.keys()], axis = 1)\n",
    "        self.stockFactors.reset_index(inplace = True)\n",
    "\n",
    "        print(\"### ValueYield COMPLETE ###\")\n",
    "\n",
    "    def generate_QualityAnnual(self,):\n",
    "\n",
    "        ## Function to calculate the volatility of the ratio\n",
    "        def calculateVol(series):\n",
    "            _max = series.rolling(window = 5, min_periods = 3).max()\n",
    "            _min = series.rolling(window = 5, min_periods = 3).min()\n",
    "            _mean = series.rolling(window = 5, min_periods = 3).mean() \n",
    "            return (_max - _min)/(_mean)\n",
    "\n",
    "        #########################\n",
    "        ## COMPANY MASTER DATA ##\n",
    "        #########################\n",
    "        if self.companyMaster is None:\n",
    "            self.__readCompanyMaster()\n",
    "\n",
    "        if self.profitLossQuality is None:\n",
    "            self.__readProfitLossQuality()\n",
    "\n",
    "        if self.financialRatio is None:\n",
    "            self.__readFinancialRatio()\n",
    "\n",
    "        if self.cashFlow is None:\n",
    "            self.__readCashFlow()\n",
    "\n",
    "        if self.stockPriceData is None:\n",
    "            self.__readPriceData()\n",
    "\n",
    "        if self.sectorData is None:\n",
    "            self.__readSectorData()\n",
    "\n",
    "        ## Read the Rebalance Dates\n",
    "        # rebalanceDates = self.__dataCursor.fetch_data_from_database(table_name = \"QualityDate\", no_of_years = 100)\n",
    "        rebalanceDates = pd.read_csv('QualityDate.csv')\n",
    "        rebalanceDates[\"Date\"] = pd.to_datetime(rebalanceDates[\"Date\"])\n",
    "        rebalances = rebalanceDates[\"Date\"].tolist()\n",
    "\n",
    "        ## Mapping the Data Receive Year\n",
    "        priceData = pd.merge(self.strategyUniverse, rebalanceDates, on = \"Date\", how = \"left\")\n",
    "        priceData = priceData.sort_values([\"Symbol\", \"Date\"]).reset_index(drop = True)\n",
    "        \n",
    "        ## Filter the dataframe for the rebalance dates.\n",
    "        priceDataTop500Rebal = priceData[priceData[\"Date\"].isin(rebalances)]\n",
    "\n",
    "        ############################\n",
    "        ### PROFIT and LOSS DATA ###\n",
    "        ############################\n",
    "        ## Mapping Company Symbol to Cash Flow Data\n",
    "        profitLoss = pd.merge(self.profitLossQuality, self.companyMaster, on = [\"FINCODE\"], how = \"inner\")\n",
    "\n",
    "        ## Calculating the Operating Margin and Gross Margin\n",
    "        profitLoss[\"OperatingMargin\"] = profitLoss[\"Operating_profit\"].div(profitLoss[\"Net_sales\"])\n",
    "        profitLoss[\"GrossMargin\"] = profitLoss[\"Gross_profits\"].div(profitLoss[\"Net_sales\"])\n",
    "\n",
    "        ## Convert the Date String to python datetime\n",
    "        profitLoss[\"Year_end\"] = pd.to_datetime(profitLoss[\"Year_end\"], format = \"%Y%m\")\n",
    "        profitLoss[\"Year\"]  = profitLoss[\"Year_end\"].dt.year\n",
    "        profitLoss[\"Year\"] = np.where(profitLoss[\"Year_end\"].dt.month == 12, profitLoss[\"Year\"]+1, profitLoss[\"Year\"])\n",
    "\n",
    "        ## Drop the duplicates and Keep the last record\n",
    "        profitLoss = profitLoss.sort_values([\"Year_end\", \"Symbol\"])#.drop_duplicates([\"Year\", \"Symbol\"], keep = \"last\")\n",
    "\n",
    "        ## Sort the dataframe by Year column and reset the dataframe\n",
    "        profitLoss.sort_values(\"Year\", inplace = True)\n",
    "        profitLoss.reset_index(drop = True, inplace = True)\n",
    "        \n",
    "        ############################\n",
    "        ### FINANCIAL RATIO DATA ###\n",
    "        ############################\n",
    "        ## Replace NaN value with 0\n",
    "        financialRatio = self.financialRatio.copy()\n",
    "        financialRatio[\"Payable_days\"] = financialRatio[\"Payable_days\"].fillna(0)\n",
    "        ## Calculating the Working Capital Days\n",
    "        financialRatio[\"WC_Days\"] = financialRatio[\"Inventory_Days\"] + financialRatio[\"Receivable_days\"] - financialRatio[\"Payable_days\"]\n",
    "        ## Mapping COmpany Symbol to Financial Ratio data\n",
    "        financialRatio = pd.merge(financialRatio, self.companyMaster, on = [\"FINCODE\"], how = \"inner\")\n",
    "\n",
    "        ## Convert the Date string to python datetime\n",
    "        financialRatio[\"Year_end\"] = pd.to_datetime(financialRatio.Year_end, format = \"%Y%m\")\n",
    "        financialRatio[\"Year\"] = financialRatio[\"Year_end\"].dt.year\n",
    "        financialRatio[\"Year\"] = np.where(financialRatio[\"Year_end\"].dt.month == 12, financialRatio[\"Year\"] + 1, financialRatio[\"Year\"])\n",
    "\n",
    "        ## Drop the duplicates and Keep the last record\n",
    "        financialRatio = financialRatio.sort_values([\"Year_end\", \"Symbol\"])#.drop_duplicates([\"Year\", \"Symbol\"], keep = \"last\")\n",
    "\n",
    "        ## Sort the dataframe by Year column and reset the dataframe\n",
    "        financialRatio.sort_values(\"Year\", inplace = True)\n",
    "        financialRatio.reset_index(drop = True, inplace = True)\n",
    "\n",
    "        ######################\n",
    "        ### CASH FLOW DATA ###\n",
    "        ######################\n",
    "        ## Mapping Company Symbol to Cash Flow Data\n",
    "        cashFlow = self.cashFlow[[\"FINCODE\", \"Year_end\",\"Cash_from_Operation\"]].copy()\n",
    "        cashFlow = pd.merge(cashFlow, self.companyMaster, on = [\"FINCODE\"], how = \"inner\")\n",
    "\n",
    "        ## Convert the Date String to python datetime\n",
    "        cashFlow[\"Year_end\"] = pd.to_datetime(cashFlow[\"Year_end\"], format = \"%Y%m\")\n",
    "        cashFlow[\"Year\"]  = cashFlow[\"Year_end\"].dt.year\n",
    "        cashFlow[\"Year\"] = np.where(cashFlow[\"Year_end\"].dt.month == 12, cashFlow[\"Year\"]+1, cashFlow[\"Year\"])\n",
    "\n",
    "        ## Drop the duplicates and Keep the last record\n",
    "        cashFlow = cashFlow.sort_values([\"Year_end\", \"Symbol\"])#.drop_duplicates([\"Year\", \"Symbol\"], keep = \"last\")\n",
    "        ## Sort the dataframe by Year column and reset the dataframe\n",
    "        cashFlow.sort_values(\"Year\", inplace = True)\n",
    "        cashFlow.reset_index(drop = True, inplace = True)\n",
    "\n",
    "        ## Merge the dataframes of Profit and Loss, Cash Flow and Financial Ratios into one dataframe\n",
    "        data = ft.reduce(lambda left, right: pd.merge(left, right, on=['Symbol','Year','FINCODE']), [profitLoss, financialRatio, cashFlow])\n",
    "        ## Calculating Cash Flow Operation (CFO) BY EBITDA  and Free cash Flow (FCF) COnversion ratio.\n",
    "        data[\"CFO_By_EBITDA\"] = data[\"Cash_from_Operation\"].div(data[\"Operating_profit\"])\n",
    "        data[\"FCF_Conversion\"] = data[\"FCF_Share\"].div(data[\"Adj_Eps\"])\n",
    "\n",
    "        ## Sort the dataframe and reset the index\n",
    "        data.sort_values([\"Symbol\", \"Year\"], inplace = True)\n",
    "        data.reset_index(drop = True, inplace = True)\n",
    "\n",
    "        ## Calculating the Volatility of Selected Financial ratio (\"ROE\", \"Adj_EPS\", \"CEPS\", \"OperatingMargin\")\n",
    "        tempFactors = ['ROE', 'Adj_Eps', 'CEPS', 'OperatingMargin']\n",
    "        data[[f\"Vol_{value}\" for value in tempFactors]] = data.groupby(\"Symbol\", group_keys = False)[tempFactors].apply(calculateVol)\n",
    "\n",
    "        ## Calculating the Percentage of Selected Financila Ratio (\"Net_Sales\", \"Oprating_profit\", \"Adj_Eps\", \"Cash_from_operation\", \"FCF_Share\")\n",
    "        tempFactors = [\"Net_sales\", \"Operating_profit\", \"Adj_Eps\", \"Cash_from_Operation\", \"FCF_Share\"]\n",
    "        data[[f\"{value}_Growth\" for value in tempFactors]] = data.groupby(\"Symbol\", group_keys = False)[tempFactors].apply(lambda ser: ser.pct_change())\n",
    "\n",
    "        ## Consolidated List of individual factors to create the Consolidated Quality Factor\n",
    "        factorUniverse = [\"FCF_Share\", \"OperatingMargin\", \"GrossMargin\", \"CFO_By_EBITDA\", \"ROE\", \"ROCE\",\n",
    "                        \"Total_Debt_Equity\", \"Interest_Cover\", \"ROA\", \"Inventory_Days\", \"Receivable_days\",\n",
    "                        \"WC_Days\", \"FCF_Conversion\",\"Vol_ROE\", \"Vol_Adj_Eps\", \"Vol_CEPS\", \"Vol_OperatingMargin\",\n",
    "                        \"Dividend_Payout_Per\"]\n",
    "\n",
    "        # \"Growth\"\n",
    "        ## Indivodual facrtor in universe are divided in two list, one to be ranked in ascending order and other list to be ranked in descending order.\n",
    "        ascendingFactorUniverse = [\"CFO_By_EBITDA\", \"Dividend_Payout_Per\", \n",
    "                                \"FCF_Conversion\",  \"FCF_Share\", \"GrossMargin\", \"Interest_Cover\", \"OperatingMargin\", \"ROA\",\n",
    "                                \"ROCE\", \"ROE\"]\n",
    "\n",
    "        descendingFactorUniverse = [\"Inventory_Days\", \"Receivable_days\", \"WC_Days\", \"Total_Debt_Equity\", \n",
    "                                    \"Vol_ROE\", \"Vol_Adj_Eps\", \"Vol_CEPS\", \"Vol_OperatingMargin\"]\n",
    "        \n",
    "        ## Filter the Necessary columns\n",
    "        data = data.filter(items = [\"Symbol\", 'Year'] + factorUniverse)\n",
    "        \n",
    "        ## Mapping the Factor to Stock price Data.\n",
    "        rank = pd.merge(priceDataTop500Rebal, data, on = [\"Symbol\", \"Year\"])\n",
    "\n",
    "        ## Sort the Dataframe and reset the index\n",
    "        rank.sort_values(\"Date\", inplace = True)\n",
    "        rank.reset_index(drop = True, inplace = True)\n",
    "\n",
    "        ## Individual Absolute rank for Ascending and Descending Factor\n",
    "        absAscRank = rank.groupby(\"Year\")[ascendingFactorUniverse].rank(ascending = True, pct = True)\n",
    "        absDescRank = rank.groupby(\"Year\")[descendingFactorUniverse].rank(ascending = False, pct = True)\n",
    "        ## Take the mean of individual factor to get Aboslute rank for each stock\n",
    "        rank[\"AbsoluteRank\"] = pd.concat([absAscRank, absDescRank],axis = 1).mean(axis = 1)\n",
    "\n",
    "        ## Individual Peer rank for Ascending and Descending Factor\n",
    "        peerAscRank = rank.groupby([\"Year\", \"Sector\"])[ascendingFactorUniverse].rank(ascending = True, pct = True)\n",
    "        peerDescRank = rank.groupby([\"Year\", \"Sector\"])[descendingFactorUniverse].rank(ascending = False, pct = True)\n",
    "        ## Take the mean of individual factor to get Peer rank for each stock\n",
    "        rank[\"PeerRank\"] = pd.concat([peerAscRank, peerDescRank],axis = 1).mean(axis = 1)\n",
    "\n",
    "        ## Mapping the Rank against The dates of each stock.\n",
    "        quality = pd.merge(priceData, rank[[\"Date\", \"Symbol\", \"AbsoluteRank\", \"PeerRank\"]], on = [\"Date\", \"Symbol\"], how = \"left\")\n",
    "\n",
    "         ## Sort the dataframe and reset the index\n",
    "        quality.sort_values([\"Symbol\", \"Date\"], inplace = True)\n",
    "        quality.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "        ## Forward fill the Rank\n",
    "        quality[[\"AbsoluteRank\", \"PeerRank\"]] = quality.groupby(\"Symbol\", group_keys = False)[[\"AbsoluteRank\", \"PeerRank\"]].ffill()\n",
    "    \n",
    "        ## Selecting the relevant columns\n",
    "        quality = quality.filter([\"Date\", \"Symbol\", \"AbsoluteRank\", \"PeerRank\"])\n",
    "\n",
    "        ## Calculate the Weighted Quality Factor Score\n",
    "        quality[\"QualityFactor\"] = quality[\"AbsoluteRank\"] * 0.05 + quality[\"PeerRank\"] * 0.95\n",
    "        ## Selecting the relevant columns\n",
    "        quality = quality.filter([\"Date\", \"Symbol\", \"QualityFactor\"])\n",
    "\n",
    "        ## Convert the long from Dataframe to wide form dataframe\n",
    "        quality = quality.pivot_table(index='Date',columns='Symbol',values='QualityFactor').reset_index()\n",
    "        ## Shifting the Date column\n",
    "        quality[\"Date\"] = quality[\"Date\"].shift(-1)\n",
    "        ## Fill NaN value of date with todays date.\n",
    "        quality.iloc[-1, quality.columns.get_loc(\"Date\")] = pd.to_datetime(date.today())\n",
    "        ## Re-converting the wide form dataframe to long form dataframe\n",
    "        quality = quality.melt(id_vars='Date', value_name= \"QualityAnnual\")\n",
    "        ## Drop na and reset the index\n",
    "        quality = quality.dropna().reset_index(drop = True)\n",
    "        quality = quality[quality[\"Date\"] >= \"2006-01-01\"].reset_index(drop = True).copy()\n",
    "\n",
    "        ## Store the Factor Data in dictionary.\n",
    "        self.factorData[\"QualityAnnual\"] = quality.copy()\n",
    "        ## Filter the data for latest update or values.\n",
    "        self.recentFactorUpdate[\"QualityAnnual\"] = quality[quality[\"Date\"] == quality[\"Date\"].max()].reset_index(drop = True)\n",
    "        ## Combining the Factors in one Dataframe\n",
    "        self.stockFactors = pd.concat([self.recentFactorUpdate[key].set_index([\"Date\", \"Symbol\"]) for key in self.recentFactorUpdate.keys() ], axis = 1)\n",
    "        self.stockFactors.reset_index(inplace = True)\n",
    "\n",
    "        print(\"### QUALITY ANNUAL COMPLETE ###\")\n",
    "            \n",
    "    def generate_QualityQuarter(self, valueYieldCall = False):\n",
    "\n",
    "        # Quarterly Networth Calculation\n",
    "        def fill_networth(row, prev_networth):\n",
    "            if pd.isna(row['NetWorth']):\n",
    "                return prev_networth + row['PAT']\n",
    "            else:\n",
    "                return row['NetWorth']\n",
    "\n",
    "        def fill_networth_column(df):\n",
    "            df = df.sort_values(by=['FINCODE', 'Date_End']) \n",
    "            df['NetWorth_Filled'] = np.nan\n",
    "            \n",
    "            for fincode in df['FINCODE'].unique():\n",
    "                prev_networth = None\n",
    "                for i, row in df[df['FINCODE'] == fincode].iterrows():\n",
    "                    if prev_networth is None:\n",
    "                        prev_networth = row['NetWorth']\n",
    "                    else:\n",
    "                        df.at[i, 'NetWorth_Filled'] = fill_networth(row, prev_networth)\n",
    "                        prev_networth = df.at[i, 'NetWorth_Filled']\n",
    "            \n",
    "            return df\n",
    "        \n",
    "        #  EPS Growth Calculation\n",
    "        def calculate_yoy_eps_growth(eps):\n",
    "            \"\"\"Calculates YoY EPS Growth based on the provided rules.\"\"\"\n",
    "            growth = []\n",
    "            for i in range(len(eps)):\n",
    "                if i < 4:\n",
    "                    growth.append(np.nan) \n",
    "                else:\n",
    "                    prev_eps = eps.iloc[i - 4]  \n",
    "                    curr_eps = eps.iloc[i]      \n",
    "                    if prev_eps > 0:\n",
    "                        growth.append((curr_eps - prev_eps) / prev_eps)\n",
    "                    elif prev_eps < 0:\n",
    "                        growth.append(-(curr_eps - prev_eps) / prev_eps)\n",
    "                    else:\n",
    "                        growth.append(np.nan)  \n",
    "            return pd.Series(growth, index=eps.index)  \n",
    "\n",
    "        # Calculate 5-year mean and std deviation for each quarter separately\n",
    "        def calc_quarterly_stats(group):\n",
    "            group = group.sort_values('Date_End')\n",
    "            group['Mean_YoY_EPS_Growth'] = group['YoY_EPS_Growth'].rolling(window=5, min_periods=1).mean()\n",
    "            group['Std_YoY_EPS_Growth'] = group['YoY_EPS_Growth'].rolling(window=5, min_periods=1).std()\n",
    "            return group\n",
    "        \n",
    "        # Weighted Average Z Quality Score\n",
    "        def calculate_weighted_avg_z(row):\n",
    "            if row['Sector'] == 'Bank':\n",
    "                return (1/2) * row['Z_ROE_ttm_fin'] - (1/2) * abs(row['Z_EPS_Growth_Variability_fin'])\n",
    "            else:\n",
    "                return (1/3) * row['Z_ROE_ttm'] - (1/3) * abs(row['Z_DE']) - (1/3) * abs(row['Z_EPS_Growth_Variability'])\n",
    "        \n",
    "        # Define a function to check for negatives in the last 16 quarters\n",
    "        def has_negative_in_last_4_years(series):\n",
    "            return series.rolling(window=16, min_periods=16).apply(lambda x: (x < 0).any(), raw=True)\n",
    "\n",
    "        #########################\n",
    "        ## COMPANY MASTER DATA ##\n",
    "        #########################\n",
    "        if self.companyMaster is None:\n",
    "            self.__readCompanyMaster()\n",
    "\n",
    "        if self.profitLossGrowth is None:\n",
    "            self.__readProfitLossGrowth()\n",
    "\n",
    "        if self.financeBalanceSheet is None:\n",
    "            self.__readFinanceBalanceSheet()\n",
    "\n",
    "        if self.stockPriceData is None:\n",
    "            self.__readPriceData()\n",
    "\n",
    "        if self.sectorData is None:\n",
    "            self.__readSectorData()\n",
    "\n",
    "\n",
    "        price_data = self.stockPriceData.copy()\n",
    "        top_500 = self.strategyUniverse.copy()\n",
    "\n",
    "        # rebalDates = self.__dataCursor.fetch_data_from_database(table_name=\"GrowthDate\", no_of_years=50)\n",
    "        rebalDates = pd.read_csv('growth_date_latest.csv', parse_dates=['Date'])\n",
    "        Rebalance_Qtr=list(rebalDates['Quarter'])\n",
    "        Rebalance_Date=list(rebalDates['Date'])\n",
    "        Qtr_date_dict=dict(zip(Rebalance_Date,Rebalance_Qtr))\n",
    "        date_Qtr_dict=dict(zip(Rebalance_Qtr,Rebalance_Date))\n",
    "\n",
    "        top_500['Quarter']=top_500['Date'].map(Qtr_date_dict)\n",
    "\n",
    "        ## Financial Balance Sheet Modfication\n",
    "        financialBalanceSheet = self.financeBalanceSheet[[\"Year_end\", \"FINCODE\", \"Share_Capital\", \"Reserve\"]]\n",
    "        financialBalanceSheet[\"NetWorth\"] = financialBalanceSheet[[\"Share_Capital\", \"Reserve\"]].sum(axis = 1)\n",
    "        financialBalanceSheet  = pd.merge(financialBalanceSheet, self.companyMaster[[\"FINCODE\", \"Symbol\"]], on = \"FINCODE\")\n",
    "        financialBalanceSheet = financialBalanceSheet[[\"Year_end\", \"FINCODE\", \"Symbol\", \"NetWorth\"]]\n",
    "\n",
    "        ## PAT Modification\n",
    "        pat = self.profitLossGrowth[[\"Date_End\", \"FINCODE\", \"PAT\"]]\n",
    "        pat[\"PAT\"] /= 10\n",
    "\n",
    "        patBS = pd.merge(pat, financialBalanceSheet, left_on = [\"Date_End\", \"FINCODE\"], \n",
    "                right_on = [\"Year_end\", \"FINCODE\"], how = \"left\")\n",
    "        patBS['Symbol'] = patBS.groupby('FINCODE', group_keys=False)['Symbol'].apply(lambda x: x.fillna(method='ffill'))\n",
    "        patBS.drop(columns = [\"Year_end\"], inplace = True)\n",
    "\n",
    "        # Apply the function\n",
    "        patBS = fill_networth_column(patBS)\n",
    "        # Fill any remaining NaN in the original NetWorth column with the filled values\n",
    "        patBS['NetWorth'] = patBS['NetWorth'].combine_first(patBS['NetWorth_Filled'])\n",
    "        patBS.drop(columns=['NetWorth_Filled'], inplace=True)  \n",
    "        patBS = patBS.dropna()\n",
    "\n",
    "        # Calculating ROE from scratch\n",
    "        patBS['ROE'] = patBS.groupby('Symbol', group_keys=False).apply(lambda x: x['PAT']/x['NetWorth'])\n",
    "        patBS['ROE_ttm'] = patBS.groupby('Symbol', group_keys=False)['ROE'].apply(lambda x: x.rolling(window=4).sum())\n",
    "\n",
    "        self.patBS = patBS.copy()\n",
    "\n",
    "        roe_ttm = patBS[['FINCODE', 'Date_End', 'ROE_ttm']].copy()\n",
    "\n",
    "        if valueYieldCall:\n",
    "            return  patBS[['FINCODE', \"Symbol\", 'Date_End', 'ROE_ttm']]\n",
    "\n",
    "        quality = self.profitLossGrowth[['FINCODE','Date_End','Debt/Equity Ratio', 'Adj_eps_abs']]\n",
    "        quality = pd.merge(quality, roe_ttm, on=['FINCODE', 'Date_End'])\n",
    "        pl_quality = pd.merge(quality, self.companyMaster, on = \"FINCODE\", how='inner')\n",
    "\n",
    "        self.pl_quality = pl_quality.copy()\n",
    "    \n",
    "        # Getting required Quality ratios\n",
    "        pl_quality = pl_quality[['FINCODE', \"Symbol\", 'Date_End', 'Debt/Equity Ratio', 'ROE_ttm', 'Adj_eps_abs']].reset_index(drop=True)\n",
    "\n",
    "        # Removing the rows with NaN Symbol Names\n",
    "        pl_quality = pl_quality[pl_quality[\"Symbol\"].notna()]\n",
    "\n",
    "        # Filtering Stocks which have been historically in top 500 universe only.\n",
    "        pl_quality = pl_quality[pl_quality[\"Symbol\"].isin(top_500.Symbol.unique())]\n",
    "\n",
    "        # Calculate Adjusted EPS TTM\n",
    "        pl_quality['Adj_eps_abs_TTM'] = pl_quality.groupby('Symbol')['Adj_eps_abs'].transform(lambda x: x.rolling(4).sum())\n",
    "\n",
    "        # Apply the function to create a flag for exclusion\n",
    "        pl_quality['exclude_flag'] = pl_quality.groupby('Symbol')['Adj_eps_abs_TTM'].transform(has_negative_in_last_4_years)\n",
    "\n",
    "        # Remove symbols with fewer than 16 quarters of data\n",
    "        # Count the number of quarters for each symbol\n",
    "        symbol_quarter_counts = pl_quality.groupby('Symbol').size()\n",
    "\n",
    "        # Create a list of symbols with at least 16 quarters\n",
    "        valid_symbols = symbol_quarter_counts[symbol_quarter_counts >= 16].index\n",
    "\n",
    "        # Filter the DataFrame to keep only rows with valid symbols\n",
    "        pl_quality = pl_quality[pl_quality['Symbol'].isin(valid_symbols)]\n",
    "\n",
    "        # Filter out rows based on the exclude flag\n",
    "        b_filtered = pl_quality[pl_quality['exclude_flag'] != 1].reset_index(drop=True)\n",
    "\n",
    "        # Drop the intermediate column used for filtering\n",
    "        b_filtered = b_filtered.drop(columns=['exclude_flag'])\n",
    "\n",
    "        # Mapping GICS to each Symbol\n",
    "        df = pd.merge(b_filtered, self.sectorData[['Symbol', 'Sector']], on='Symbol', how='left')\n",
    "\n",
    "        # Calculating ROE for TTM for the past 4 Years years\n",
    "        df['ROE_ttm'] = df.groupby('Symbol')['ROE_ttm'].transform(lambda x : x.rolling(16).mean())\n",
    "\n",
    "        # Create a new column for quarter information\n",
    "        df['Quarter'] = pd.to_datetime(df['Date_End'], format='%Y%m').dt.quarter\n",
    "\n",
    "        # Calculating YoY EPS Growth for each quarter\n",
    "        df['YoY_EPS_Growth'] = df.groupby('Symbol')['Adj_eps_abs'].transform(calculate_yoy_eps_growth)\n",
    "\n",
    "        # Apply function to each SYMBOL and Quarter group\n",
    "        df = df.groupby(['Symbol', 'Quarter'], group_keys=False).apply(calc_quarterly_stats)\n",
    "\n",
    "        # Dropping Quarter column after calculation to keep the original dataframe structure\n",
    "        df.drop(columns=['Quarter'], inplace=True)\n",
    "\n",
    "        # Calculating rolling 5-year mean and std deviation for EPS growth\n",
    "        df['Mean_YoY_EPS_Growth'] = df.groupby('Symbol')['YoY_EPS_Growth'].transform(lambda x: x.rolling(window=20).mean())\n",
    "        df['Std_YoY_EPS_Growth'] = df.groupby('Symbol')['YoY_EPS_Growth'].transform(lambda x: x.rolling(window=20).std())\n",
    "\n",
    "        # Calulating EPS Growth Variability\n",
    "        df['EPS_Growth_Variability'] = df['Mean_YoY_EPS_Growth'].div(df['Std_YoY_EPS_Growth'])\n",
    "\n",
    "        # Split data into financial and non-financial sectors\n",
    "        financials_df = df[df['Sector'] == 'Bank']\n",
    "        non_financials_df = df[df['Sector'] != 'Bank']\n",
    "\n",
    "        # Calculate mean and std for financials\n",
    "        financials_df['mean_roe_fin'] = financials_df.groupby('Date_End')['ROE_ttm'].transform('mean')\n",
    "        financials_df['std_roe_fin'] = financials_df.groupby('Date_End')['ROE_ttm'].transform('std')\n",
    "\n",
    "        financials_df['mean_eps_growth_variability_fin'] = financials_df.groupby('Date_End')['EPS_Growth_Variability'].transform('mean')\n",
    "        financials_df['std_eps_growth_variability_fin'] = financials_df.groupby('Date_End')['EPS_Growth_Variability'].transform('std')\n",
    "\n",
    "        # Calculate mean and std for non-financials\n",
    "        non_financials_df['mean_roe'] = non_financials_df.groupby('Date_End')['ROE_ttm'].transform('mean')\n",
    "        non_financials_df['std_roe'] = non_financials_df.groupby('Date_End')['ROE_ttm'].transform('std')\n",
    "\n",
    "        non_financials_df['mean_de'] = non_financials_df.groupby('Date_End')['Debt/Equity Ratio'].transform('mean')\n",
    "        non_financials_df['std_de'] = non_financials_df.groupby('Date_End')['Debt/Equity Ratio'].transform('std')\n",
    "\n",
    "        non_financials_df['mean_eps_growth_variability'] = non_financials_df.groupby('Date_End')['EPS_Growth_Variability'].transform('mean')\n",
    "        non_financials_df['std_eps_growth_variability'] = non_financials_df.groupby('Date_End')['EPS_Growth_Variability'].transform('std')\n",
    "\n",
    "        # Combine the financials and non-financials dataframes back together\n",
    "        df = pd.concat([financials_df, non_financials_df]).reset_index(drop=True)\n",
    "\n",
    "        # Z-Score Calculation\n",
    "        df['Z_ROE_ttm'] = (df['ROE_ttm'] - df['mean_roe']) / df['std_roe']\n",
    "        df['Z_ROE_ttm_fin'] = (df['ROE_ttm'] - df['mean_roe_fin']) / df['std_roe_fin']\n",
    "        df['Z_DE'] = (df['Debt/Equity Ratio'] - df['mean_de']) / df['std_de']\n",
    "        df['Z_EPS_Growth_Variability'] = (df['EPS_Growth_Variability'] - df['mean_eps_growth_variability']) / df['std_eps_growth_variability']\n",
    "        df['Z_EPS_Growth_Variability_fin'] = (df['EPS_Growth_Variability'] - df['mean_eps_growth_variability_fin']) / df['std_eps_growth_variability_fin']\n",
    "\n",
    "        df['WeightedAvgZ'] = df.apply(calculate_weighted_avg_z, axis=1)\n",
    "\n",
    "        # Calculate Normalized Quality Score\n",
    "        df['Quality_Score'] = np.where(df['WeightedAvgZ'] >= 0, \n",
    "                                    1 + df['WeightedAvgZ'], \n",
    "                                    (1 - df['WeightedAvgZ'])**-1)\n",
    "\n",
    "        df = df[['Date_End', 'Symbol', 'Sector','Quality_Score']].dropna().reset_index(drop=True).dropna()\n",
    "\n",
    "        df['Quality_pct_rank'] = df.groupby('Date_End', group_keys=False)['Quality_Score'].apply(lambda x : x.rank(pct=True))\n",
    "        # GrowthDate = self.__dataCursor.fetch_data_from_database(table_name='GrowthDate', no_of_years=25)[['Date', 'Qtr']]\n",
    "        GrowthDate = pd.read_csv('growth_date_latest.csv', parse_dates=['Date'])[['Date', 'Qtr']]\n",
    "        GrowthDate['Qtr'] = GrowthDate['Qtr'].astype('int')\n",
    "\n",
    "        # GrowthDate Mapping  \n",
    "        final_df =  pd.merge(df, GrowthDate, left_on='Date_End', right_on='Qtr').drop(columns=['Date_End', 'Qtr'])\n",
    "        final_df = final_df[['Date', 'Symbol', 'Sector', 'Quality_Score', 'Quality_pct_rank']].sort_values(by='Date').reset_index(drop=True)\n",
    "        \n",
    "        price_data['Date'] = pd.to_datetime(price_data['Date'])\n",
    "        # final_df['Date'] = pd.to_datetime(final_df['Date'])\n",
    "        merged_df = pd.merge(final_df[['Date', 'Symbol', 'Quality_pct_rank']], price_data, on=['Date', 'Symbol'], how='outer')\n",
    "\n",
    "        merged_df['Quality_pct_rank'] = merged_df.groupby('Symbol', group_keys=False)['Quality_pct_rank'].apply(lambda x: x.fillna(method='ffill'))\n",
    "\n",
    "        merged_df = merged_df.groupby(\"Date\").apply(lambda x: x.sort_values(\"Mcap\", ascending = False).head(500)).reset_index(drop = True)\n",
    "\n",
    "        merged_df[\"Quality_pct_rank\"] = merged_df.groupby(\"Date\")[\"Quality_pct_rank\"].rank(pct = True)\n",
    "\n",
    "        qualityfinal = merged_df[['Date','Symbol','Quality_pct_rank']]\n",
    "\n",
    "        top500 = qualityfinal.filter([\"Symbol\", \"Date\", \"Quality_pct_rank\"])\n",
    "\n",
    "        ## Convert the long from Dataframe to wide form dataframe\n",
    "        top500 = top500.pivot_table(index='Date',columns='Symbol',values='Quality_pct_rank').reset_index()\n",
    "        ## Shifting the Date column\n",
    "        top500[\"Date\"] = top500[\"Date\"].shift(-1)\n",
    "        ## Fill NaN value of date with todays date.\n",
    "        top500.iloc[-1, top500.columns.get_loc(\"Date\")] = pd.to_datetime(date.today())\n",
    "\n",
    "        ## Re-converting the wide form dataframe to long form dataframe\n",
    "        top500 = top500.melt(id_vars='Date', value_name = \"QualityQuarter\")\n",
    "        ## Drop na and reset the index\n",
    "        top500 = top500.dropna().reset_index(drop = True)\n",
    "        top500 = top500[top500[\"Date\"] >= \"2006-01-01\"].reset_index(drop = True).copy()\n",
    "\n",
    "         ## Store the Factor Data in dictionary.\n",
    "        self.factorData[\"QualityQuarter\"] = top500.copy()\n",
    "        ## Filter the data for latest update or values.\n",
    "        self.recentFactorUpdate[\"QualityQuarter\"] = top500[top500[\"Date\"] == top500[\"Date\"].max()].reset_index(drop = True)\n",
    "        ## Combining the Factors in one Dataframe\n",
    "        self.stockFactors = pd.concat([self.recentFactorUpdate[key].drop_duplicates([\"Date\",\"Symbol\"]).set_index([\"Date\", \"Symbol\"]) for key in self.recentFactorUpdate.keys()], axis = 1)\n",
    "        self.stockFactors.reset_index(inplace = True)\n",
    "\n",
    "        print(\"### QUALITY QUARTER COMPLETE ###\")\n",
    "\n",
    "    def generate_ValueYieldNoPeg(self):\n",
    "\n",
    "        ## Fetch the Stock Value Data\n",
    "        if self.stockValueData is None:\n",
    "            self.__readValueData()\n",
    "\n",
    "        ## Fetch the \n",
    "        if self.stockPriceData is None:\n",
    "            self.__readPriceData()\n",
    "\n",
    "        if self.sectorData is None:\n",
    "            self.__readSectorData()\n",
    "\n",
    "        \n",
    "        ### VALUE ###\n",
    "        # Filter the rows for Top-500 companies historically\n",
    "        valueData = self.stockValueData[self.stockValueData[\"Symbol\"].isin(self.strategyUniverse[\"Symbol\"].unique())].copy()\n",
    "\n",
    "        ## List of Fundamental Factor Value.\n",
    "        fundamentalValueFactor = [\"EV_EBITDA\", \"PS\", \"PB\", \"PE\"]\n",
    "        ## Replace -ve value with NaN value for all the fundamental value factor\n",
    "        for factor in fundamentalValueFactor:\n",
    "            valueData[factor] = np.where(valueData[factor] <= 0, np.nan, valueData[factor])\n",
    "            valueData[factor] = 1/valueData[factor]\n",
    "\n",
    "        ## Sort and reset the index\n",
    "        valueData.sort_values([\"Symbol\", \"Date\"], inplace = True)\n",
    "        valueData.reset_index(drop = True, inplace = True)\n",
    "\n",
    "\n",
    "        ### Dividend ###\n",
    "        dividendData = self.generate_Dividend(valueYieldCall = True)\n",
    "\n",
    "        ## EPS ###\n",
    "        epsData = self.generate_Growth(valueYieldCall = True)\n",
    "\n",
    "        ## ROE\n",
    "        roe = self.generate_QualityQuarter(valueYieldCall=True)\n",
    "        roe[\"Date_End\"] = pd.to_datetime(roe['Date_End'],format=\"%Y%m\")\n",
    "\n",
    "        growth = pd.merge(epsData[[\"Date\", \"Date_End\", \"Symbol\",\"EPS_DILUTED_change\"]], \n",
    "                          roe[[\"Date_End\", \"Symbol\", \"ROE_ttm\"]], on = [\"Date_End\", \"Symbol\"], how = \"left\")\n",
    "\n",
    "        final=pd.merge(valueData,dividendData,on=['Date','Symbol'])\n",
    "        final=pd.merge(final,growth,on=['Date','Symbol'],how='left')\n",
    "\n",
    "        final = final.sort_values([\"Symbol\",\"Date\"]).reset_index(drop = True)\n",
    "        final[['ROE_ttm', \"EPS_DILUTED_change\"]]=final.groupby('Symbol')[['ROE_ttm', \"EPS_DILUTED_change\"]].ffill()\n",
    "        final['PEG']=final['PE']/final['EPS_DILUTED_change']\n",
    "\n",
    "        factorUniverse = ['EV_EBITDA', 'PS','PB', 'PE', 'Dividend','ROE_ttm']\n",
    "\n",
    "        final = pd.merge(self.strategyUniverse, final[[\"Date\", \"Symbol\"] + factorUniverse], on = [\"Date\",\"Symbol\"], how = \"left\")\n",
    "\n",
    "        final[\"AbsRank\"] = final.groupby(\"Date\")[factorUniverse].rank(pct = True).mean(axis = 1)\n",
    "        final[\"PeerRank\"] = final.groupby([\"Date\", \"Sector\"])[factorUniverse].rank(pct = True).mean(axis = 1)\n",
    "\n",
    "        final['ValueYieldNoPeg'] = final['PeerRank']*0.95 + final['AbsRank']*0.05\n",
    "        final['ValueABSNoPeg'] = final['PeerRank']*0.05 + final['AbsRank']*0.95        \n",
    "\n",
    "        result = list()\n",
    "\n",
    "        # Iterate over each volatility-related column\n",
    "        for col in [\"ValueYieldNoPeg\", \"ValueABSNoPeg\"]:\n",
    "\n",
    "            # Filter the dataframe to keep only the \"Symbol\", \"Date\", and current column\n",
    "            temp = final.filter([\"Symbol\", \"Date\", col])\n",
    "\n",
    "            # Pivot the table to have dates as rows and symbols as columns, with values from the current column\n",
    "            temp = temp.pivot_table(index='Date', columns='Symbol', values=col).reset_index()\n",
    "\n",
    "            # Shift the \"Date\" column up by one row to align data with the next date\n",
    "            temp[\"Date\"] = temp[\"Date\"].shift(-1)\n",
    "\n",
    "            # Set the last row's \"Date\" to today's date to capture current data\n",
    "            temp.iloc[-1, temp.columns.get_loc(\"Date\")] = pd.to_datetime(date.today())\n",
    "\n",
    "            # Unpivot the table to return it to long format with \"Date\", \"Symbol\", and current column values\n",
    "            temp = temp.melt(id_vars='Date', value_name=col)\n",
    "\n",
    "            # Drop rows with missing values and reset the index\n",
    "            temp = temp.dropna().reset_index(drop=True)\n",
    "\n",
    "            # Filter data to include only records from January 1, 2006, onwards and reset the index\n",
    "            temp = temp[temp[\"Date\"] >= \"2006-01-01\"].reset_index(drop=True).copy()\n",
    "\n",
    "            # Set the index to \"Date\" and \"Symbol\" for each transformed column and add to the result list\n",
    "            result.append(temp.set_index([\"Date\", \"Symbol\"]))\n",
    "\n",
    "\n",
    "        # Concatenate all transformed columns along the columns axis to create a combined dataframe\n",
    "        scores = pd.concat(result, axis=1).reset_index()\n",
    "\n",
    "        ## Store the Factor Data in dictionary.\n",
    "        self.factorData[\"ValueYieldNoPeg\"] = scores.copy()\n",
    "        ## Filter the data for latest update or values.\n",
    "        self.recentFactorUpdate[\"ValueYieldNoPeg\"] = scores[scores[\"Date\"] == scores[\"Date\"].max()].reset_index(drop = True)\n",
    "        ## Combining the Factors in one Dataframe\n",
    "        self.stockFactors = pd.concat([self.recentFactorUpdate[key].set_index([\"Date\", \"Symbol\"]) for key in self.recentFactorUpdate.keys()], axis = 1)\n",
    "        self.stockFactors.reset_index(inplace = True)\n",
    "\n",
    "        print(\"### ValueYieldNoPEG COMPLETE ###\")\n",
    "\n",
    "    def generate_ValueYieldExDiv(self):\n",
    "\n",
    "        ## Fetch the Stock Value Data\n",
    "        if self.stockValueData is None:\n",
    "            self.__readValueData()\n",
    "\n",
    "        ## Fetch the \n",
    "        if self.stockPriceData is None:\n",
    "            self.__readPriceData()\n",
    "\n",
    "        if self.sectorData is None:\n",
    "            self.__readSectorData()\n",
    "\n",
    "        \n",
    "        ### VALUE ###\n",
    "        # Filter the rows for Top-500 companies historically\n",
    "        valueData = self.stockValueData[self.stockValueData[\"Symbol\"].isin(self.strategyUniverse[\"Symbol\"].unique())].copy()\n",
    "\n",
    "        ## List of Fundamental Factor Value.\n",
    "        fundamentalValueFactor = [\"EV_EBITDA\", \"PS\", \"PB\", \"PE\"]\n",
    "        ## Replace -ve value with NaN value for all the fundamental value factor\n",
    "        for factor in fundamentalValueFactor:\n",
    "            valueData[factor] = np.where(valueData[factor] <= 0, np.nan, valueData[factor])\n",
    "            valueData[factor] = 1/valueData[factor]\n",
    "\n",
    "        ## Sort and reset the index\n",
    "        valueData.sort_values([\"Symbol\", \"Date\"], inplace = True)\n",
    "        valueData.reset_index(drop = True, inplace = True)\n",
    "\n",
    "\n",
    "        ### Dividend ###\n",
    "        dividendData = self.generate_Dividend(valueYieldCall = True)\n",
    "\n",
    "        ## EPS ###\n",
    "        epsData = self.generate_Growth(valueYieldCall = True)\n",
    "\n",
    "        ## ROE\n",
    "        roe = self.generate_QualityQuarter(valueYieldCall=True)\n",
    "        roe[\"Date_End\"] = pd.to_datetime(roe['Date_End'],format=\"%Y%m\")\n",
    "\n",
    "        growth = pd.merge(epsData[[\"Date\", \"Date_End\", \"Symbol\",\"EPS_DILUTED_change\"]], \n",
    "                          roe[[\"Date_End\", \"Symbol\", \"ROE_ttm\"]], on = [\"Date_End\", \"Symbol\"], how = \"left\")\n",
    "\n",
    "        final=pd.merge(valueData,dividendData,on=['Date','Symbol'])\n",
    "        final=pd.merge(final,growth,on=['Date','Symbol'],how='left')\n",
    "\n",
    "        final = final.sort_values([\"Symbol\",\"Date\"]).reset_index(drop = True)\n",
    "        final[['ROE_ttm', \"EPS_DILUTED_change\"]]=final.groupby('Symbol')[['ROE_ttm', \"EPS_DILUTED_change\"]].ffill()\n",
    "        final['PEG']=final['PE']/final['EPS_DILUTED_change']\n",
    "\n",
    "        factorUniverse = ['EV_EBITDA', 'PS','PB', 'PE','PEG','ROE_ttm']\n",
    "\n",
    "        final = pd.merge(self.strategyUniverse, final[[\"Date\", \"Symbol\"] + factorUniverse], on = [\"Date\",\"Symbol\"], how = \"left\")\n",
    "\n",
    "        final[\"AbsRank\"] = final.groupby(\"Date\")[factorUniverse].rank(pct = True).mean(axis = 1)\n",
    "        final[\"PeerRank\"] = final.groupby([\"Date\", \"Sector\"])[factorUniverse].rank(pct = True).mean(axis = 1)\n",
    "\n",
    "        final['ValueYieldExDiv'] = final['PeerRank']*0.95 + final['AbsRank']*0.05\n",
    "        final['ValueABSExDiv'] = final['PeerRank']*0.05 + final['AbsRank']*0.95        \n",
    "\n",
    "        result = list()\n",
    "\n",
    "        # Iterate over each volatility-related column\n",
    "        for col in [\"ValueYieldExDiv\", \"ValueABSExDiv\"]:\n",
    "\n",
    "            # Filter the dataframe to keep only the \"Symbol\", \"Date\", and current column\n",
    "            temp = final.filter([\"Symbol\", \"Date\", col])\n",
    "\n",
    "            # Pivot the table to have dates as rows and symbols as columns, with values from the current column\n",
    "            temp = temp.pivot_table(index='Date', columns='Symbol', values=col).reset_index()\n",
    "\n",
    "            # Shift the \"Date\" column up by one row to align data with the next date\n",
    "            temp[\"Date\"] = temp[\"Date\"].shift(-1)\n",
    "\n",
    "            # Set the last row's \"Date\" to today's date to capture current data\n",
    "            temp.iloc[-1, temp.columns.get_loc(\"Date\")] = pd.to_datetime(date.today())\n",
    "\n",
    "            # Unpivot the table to return it to long format with \"Date\", \"Symbol\", and current column values\n",
    "            temp = temp.melt(id_vars='Date', value_name=col)\n",
    "\n",
    "            # Drop rows with missing values and reset the index\n",
    "            temp = temp.dropna().reset_index(drop=True)\n",
    "\n",
    "            # Filter data to include only records from January 1, 2006, onwards and reset the index\n",
    "            temp = temp[temp[\"Date\"] >= \"2006-01-01\"].reset_index(drop=True).copy()\n",
    "\n",
    "            # Set the index to \"Date\" and \"Symbol\" for each transformed column and add to the result list\n",
    "            result.append(temp.set_index([\"Date\", \"Symbol\"]))\n",
    "\n",
    "\n",
    "        # Concatenate all transformed columns along the columns axis to create a combined dataframe\n",
    "        scores = pd.concat(result, axis=1).reset_index()\n",
    "\n",
    "        ## Store the Factor Data in dictionary.\n",
    "        self.factorData[\"ValueYieldExDiv\"] = scores.copy()\n",
    "        ## Filter the data for latest update or values.\n",
    "        self.recentFactorUpdate[\"ValueYieldExDiv\"] = scores[scores[\"Date\"] == scores[\"Date\"].max()].reset_index(drop = True)\n",
    "        ## Combining the Factors in one Dataframe\n",
    "        self.stockFactors = pd.concat([self.recentFactorUpdate[key].set_index([\"Date\", \"Symbol\"]) for key in self.recentFactorUpdate.keys()], axis = 1)\n",
    "        self.stockFactors.reset_index(inplace = True)\n",
    "\n",
    "        print(\"### ValueYieldExDiv COMPLETE ###\")\n",
    "\n",
    "    def generate_ShortAM(self,):\n",
    "\n",
    "        # Function to calculate log returns\n",
    "        def calculate_log_returns(df):\n",
    "            df['LogReturn'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "            return df.dropna()\n",
    "\n",
    "        # Function to calculate annualized standard deviation\n",
    "        def calculate_annualized_std(df, window=252):\n",
    "            return df['LogReturn'].rolling(window).std() * np.sqrt(window)\n",
    "\n",
    "        # Function to calculate momentum ratios\n",
    "        def calculate_momentum_ratios(series, period):\n",
    "            return series / series.shift(period) - 1\n",
    "\n",
    "        ################\n",
    "        ## PRICE DATA ##\n",
    "        ################\n",
    "        # Check if stock price data is loaded; if not, read it\n",
    "        if self.stockPriceData is None:\n",
    "            self.__readPriceData()\n",
    "\n",
    "        ## Drop the Unnecessary COlumns\n",
    "        priceData = self.stockPriceData.drop(columns = [\"Open\", \"High\", \"Low\", \"Volume\", \"Mcap\"]).copy()\n",
    "        \n",
    "        ## Filter the Symbol which are present in strategy universe\n",
    "        priceData = priceData[priceData[\"Symbol\"].isin(self.strategyUniverse[\"Symbol\"].unique())].reset_index(drop = True)\n",
    "\n",
    "        # Define periods for momentum ratios (in trading days)\n",
    "        periods = {\n",
    "            'MR1': 21,   # 1 month\n",
    "            'MR2': 42,   # 2 months\n",
    "            'MR3': 63,   # 3 months\n",
    "        }\n",
    "\n",
    "        # Apply log return calculation\n",
    "        priceData = priceData.groupby('Symbol', group_keys=False).apply(calculate_log_returns)\n",
    "    \n",
    "        # Calculate momentum ratios for each period\n",
    "        for label, period in periods.items():\n",
    "            priceData[label] = priceData.groupby('Symbol')['Close'].transform(lambda x: calculate_momentum_ratios(x, period))\n",
    "\n",
    "        # Calculate annualized standard deviation\n",
    "        priceData['AnnualizedStd'] = priceData.groupby('Symbol', group_keys=False).apply(calculate_annualized_std)\n",
    "\n",
    "        # Normalize the momentum ratios by dividing by the annualized standard deviation\n",
    "        for label in periods.keys():\n",
    "            priceData[label] /= priceData['AnnualizedStd']\n",
    "\n",
    "        priceData = priceData.sort_values([\"Date\", \"Symbol\"]).reset_index(drop = True)\n",
    "\n",
    "        # Calculate the mean and std deviation of each momentum ratio across the universe\n",
    "        for label in periods.keys():\n",
    "            priceData[f'mu_{label}'] = priceData.groupby('Date')[label].transform(lambda x: x.mean())\n",
    "            priceData[f'sigma_{label}'] = priceData.groupby('Date')[label].transform(lambda x: x.std())\n",
    "\n",
    "        # Calculate Z-scores for each period\n",
    "        for label in periods.keys():\n",
    "            priceData[f'Z_{label}'] = (priceData[label] - priceData[f'mu_{label}']) / priceData[f'sigma_{label}']\n",
    "\n",
    "        # Define specific combinations for which to calculate the final Z-scores\n",
    "        metrics = ['Z_MR1', 'Z_MR2', 'Z_MR3']\n",
    "\n",
    "        # Weighted average Z-score\n",
    "        priceData[\"WtdZScore\"] =priceData[metrics].mean(axis= 1)\n",
    "\n",
    "        # Normalized momentum score\n",
    "        priceData[f'Momentum'] = np.where(priceData[f'WtdZScore'] >= 0,\n",
    "                                                1 + priceData[f'WtdZScore'],\n",
    "                                                (1 - priceData[f'WtdZScore']) ** -1)\n",
    "\n",
    "        ## Filter the Required Columns  \n",
    "        priceData = priceData.filter(items = [\"Date\", \"Symbol\", \"Momentum\"])\n",
    "\n",
    "        ## Merge the Computed Metrics against the symbol on each day's universe\n",
    "        priceData = pd.merge(self.strategyUniverse, priceData, on = [\"Date\", \"Symbol\"], how = \"left\")\n",
    "\n",
    "        ## Computing the Percentile Score of the Symbols on each Date\n",
    "        priceData[\"Momentum\"] = priceData.groupby([\"Date\"])[\"Momentum\"].rank(ascending = True, pct = True)\n",
    "\n",
    "        ## Computing the Aggreate rank of Peer (Theme / Sector / GICS) on each date using Symbol\n",
    "        priceData[\"PeerMomentum\"] = priceData.groupby([\"Date\", \"Peer\"])[\"Momentum\"].transform(lambda x: x.mean())\n",
    "\n",
    "        ## Re-Ranki The Agg.Score of Peer (Theme / Sector / GICS) on each Date.\n",
    "        priceData[\"PeerMomentum\"] = priceData.groupby([\"Date\"])[\"PeerMomentum\"].rank(ascending = True, pct = True)\n",
    "\n",
    "        ## Filter the data after year 2006\n",
    "        priceData = priceData[priceData[\"Date\"] >= \"2006-01-01\"].reset_index(drop = True)\n",
    "         \n",
    "        # Initialize an empty list to store the transformed data for each volatility column\n",
    "        result = list()\n",
    "\n",
    "        # Iterate over each volatility-related column\n",
    "        for col in [\"Momentum\", \"PeerMomentum\"]:\n",
    "\n",
    "            # Filter the dataframe to keep only the \"Symbol\", \"Date\", and current column\n",
    "            temp = priceData.filter([\"Symbol\", \"Date\", col])\n",
    "\n",
    "            # Pivot the table to have dates as rows and symbols as columns, with values from the current column\n",
    "            temp = temp.pivot_table(index='Date', columns='Symbol', values=col).reset_index()\n",
    "\n",
    "            # Shift the \"Date\" column up by one row to align data with the next date\n",
    "            temp[\"Date\"] = temp[\"Date\"].shift(-1)\n",
    "\n",
    "            # Set the last row's \"Date\" to today's date to capture current data\n",
    "            temp.iloc[-1, temp.columns.get_loc(\"Date\")] = pd.to_datetime(date.today())\n",
    "\n",
    "            # Unpivot the table to return it to long format with \"Date\", \"Symbol\", and current column values\n",
    "            temp = temp.melt(id_vars='Date', value_name=col)\n",
    "\n",
    "            # Drop rows with missing values and reset the index\n",
    "            temp = temp.dropna().reset_index(drop=True)\n",
    "\n",
    "            # Filter data to include only records from January 1, 2006, onwards and reset the index\n",
    "            temp = temp[temp[\"Date\"] >= \"2006-01-01\"].reset_index(drop=True).copy()\n",
    "\n",
    "            # Set the index to \"Date\" and \"Symbol\" for each transformed column and add to the result list\n",
    "            result.append(temp.set_index([\"Date\", \"Symbol\"]))\n",
    "\n",
    "        \n",
    "        # Concatenate all transformed columns along the columns axis to create a combined dataframe\n",
    "        scores = pd.concat(result, axis=1).reset_index()\n",
    "        scores.columns = scores.columns.str.replace(\"Momentum\", \"AM\").str.replace(\"Peer\", self.peer)\n",
    "        scores.rename(columns = {\"AM\" : \"ShortAM\", f\"{self.peer}AM\" : f\"Short{self.peer}AM\"},inplace=True)\n",
    "        self.x = scores.copy()\n",
    "\n",
    "        # Sort the final dataframe by \"Symbol\" and \"Date\" columns for organized viewing and reset the index\n",
    "        scores = scores.sort_values([\"Symbol\", \"Date\"]).reset_index(drop=True)\n",
    "\n",
    "        ## Store the Factor Data in dictionary.\n",
    "        self.factorData[\"ShortAM\"] = scores.copy()\n",
    "\n",
    "        ## Filter the data for latest update or values.\n",
    "        self.recentFactorUpdate[\"ShortAM\"] = scores[scores[\"Date\"] == scores[\"Date\"].max()].reset_index(drop = True)\n",
    "        \n",
    "        ## Combining the Factors in one Dataframe\n",
    "        self.stockFactors = pd.concat([self.recentFactorUpdate[key].set_index([\"Date\", \"Symbol\"]) for key in self.recentFactorUpdate.keys()], axis = 1)\n",
    "        self.stockFactors.reset_index(inplace = True)\n",
    "\n",
    "        print(\"### SHORT MOMENTUM COMPLETE ###\")\n",
    "\n",
    "        del priceData, result, temp, scores\n",
    "    \n",
    "    def generate_AllFactors(self,save_appender = False):\n",
    "        ## FUnction call for LTM Factor\n",
    "        # self.generate_LTM()\n",
    "        # ## Function call for Momentum Factor\n",
    "        # self.generate_Momentum()\n",
    "        # ## Function call for Theme Factor\n",
    "        # self.generate_Theme()\n",
    "        # ## Function call for Volatility Factor\n",
    "        # self.generate_Volatility()\n",
    "        ## Function call for Value Factor\n",
    "        # self.generate_ValueYield()\n",
    "        ## Function call for Growth Factor\n",
    "        self.generate_Growth()\n",
    "        ## Function call for Quality Factor\n",
    "        self.generate_QualityAnnual()\n",
    "        ## Function call for Quality Factor\n",
    "        self.generate_QualityQuarter()\n",
    "        ## Function call for EM Factor\n",
    "        # self.generate_EM()\n",
    "        ## Function call for Dividend Factor\n",
    "        # self.generate_Dividend()\n",
    "        ## Function call for LTMA Factor\n",
    "        # self.generate_LTMA()\n",
    "        ## Function call for Low Vol Factor\n",
    "        # self.generate_LowVol()\n",
    "        ## Function call for AM Factor\n",
    "        # self.generate_AM()\n",
    "        ## Function call for Short AM Factor\n",
    "        # self.generate_ShortAM()\n",
    "        ## Function call for AM Factor\n",
    "        # self.generate_ValueYieldNoPeg()\n",
    "        ## Function call for AM Factor\n",
    "        # self.generate_ValueYieldExDiv()\n",
    "\n",
    "        # Extract unique dates from the 'Date' column of the strategyUniverse DataFrame.\n",
    "        # Drop duplicates and reset the index to create a clean, unique list of dates.\n",
    "        dates = self.strategyUniverse[[\"Date\"]].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "        # Create a new column 'DateO' which is the next date (shifted upwards by one row).\n",
    "        # This maps each date to the following date in the dataset.\n",
    "        dates[\"DateO\"] = dates[\"Date\"].shift(-1)\n",
    "\n",
    "        # Create a dictionary to map each date to its corresponding next date ('DateO').\n",
    "        dateMapper = dict(zip(dates[\"Date\"], dates[\"DateO\"]))\n",
    "\n",
    "        # Make a copy of the original strategyUniverse DataFrame to avoid modifying it directly.\n",
    "        strategyUniverse = self.strategyUniverse.copy()\n",
    "\n",
    "        # Map the 'DateO' column in the copied DataFrame using the dateMapper.\n",
    "        strategyUniverse[\"DateO\"] = strategyUniverse[\"Date\"].replace(dateMapper)\n",
    "\n",
    "        # Fill any missing values in 'DateO' (last row) with today's date.\n",
    "        strategyUniverse[\"DateO\"] = strategyUniverse[\"DateO\"].fillna(pd.to_datetime(date.today()))\n",
    "\n",
    "        # Drop the original 'Date' column as 'DateO' will now replace it.\n",
    "        strategyUniverse.drop(columns=[\"Date\"], inplace=True)\n",
    "\n",
    "        # Rename the 'DateO' column back to 'Date' to maintain consistency in column names.\n",
    "        strategyUniverse.rename(columns={'DateO': \"Date\"}, inplace=True)\n",
    "\n",
    "        # Reorder the columns so that the 'Date' column is the last one in the DataFrame.\n",
    "        strategyUniverse = strategyUniverse[list(strategyUniverse.columns[-1:]) + list(strategyUniverse.columns[:-2])]\n",
    "\n",
    "        ## Generating the Appender file only when all the factors are generated.\n",
    "        self.appender = pd.DataFrame()\n",
    "        for key in self.factorData.keys():\n",
    "            if len(self.appender) == 0:\n",
    "                self.appender = pd.merge(strategyUniverse,  self.factorData[key], on = [\"Date\", \"Symbol\"], how = \"left\")\n",
    "            else:\n",
    "                self.appender = pd.merge(self.appender, self.factorData[key], on = [\"Date\", \"Symbol\"], how= \"left\")\n",
    "\n",
    "        ## Save the appender file to csv only if the flag is True\n",
    "        if save_appender:\n",
    "            ## If there is no directory then create the directory\n",
    "            if not os.path.isdir(\"./DailyAppender\"):\n",
    "                os.mkdir(\"./DailyAppender\")\n",
    "\n",
    "            # self.appender.to_csv(self.uniqueFileName(f\"./DailyAppender/Appender_{date.today().strftime('%d%b%Y')}.csv\"), index = False)\n",
    "            self.appender.to_csv(f\"./DailyAppender/Appender_Cons_latest.csv\", index = False)\n",
    "\n",
    "        print(\"### ALL THE FACTORS ARE GENERATED ###\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### LTM COMPLETE ###\n",
      "### ValueYield COMPLETE ###\n",
      "### GROWTH COMPLETE ###\n",
      "### QUALITY ANNUAL COMPLETE ###\n",
      "### QUALITY QUARTER COMPLETE ###\n",
      "### EM COMPLETE ###\n",
      "### DIVIDEND COMPLETE ###\n",
      "### LTMA COMPLETE ###\n",
      "### LOWVOL COMPLETE ###\n",
      "### MOMENTUM COMPLETE ###\n",
      "### ULTRA SHORT MOMENTUM COMPLETE ###\n",
      "### SHORT MOMENTUM COMPLETE ###\n",
      "### ValueYieldNoPEG COMPLETE ###\n",
      "### ValueYieldExDiv COMPLETE ###\n",
      "### MID MOMENTUM COMPLETE ###\n",
      "### LONG MOMENTUM COMPLETE ###\n",
      "### MID MOMENTUM COMPLETE ###\n",
      "### BETA COMPLETE ###\n",
      "### TREND COMPLETE ###\n",
      "### SHIFTED AM COMPLETE ###\n",
      "### 3D QUALITY COMPLETE ###\n",
      "### 3D VALUE COMPLETE ###\n",
      "### ALL THE FACTORS ARE GENERATED ###\n",
      "### GROWTH COMPLETE ###\n",
      "### QUALITY ANNUAL COMPLETE ###\n",
      "### QUALITY QUARTER COMPLETE ###\n",
      "### ALL THE FACTORS ARE GENERATED ###\n",
      "full appender generated\n"
     ]
    }
   ],
   "source": [
    "## Import the Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import functools as ft\n",
    "import datetime\n",
    "import os\n",
    "import itertools\n",
    "import logging\n",
    "import smtplib\n",
    "import sys\n",
    "import itertools\n",
    "import zipfile\n",
    "import shutil\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from tqdm import tqdm\n",
    "from email.message import EmailMessage\n",
    "\n",
    "# from rebalancedate import rebalancedates\n",
    "\n",
    "# from AQUA import AQUA\n",
    "# from getNav import navCalculator\n",
    "\n",
    "\n",
    "\n",
    "# A function to get Yesterday's Date\n",
    "def Yesterdays_Date():\n",
    "    todays_date = datetime.date.today()\n",
    "    yesterdays_date = todays_date - datetime.timedelta(days=1)\n",
    "    return yesterdays_date\n",
    "\n",
    "# set logger\n",
    "def set_logger(log_folderpath: str):\n",
    "    \n",
    "    # Setting up the logger\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    formatter = logging.Formatter('%(asctime)s %(message)s')\n",
    "\n",
    "    #date\n",
    "    date_str = Yesterdays_Date().strftime('%d%m%Y')\n",
    "    today_date = datetime.date.today().strftime('%Y-%m-%d')\n",
    "\n",
    "    # Creating log folder datewise\n",
    "    if not os.path.exists(log_folderpath):\n",
    "        os.makedirs(log_folderpath)\n",
    "\n",
    "    # Setting log file name as the date for which updation is taking place\n",
    "    # file_handler = logging.FileHandler(fr\"{log_folderpath_date}\\Factor_Sheet_log_{date_str}.log\", mode='a')\n",
    "    file_handler = logging.FileHandler(fr\"{log_folderpath}\\Factor_Sheet_log_{today_date}.log\", mode='a')\n",
    "    file_handler.setFormatter(formatter)\n",
    "    if (logger.hasHandlers()):\n",
    "        logger.handlers.clear()\n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "    # Logging\n",
    "    logger.debug(f\"Date:{today_date}\")\n",
    "    logger.debug('logger set')\n",
    "\n",
    "    return logger\n",
    "\n",
    "# send mail\n",
    "def send_mail(from_email: str, recipients: list,  file_path_ls: list = None, cc_email: list = None, subject: str = None, plain_body_text: str = None, html_body_text: str = None):\n",
    "\n",
    "    logger.debug('sending_email')\n",
    "    logger.debug(f'from_email = {from_email}')\n",
    "    logger.debug(f'recipients = {recipients}')\n",
    "    logger.debug(f'cc_email = {cc_email}')\n",
    "    logger.debug(f'file_path_ls = {file_path_ls}')\n",
    "    logger.debug(f'subject = {subject}')\n",
    "    logger.debug(f'plain_body_text = {plain_body_text}')\n",
    "    logger.debug(f'html_body_text = {html_body_text}')\n",
    "\n",
    "    msg = EmailMessage()\n",
    "\n",
    "    #recipients\n",
    "    recipients_emails_str = ', '.join(recipients)\n",
    "\n",
    "    #cc_email\n",
    "    if cc_email != None:\n",
    "        cc_email_str = ', '.join(cc_email)\n",
    "\n",
    "    # Set plain text (fallback)\n",
    "    if plain_body_text != None:\n",
    "        msg.set_content(plain_body_text)\n",
    "\n",
    "    # Set HTML content\n",
    "    if html_body_text != None:\n",
    "        msg.add_alternative(html_body_text, subtype='html')\n",
    "\n",
    "    #add attachment:\n",
    "    if file_path_ls != None:\n",
    "        for file_path, maintype, subtype, filename in file_path_ls:\n",
    "            try:\n",
    "                with open(file_path, 'rb') as fp:\n",
    "                    file_data = fp.read()\n",
    "                    msg.add_attachment(file_data, maintype=maintype, subtype=subtype, filename=filename)\n",
    "            except FileNotFoundError as e:\n",
    "                logger.debug(f'error = {e}')\n",
    "\n",
    "\n",
    "    #mail subject, from, to\n",
    "    if subject != None:\n",
    "        msg['Subject'] = subject\n",
    "    msg['From'] = from_email\n",
    "    msg['To'] = recipients_emails_str\n",
    "    if cc_email != None:\n",
    "        msg['Cc'] = cc_email_str\n",
    "\n",
    "    # Send email\n",
    "    with smtplib.SMTP(host='192.168.11.18', port=25) as s:\n",
    "        s.send_message(msg)\n",
    "        logger.debug('mail send')\n",
    "\n",
    "# Folder Path where log files will be stored\n",
    "today_date = datetime.date.today().strftime('%Y-%m-%d')\n",
    "# log_folderpath = fr\"C:\\Users\\ronaksinghsahni\\OneDrive - PRABHUDAS LILLADHER PVT. LTD\\Documents\\sagar_appender\\output\\{today_date}\"\n",
    "# log_folderpath = fr\"C:\\Users\\Administrator\\PycharmProjects\\Projects\\sagar_appender\\akshat_appender\\output\\{today_date}_akshat\"\n",
    "log_folderpath = fr\"output\\{today_date}_akshat\"\n",
    "# log_folderpath = fr\"C:\\Users\\Administrator\\PycharmProjects\\Projects\\sagar_appender\\output\\{today_date}\\Factor_Sheet_log_{today_date}.txt\"\n",
    "\n",
    "logger = set_logger(log_folderpath)\n",
    "\n",
    "# try:\n",
    "factor = EquityFactors(noOfYears = 30, correct = True, peer = \"Sector\")\n",
    "factor.generate_AllFactors(save_appender=True)\n",
    "appender = factor.appender.copy()\n",
    "logger.debug('EquityFactors completed appender generated')\n",
    "full_appender = True\n",
    "# appender = pd.read_csv('DailyAppender/Appender_latest.csv', parse_dates=['Date'])\n",
    "# except Exception as e:\n",
    "#     logger.debug(f'error occurred in function EquityFactors (appender): {e}')\n",
    "\n",
    "\n",
    "#read consol factor csv files: appender, stockPriceData, benchmark\n",
    "# class ReadCSVConsolFactor:\n",
    "\n",
    "#     def __init__(self):\n",
    "#         ## Initialize the variables\n",
    "#         self.appender = pd.read_csv('DailyAppender/Appender_Cons_latest.csv', parse_dates=['Date'])\n",
    "#         self.stockPriceData = pd.read_csv('DailyAppender/stockPriceData.csv', parse_dates=['Date'])\n",
    "#         self.benchmark = pd.read_csv('DailyAppender/benchmark.csv', parse_dates=['Date'])\n",
    "\n",
    "# consol_factor = ReadCSVConsolFactor()\n",
    "# consol_factor_df = consol_factor.appender.copy()\n",
    "\n",
    "# try:\n",
    "consol_factor = EquityFactorsConsol(noOfYears = 30, correct = True, peer = \"Sector\")\n",
    "consol_factor.generate_AllFactors(save_appender=False)\n",
    "consol_factor_df = consol_factor.appender.copy()\n",
    "# consol_factor_df.to_csv('DailyAppender/Appender_Cons_latest.csv', index=False)\n",
    "logger.debug('EquityFactorsConsol completed appender_consol generated')\n",
    "consol_factor_df.rename(columns={'Growth': 'GrowthConsol', 'QualityAnnual': 'QualityAnnualConsol', 'QualityQuarter': 'QualityQuarterConsol'}, inplace=True)\n",
    "consol_factor_df = consol_factor_df[['Date', 'Symbol','GrowthConsol', 'QualityAnnualConsol', 'QualityQuarterConsol']]\n",
    "# except Exception as e:\n",
    "#     logger.debug(f'error occurred in function EquityFactorsConsol (appender_consol): {e}')\n",
    "\n",
    "#generate output csv of stockPriceData and benchmark\n",
    "# consol_factor.stockPriceData.to_csv('DailyAppender/stockPriceData.csv', index=False)\n",
    "# consol_factor.benchmark[['Date', 'BenchmarkPrice']].to_csv('DailyAppender/benchmark.csv', index=False)\n",
    "# logger.debug('stockPriceData.csv and benchmark.csv generated')\n",
    "\n",
    "# shift date by -1 and shift nan date to todays date\n",
    "def date_shift(df):\n",
    "    df['date_shift'] = df.groupby(['Symbol'])['Date'].shift(-1)\n",
    "    todays_date = datetime.datetime.today()\n",
    "    todays_date = pd.to_datetime(todays_date.strftime('%Y-%m-%d'))\n",
    "    # df['date_shift'].fillna(pd.to_datetime(todays_date), inplace=True)\n",
    "    df['date_shift'] = df['date_shift'].fillna(pd.to_datetime(todays_date))\n",
    "    df['Date'] = df['date_shift']\n",
    "    df.drop(columns={'date_shift'}, inplace=True)\n",
    "    return df\n",
    "\n",
    "appender_final = appender.copy()\n",
    "\n",
    "def calculate_distance52Low(df):  \n",
    "    df['52W_Low'] = df.groupby('Symbol')['Close'].transform(lambda x: x.rolling(window=252, min_periods=1).min())\n",
    "    df['percentage_change_52WLow'] = ((df['52W_Low'] / df['Close']) - 1).mul(100)\n",
    "    # df = df[df['Date'] == df['Date'].iloc[-1]].reset_index(drop=True)\n",
    "    df = df[['Date', 'Symbol', 'percentage_change_52WLow']]\n",
    "    return df\n",
    "\n",
    "def calculate_append_value_price(appender_final, price_data):\n",
    "    # 52 week low\n",
    "    # price_data = consol_factor.stockPriceData.copy()\n",
    "    price_data.rename(columns={'Price': 'Close'}, inplace=True)\n",
    "    price_data = price_data[['Date', 'Symbol', 'Close']].reset_index(drop=True)\n",
    "    latest_52WLow = calculate_distance52Low(price_data.copy(deep=True))\n",
    "    latest_52WLow = date_shift(latest_52WLow)\n",
    "    # ValuePrice\n",
    "    appender_final = pd.merge(appender_final, latest_52WLow, how='left', on=['Date', 'Symbol'])\n",
    "    appender_final['ValuePrice'] = appender_final.groupby('Date')['percentage_change_52WLow'].rank(pct=True, ascending=True)\n",
    "    # appender_final['ValuePrice'] = appender_final['percentage_change_52WLow'].rank(pct=True, ascending=True)\n",
    "    appender_final.drop(columns={'percentage_change_52WLow'}, inplace=True)\n",
    "    return appender_final\n",
    "\n",
    "try:\n",
    "    appender_final = calculate_append_value_price(appender_final, consol_factor.stockPriceData.copy())\n",
    "    logger.debug('calculate_append_value_price completed')\n",
    "except Exception as e:\n",
    "    logger.debug(f'error occurred in function calculate_append_value_price: {e}')\n",
    "\n",
    "#append factor sector, theme\n",
    "def append_factor_sector_theme(appender_final, factor_ls, classification_type_ls):\n",
    "    for classification in classification_type_ls:\n",
    "        for factor_name in factor_ls:\n",
    "            factor_sector = appender_final.groupby(['Date', classification])[factor_name].mean().rank(pct=True, ascending=True).reset_index()\n",
    "            factor_sector.rename(columns={factor_name: f'{classification}{factor_name}'}, inplace=True)\n",
    "            appender_final = pd.merge(appender_final, factor_sector, how='left', on=['Date', classification])\n",
    "    return appender_final\n",
    "\n",
    "# try:\n",
    "# consol factors\n",
    "appender_final = pd.merge(appender_final, consol_factor_df, how='left', on=['Date', 'Symbol'])\n",
    "\n",
    "append_factor_sector_theme_ls = ['HighBeta', 'LowBeta', 'ValueYield', 'ValueABS', 'ValuePrice', 'Growth', 'GrowthConsol', 'TrendMR', 'AntiTrendMR']\n",
    "appender_final = append_factor_sector_theme(appender_final, factor_ls=append_factor_sector_theme_ls, classification_type_ls=['Sector', 'Theme'])\n",
    "\n",
    "append_factor_theme_ls = ['AM', 'UltraShortAM', 'ShortAM', 'MidAM', 'LongAM', 'LTM', 'LTMA', 'EM', 'LowVol', 'DownVol', 'AvgVol']\n",
    "appender_final = append_factor_sector_theme(appender_final, factor_ls=append_factor_theme_ls, classification_type_ls=['Theme'])\n",
    "\n",
    "logger.debug('append_factor_sector_theme completed')\n",
    "# except Exception as e:\n",
    "#     logger.debug(f'error occurred in function append_factor_sector_theme: {e}')\n",
    "\n",
    "#full appender generate\n",
    "if full_appender:\n",
    "    appender_final.to_csv('DailyAppender/Full_appender.csv', index=False)\n",
    "    print('full appender generated')\n",
    "    logger.debug('full appender generated')\n",
    "    # sys.exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Mcap</th>\n",
       "      <th>MCAP_Type</th>\n",
       "      <th>Sector</th>\n",
       "      <th>Theme</th>\n",
       "      <th>GICS</th>\n",
       "      <th>LTM</th>\n",
       "      <th>ValueYield</th>\n",
       "      <th>ValueABS</th>\n",
       "      <th>...</th>\n",
       "      <th>ThemeUltraShortAM</th>\n",
       "      <th>ThemeShortAM</th>\n",
       "      <th>ThemeMidAM</th>\n",
       "      <th>ThemeLongAM</th>\n",
       "      <th>ThemeLTM</th>\n",
       "      <th>ThemeLTMA</th>\n",
       "      <th>ThemeEM</th>\n",
       "      <th>ThemeLowVol</th>\n",
       "      <th>ThemeDownVol</th>\n",
       "      <th>ThemeAvgVol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2006-01-03</td>\n",
       "      <td>3IINFOLTD</td>\n",
       "      <td>10308.833182</td>\n",
       "      <td>Small</td>\n",
       "      <td>IT</td>\n",
       "      <td>IT - Software</td>\n",
       "      <td>InformationTechnology</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.411617</td>\n",
       "      <td>0.657857</td>\n",
       "      <td>0.574197</td>\n",
       "      <td>0.427606</td>\n",
       "      <td>0.006074</td>\n",
       "      <td>0.463739</td>\n",
       "      <td>0.457505</td>\n",
       "      <td>0.583383</td>\n",
       "      <td>0.523855</td>\n",
       "      <td>0.562052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2006-01-03</td>\n",
       "      <td>3MINDIA</td>\n",
       "      <td>9662.050539</td>\n",
       "      <td>Small</td>\n",
       "      <td>Diversified</td>\n",
       "      <td>Diversified</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.385273</td>\n",
       "      <td>0.336062</td>\n",
       "      <td>0.505973</td>\n",
       "      <td>0.729573</td>\n",
       "      <td>0.210003</td>\n",
       "      <td>0.664657</td>\n",
       "      <td>0.420483</td>\n",
       "      <td>0.530650</td>\n",
       "      <td>0.549679</td>\n",
       "      <td>0.535854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2006-01-03</td>\n",
       "      <td>63MOONS</td>\n",
       "      <td>58362.450476</td>\n",
       "      <td>Large</td>\n",
       "      <td>IT</td>\n",
       "      <td>IT - Software</td>\n",
       "      <td>InformationTechnology</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.411617</td>\n",
       "      <td>0.657857</td>\n",
       "      <td>0.574197</td>\n",
       "      <td>0.427606</td>\n",
       "      <td>0.006074</td>\n",
       "      <td>0.463739</td>\n",
       "      <td>0.457505</td>\n",
       "      <td>0.583383</td>\n",
       "      <td>0.523855</td>\n",
       "      <td>0.562052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2006-01-03</td>\n",
       "      <td>AARTIIND</td>\n",
       "      <td>5455.246093</td>\n",
       "      <td>Small</td>\n",
       "      <td>Chemicals</td>\n",
       "      <td>Chemicals</td>\n",
       "      <td>Chemicals</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.637129</td>\n",
       "      <td>0.427392</td>\n",
       "      <td>0.471549</td>\n",
       "      <td>0.552877</td>\n",
       "      <td>0.064611</td>\n",
       "      <td>0.556062</td>\n",
       "      <td>0.551893</td>\n",
       "      <td>0.200178</td>\n",
       "      <td>0.206188</td>\n",
       "      <td>0.193764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2006-01-03</td>\n",
       "      <td>ABAN</td>\n",
       "      <td>21307.541602</td>\n",
       "      <td>Mid</td>\n",
       "      <td>Crude Oil</td>\n",
       "      <td>Oil Exploration</td>\n",
       "      <td>Energy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.879100</td>\n",
       "      <td>0.748131</td>\n",
       "      <td>0.409333</td>\n",
       "      <td>0.625954</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.645305</td>\n",
       "      <td>0.394864</td>\n",
       "      <td>0.561185</td>\n",
       "      <td>0.690176</td>\n",
       "      <td>0.601453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2412495</th>\n",
       "      <td>2025-06-24</td>\n",
       "      <td>ZENSARTECH</td>\n",
       "      <td>192371.773014</td>\n",
       "      <td>Small</td>\n",
       "      <td>IT</td>\n",
       "      <td>IT - Software</td>\n",
       "      <td>InformationTechnology</td>\n",
       "      <td>0.706377</td>\n",
       "      <td>0.661956</td>\n",
       "      <td>0.610128</td>\n",
       "      <td>...</td>\n",
       "      <td>0.812606</td>\n",
       "      <td>0.793572</td>\n",
       "      <td>0.403107</td>\n",
       "      <td>0.459416</td>\n",
       "      <td>0.601718</td>\n",
       "      <td>0.444828</td>\n",
       "      <td>0.493615</td>\n",
       "      <td>0.512314</td>\n",
       "      <td>0.468275</td>\n",
       "      <td>0.495354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2412496</th>\n",
       "      <td>2025-06-24</td>\n",
       "      <td>ZENTEC</td>\n",
       "      <td>171530.668333</td>\n",
       "      <td>Small</td>\n",
       "      <td>IT</td>\n",
       "      <td>IT - Software</td>\n",
       "      <td>InformationTechnology</td>\n",
       "      <td>0.924433</td>\n",
       "      <td>0.291073</td>\n",
       "      <td>0.288632</td>\n",
       "      <td>...</td>\n",
       "      <td>0.812606</td>\n",
       "      <td>0.793572</td>\n",
       "      <td>0.403107</td>\n",
       "      <td>0.459416</td>\n",
       "      <td>0.601718</td>\n",
       "      <td>0.444828</td>\n",
       "      <td>0.493615</td>\n",
       "      <td>0.512314</td>\n",
       "      <td>0.468275</td>\n",
       "      <td>0.495354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2412497</th>\n",
       "      <td>2025-06-24</td>\n",
       "      <td>ZFCVINDIA</td>\n",
       "      <td>250523.849472</td>\n",
       "      <td>Small</td>\n",
       "      <td>Automobile &amp; Ancillaries</td>\n",
       "      <td>Auto Ancillary</td>\n",
       "      <td>Automobile &amp; Ancillaries</td>\n",
       "      <td>0.303555</td>\n",
       "      <td>0.320072</td>\n",
       "      <td>0.411037</td>\n",
       "      <td>...</td>\n",
       "      <td>0.350545</td>\n",
       "      <td>0.607686</td>\n",
       "      <td>0.523744</td>\n",
       "      <td>0.365520</td>\n",
       "      <td>0.598362</td>\n",
       "      <td>0.505113</td>\n",
       "      <td>0.408815</td>\n",
       "      <td>0.442001</td>\n",
       "      <td>0.440641</td>\n",
       "      <td>0.447037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2412498</th>\n",
       "      <td>2025-06-24</td>\n",
       "      <td>ZYDUSLIFE</td>\n",
       "      <td>963066.551829</td>\n",
       "      <td>Mid</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>Pharmaceuticals &amp; Drugs</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>0.392913</td>\n",
       "      <td>0.806724</td>\n",
       "      <td>0.694777</td>\n",
       "      <td>...</td>\n",
       "      <td>0.433340</td>\n",
       "      <td>0.394800</td>\n",
       "      <td>0.356062</td>\n",
       "      <td>0.652217</td>\n",
       "      <td>0.304999</td>\n",
       "      <td>0.548429</td>\n",
       "      <td>0.469953</td>\n",
       "      <td>0.569681</td>\n",
       "      <td>0.592292</td>\n",
       "      <td>0.578240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2412499</th>\n",
       "      <td>2025-06-24</td>\n",
       "      <td>ZYDUSWELL</td>\n",
       "      <td>127410.641931</td>\n",
       "      <td>Small</td>\n",
       "      <td>FMCG</td>\n",
       "      <td>Consumer Food</td>\n",
       "      <td>Consumer Staples</td>\n",
       "      <td>0.318651</td>\n",
       "      <td>0.277663</td>\n",
       "      <td>0.249629</td>\n",
       "      <td>...</td>\n",
       "      <td>0.467106</td>\n",
       "      <td>0.361714</td>\n",
       "      <td>0.501832</td>\n",
       "      <td>0.617188</td>\n",
       "      <td>0.259321</td>\n",
       "      <td>0.537941</td>\n",
       "      <td>0.479920</td>\n",
       "      <td>0.718564</td>\n",
       "      <td>0.727590</td>\n",
       "      <td>0.729007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2412500 rows  76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Date      Symbol           Mcap MCAP_Type  \\\n",
       "0       2006-01-03   3IINFOLTD   10308.833182     Small   \n",
       "1       2006-01-03     3MINDIA    9662.050539     Small   \n",
       "2       2006-01-03     63MOONS   58362.450476     Large   \n",
       "3       2006-01-03    AARTIIND    5455.246093     Small   \n",
       "4       2006-01-03        ABAN   21307.541602       Mid   \n",
       "...            ...         ...            ...       ...   \n",
       "2412495 2025-06-24  ZENSARTECH  192371.773014     Small   \n",
       "2412496 2025-06-24      ZENTEC  171530.668333     Small   \n",
       "2412497 2025-06-24   ZFCVINDIA  250523.849472     Small   \n",
       "2412498 2025-06-24   ZYDUSLIFE  963066.551829       Mid   \n",
       "2412499 2025-06-24   ZYDUSWELL  127410.641931     Small   \n",
       "\n",
       "                           Sector                    Theme  \\\n",
       "0                              IT            IT - Software   \n",
       "1                     Diversified              Diversified   \n",
       "2                              IT            IT - Software   \n",
       "3                       Chemicals                Chemicals   \n",
       "4                       Crude Oil          Oil Exploration   \n",
       "...                           ...                      ...   \n",
       "2412495                        IT            IT - Software   \n",
       "2412496                        IT            IT - Software   \n",
       "2412497  Automobile & Ancillaries           Auto Ancillary   \n",
       "2412498                Healthcare  Pharmaceuticals & Drugs   \n",
       "2412499                      FMCG            Consumer Food   \n",
       "\n",
       "                             GICS       LTM  ValueYield  ValueABS  ...  \\\n",
       "0           InformationTechnology       NaN         NaN       NaN  ...   \n",
       "1                     Industrials       NaN         NaN       NaN  ...   \n",
       "2           InformationTechnology       NaN         NaN       NaN  ...   \n",
       "3                       Chemicals       NaN         NaN       NaN  ...   \n",
       "4                          Energy       NaN         NaN       NaN  ...   \n",
       "...                           ...       ...         ...       ...  ...   \n",
       "2412495     InformationTechnology  0.706377    0.661956  0.610128  ...   \n",
       "2412496     InformationTechnology  0.924433    0.291073  0.288632  ...   \n",
       "2412497  Automobile & Ancillaries  0.303555    0.320072  0.411037  ...   \n",
       "2412498                Healthcare  0.392913    0.806724  0.694777  ...   \n",
       "2412499          Consumer Staples  0.318651    0.277663  0.249629  ...   \n",
       "\n",
       "         ThemeUltraShortAM  ThemeShortAM  ThemeMidAM  ThemeLongAM  ThemeLTM  \\\n",
       "0                 0.411617      0.657857    0.574197     0.427606  0.006074   \n",
       "1                 0.385273      0.336062    0.505973     0.729573  0.210003   \n",
       "2                 0.411617      0.657857    0.574197     0.427606  0.006074   \n",
       "3                 0.637129      0.427392    0.471549     0.552877  0.064611   \n",
       "4                 0.879100      0.748131    0.409333     0.625954       NaN   \n",
       "...                    ...           ...         ...          ...       ...   \n",
       "2412495           0.812606      0.793572    0.403107     0.459416  0.601718   \n",
       "2412496           0.812606      0.793572    0.403107     0.459416  0.601718   \n",
       "2412497           0.350545      0.607686    0.523744     0.365520  0.598362   \n",
       "2412498           0.433340      0.394800    0.356062     0.652217  0.304999   \n",
       "2412499           0.467106      0.361714    0.501832     0.617188  0.259321   \n",
       "\n",
       "         ThemeLTMA   ThemeEM  ThemeLowVol  ThemeDownVol  ThemeAvgVol  \n",
       "0         0.463739  0.457505     0.583383      0.523855     0.562052  \n",
       "1         0.664657  0.420483     0.530650      0.549679     0.535854  \n",
       "2         0.463739  0.457505     0.583383      0.523855     0.562052  \n",
       "3         0.556062  0.551893     0.200178      0.206188     0.193764  \n",
       "4         0.645305  0.394864     0.561185      0.690176     0.601453  \n",
       "...            ...       ...          ...           ...          ...  \n",
       "2412495   0.444828  0.493615     0.512314      0.468275     0.495354  \n",
       "2412496   0.444828  0.493615     0.512314      0.468275     0.495354  \n",
       "2412497   0.505113  0.408815     0.442001      0.440641     0.447037  \n",
       "2412498   0.548429  0.469953     0.569681      0.592292     0.578240  \n",
       "2412499   0.537941  0.479920     0.718564      0.727590     0.729007  \n",
       "\n",
       "[2412500 rows x 76 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
